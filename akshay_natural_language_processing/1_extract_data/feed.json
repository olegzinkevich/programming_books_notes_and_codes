[
 {
  "title": "Four short links: 9 April 2019",
  "content": "From Chrome to Edge, Old Web, Public Sans, and The Feedback Fallacy\n\nWhat Microsoft Removed from Chrome to make Edge (The Verge) -- Microsoft has removed or replaced more than 50 of Google\u2019s services that come as part of Chromium, including things like ad blocking, Google Now, Google Cloud Messaging, and Chrome OS-related services.\n\n\nIt Seems that Google is Forgetting the Old Web -- it seems more correct to say that Google forgets stuff that is more than 10 years old. If this is the case, Google will remember and index a smaller part of the web every year. Google may do so simply because it would be impossible to do more, for economical and/or technological constraints, which sooner or later would also hit its competitors. But this only makes bigger the problem of what to remember, what to forget, and, above all, how and who should remember and forget.\n\n\nPublic Sans -- Open source. A strong, neutral typeface for text or display. From USWDS.\n\nThe Feedback Fallacy (HBR) -- identifies three theories underpinning coworker feedback, and shows how they're all wrong. What these three theories have in common is self-centeredness: they take our own expertise and what we are sure is our colleagues\u2019 inexpertise as givens; they assume that my way is necessarily your way. But as it turns out, in extrapolating from what creates our own performance to what might create performance in others, we overreach. Research reveals that none of these theories is true. Gives advice on how to give feedback more effectively, too. At best, this fetish with feedback is good only for correcting mistakes\u2014in the rare cases where the right steps are known and can be evaluated objectively. And at worst, it\u2019s toxic.\n\nContinue reading Four short links: 9 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/w0mnFBrS9F4/four-short-links-9-april-2019"
 },
 {
  "title": "Why a data scientist is not a data engineer",
  "content": "Or, why science and engineering are still different disciplines.\n\"A scientist can discover a new star, but he cannot make one. He would have to ask an engineer to do it for him.\"\n\n\u2013Gordon Lindsay Glegg, The Design of Design (1969)\n\n\n\n\nA few months ago, I wrote about the differences between data engineers and data scientists. I talked about their skills and common starting points.\n\n\n\nAn interesting thing happened: the data scientists started pushing back, arguing that they are, in fact, as skilled as data engineers at data engineering. That was interesting because the data engineers didn\u2019t push back saying they\u2019re data scientists.\n\n\n\nSo, I\u2019ve spent the past few months gathering data and observing the behaviors of data scientists in their natural habitat. This post will offer more information about why a data scientist is not a data engineer.\n\nWhy does this even matter?\n\nSome people complained that this data scientist versus data engineer is a mere focus on titles. \u201cTitles shouldn\u2019t hold people back from learning or doing new things,\u201d they argued. I agree; learn as much as you can. Just know that your learning may only scratch the surface of what\u2019s necessary to put something in production. Otherwise, this leads to failure with big data projects.\n\nIt\u2019s also feeding into the management level at companies. They\u2019re hiring data scientists expecting them to be data engineers.\n\nI\u2019ve heard this same story from a number of companies. They all play out the same: a company decides that data science is the way to get VC money, tons of ROI, mad street cred in their business circle, or some other reasons. This decision happens at C-level or VP-level. Let\u2019s call this C-level person Alice.\n\n\n\nThe company goes on an exhaustive search to find the best data scientist ever. Let\u2019s call this data scientist Bob.\n\n\n\nIt\u2019s Bob\u2019s first day. Alice comes up to Bob and excitedly tells him about all the projects she has in mind.\n\n\n\n\u201cThat\u2019s great. Where are these data pipelines and where is your Spark cluster?\u201d Bob asks.\n\n\n\nAlice responds, \u201cThat\u2019s what we\u2019re expecting you to do. We hired you to do data science.\u201d\n\n\n\n\u201cI don\u2019t know how to do any of that,\u201d says Bob.\n\n\n\nAlice looks at him quizzically, \u201cBut you\u2019re a data scientist. Right? This is what you do.\u201d\n\n\n\n\u201cNo, I use the data pipelines and data products that are already created.\u201d\n\n\n\nAlice goes back to her office to figure out what happened. She stares at overly simplistic diagrams like the one shown in Figure 1 and can\u2019t figure out why Bob can\u2019t do the simple big data tasks.\n\n\nFigure 1. Overly simplistic venn diagram with data scientists and data engineers. Illustration by Jesse Anderson, used with permission.\n\n\n\n\nThe limelight\n\nThere are two questions that come out of these interactions:\n\n\n\n\n\tWhy doesn\u2019t management understand that data scientists aren\u2019t data engineers?\n\tWhy do some data scientists think they\u2019re data engineers?\n\n\n\n\nI\u2019ll start with the management side. Later on, we\u2019ll talk about the data scientists themselves.\n\n\n\nLet\u2019s face it. Data engineering is not in the limelight. It isn\u2019t being proclaimed as the best job of the 21st century. It isn\u2019t getting all of the media buzz. Conferences aren\u2019t telling CxOs about the virtues of data engineering. If you only look at the cursory message, it\u2019s all about data science and hiring data scientists.\n\n\n\nThis is starting to change. We have conferences on data engineering. There is a gradual recognition of the need for data engineering. I\u2019m hoping pieces like this one shed light on this necessity. I\u2019m hoping my body of work will educate organizations on this critical need.\n\nRecognition and appreciation\n\nEven when organizations have data science and data engineering teams, there is still a lack of appreciation for the work that went into the data engineering side.\n\n\n\nYou even see this lack of credit during conference talks. The data scientist is talking about what they\u2019ve created. I can see the extensive data engineering that went into their model, but it\u2019s never called out during the talk. I don\u2019t expect the talk to cover it in detail, but it would be nice to acknowledge the work that went into enabling their creation. Management and beginners to data science perceive that everything was possible with the data scientist\u2019s skill set.\n\nHow to get appreciation\n\nLately, I\u2019ve been getting questions from data engineers on how to get into their company\u2019s limelight. They\u2019re feeling that when a data scientist goes to show their latest creation, they\u2019re either taking all of the credit or they\u2019re given all of the credit by the management. Their basic question is: \u201cHow can I get the data scientists to stop taking credit for something that was both of our work?\u201d\n\n\n\nThat\u2019s a valid question from what I\u2019m seeing at companies. Management doesn\u2019t realize (and it isn\u2019t socialized) the data engineering work that goes into all things data science. If you\u2019re reading this and you\u2019re thinking:\n\n\n\tMy data scientists are data engineers\n\tMy data scientists are creating really complicated data pipelines\n\tJesse must not know what he\u2019s talking about\n\n\n...you probably have a data engineer in the background who isn\u2019t getting any limelight.\n\n\n\nSimilar to when data scientists quit without a data engineer, data engineers who don\u2019t get recognition and appreciation will quit. Don\u2019t kid yourself; there\u2019s an equally hot job market for qualified data engineers as there is for data scientists.\n\nData science only happens with a little help from our friends\n\nFigure 2. Even the Italians knew the importance of data engineers in the 1400s. Image from the Met Museum, public domain.\n\n\nYou might have heard about the myth of Atlas. He was punished by having to hold up the world/sky/celestial spheres. The earth only exists in its current form because Atlas holds it up.\n\n\n\nIn a similar way, data engineers hold up the world of data science. There isn\u2019t much thought or credit that goes to the person holding up the world, but there should be. All levels of an organization should understand that data science is only enabled through the work of the data engineering team.\n\nData scientists aren\u2019t data engineers\n\nThat brings us to why data scientists think they\u2019re data engineers.\n\n\n\nA few caveats to head off comments before we continue:\n\n\n\tI think data scientists are really smart, and I enjoy working with them.\n\tI\u2019m wondering if this intelligence causes a higher IQ Dunning-Kruger effect.\n\tSome of the best data engineers I\u2019ve known have been data scientists, though this number is very small.\n\tThere is a consistent overestimation when assessing our own skills.\n\n\n\n\n\nFigure 3. Empirical diagram of data scientists\u2019 perceived data engineering skills versus their actual skills. Illustration by Jesse Anderson, used with permission.\n\n\n\n\nIn talking to data scientists about their data engineering skills, I\u2019ve found their self-assessments to vary wildly. It\u2019s an interesting social experiment in biases. Most data scientists over assessed their own data engineering abilities. Some gave an accurate assessment, but none of them gave a lower assessment than their actual ability.\n\n\n\nThere are two things missing from this diagram:\n\n\n\tWhat is the skill level of data engineers?\n\tWhat is the skill level needed for a moderately complicated data pipeline?\n\n\n\nFigure 4. Empirical diagram of data scientists\u2019 and data engineers\u2019 data engineering skills with the skill needed to create a moderately complicated data pipeline. Illustration by Jesse Anderson, used with permission.\n\n\nFrom this figure, you can start to see the differences in the required data engineering abilities. In fact, I\u2019m being more generous with the number of data scientists able to create a moderately complicated data pipeline. The reality may be that data scientists should be half of what the diagram shows.\n\n\n\nOverall, it shows the approximate portions of the two groups who can and cannot create data pipelines. Yes, some data engineers can\u2019t create a moderately complicated data pipeline. Conversely, most data scientists can\u2019t, either. This comes back to the business issue at hand: organizations are giving their big data projects to individuals who lack the ability to succeed with the project.\n\n\n\n\n\n\nWhat\u2019s a moderately complicated data pipeline?\nA moderately complicated data pipeline is one step above the bare minimum to create a data pipeline. An example of a bare minimum is processing text files stored in HDFS/S3 with Spark. An example of a moderately complicated data pipeline is to start optimizing your storage with a correctly used NoSQL database that uses a binary format like Avro.\nI think data scientists are thinking that their simple data pipeline is what data engineering is. The reality is that they\u2019re talking about hello-world levels and a far more complicated data pipeline is required. In the past, a data engineer performed the really difficult data engineering behind the scenes and the data scientists didn\u2019t have to deal with it.\n\n\n\nYou might think, \u201cGood, so 20% of my data scientists can actually do this. I don't need a data engineer after all.\u201d First, remember this chart is being charitable in showing data scientists\u2019 abilities. Remember that moderately complicated is still a pretty low bar. I need to create another diagram to show how few data scientists can handle the next step up in complexity. This is where the percentage drops to 1% or less of data scientists.\n\nWhy aren\u2019t data scientists data engineers?\n\nSometimes I prefer to see the reflected manifestations of problems. These are a few examples of the manisted problems that make data scientists lack the data engineering skill set.\n\nUniversity and other courses\n\nData science is the hot new course out there for universities and online courses. There are all sorts of offerings out there, but virtually all of them have the same problem: they either completely lack or have one data engineering class.\n\n\n\nWhen I see a new university\u2019s data science curriculum announced, I take a look at it. Sometimes, I\u2019ll be asked for comments on a university\u2019s proposed data science curriculum. I give them same feedback: \u201cAre you expecting expert programmers? Because there isn\u2019t any coverage of the programming or systems required to even consume a data pipeline that\u2019s been created.\u201d\n\n\n\nThe course outlines generally focus on the statistics and math required. This reflects what companies and academics think data science should look like. The real world looks rather different. The poor students are left to fend for themselves for the rest of these non-trivial learnings.\n\n\n\nWe can take a step back and look at this academically by looking at course requirements for a master\u2019s degree in distributed systems. Obviously, a data scientist doesn\u2019t need this level of depth, but it helps show what\u2019s missing and the big holes in a data scientist\u2019s skill set. There are some major deficiencies.\n\nData engineering != Spark\n\nA common misconception from data scientists\u2014and management\u2014is that data engineering is just writing some Spark code to process a file. Spark is a good solution for batch compute, but it isn\u2019t the only technology you\u2019ll need. A big data solution will require 10-30 different technologies all working together.\n\n\n\nThis sort of thinking lies at the heart of big data failures. Management thinks they have a new silver bullet to kill all of their big data problems. The reality is far more complicated than that.\n\n\n\nWhen I mentor an organization on big data, I check for this misconception at all layers of the organization. If it does exist, I make sure I talk about all of the technologies they\u2019ll need. This removes the misconception that there\u2019s an easy button in big data and there\u2019s a single technology to solve all of it.\n\nWhere is the code from?\n\nSometimes data scientists will tell me how easy data engineering is. I\u2019ll get them to tell me how and why they think that. \u201cI can get all the code I need from StackOverflow or Reddit. If I need to create something from scratch, I can copy someone\u2019s design in a conference talk or a whitepaper.\n\n\n\nTo the non-engineer, this might seem OK. To the engineer, this starts major alarm bells. The legal issues aside, this isn\u2019t engineering. There are very few cookie-cutter problems in big data. Everything after \u201chello world\u201d has more complexity that needs a data engineer because there isn't a cookie-cutter approach to dealing with it. Getting your design copied from a white paper could lead to a poor performing design or worse.\n\n\n\nI\u2019ve dealt with a few data science teams who\u2019ve tried this monkey-see-monkey-do approach. It doesn\u2019t work well. This is due to big data\u2019s spike in complexity and the extreme focus on use cases. The data science team will often drop the project as it exceeds their data engineering abilities.\n\n\n\nPut simply, there's a big difference between \u201cI can copy code from stackoverflow\u201d or \u201cI can modify something that's already been written\u201d and \u201cI can create this system from scratch.\u201d\n\n\n\nPersonally, I\u2019m worried that data science teams are going to be these sources of massive technical debt that squelches big data productivity in organizations. By the time it\u2019s found out, the technical debt will be so high it might be infeasible to correct it.\n\n\n\nWhat\u2019s the longest their code has been in production?\n\nA core difference for data scientists is their depth. This depth is shown in two ways. What\u2019s the longest time their code been in production\u2014or has it ever been in production? What is the longest, largest, or most complicated program they have ever written?\n\n\n\nThis isn\u2019t about gamesmanship or who\u2019s better; it\u2019s showing if they know what happens when you put something in production and how to maintain code. Writing a 20-line program is comparatively easy. Writing 1,000 lines of code that\u2019s maintainable and coherent is another situation all together. People who\u2019ve never written more than 20 lines don\u2019t understand the miles of difference in maintainability. All of their complaints about Java verbosity or why programming best practices need to be used come into focus with large software projects.\n\n\n\nMoving fast and breaking things works well when evaluating and discovering data. It requires a different and more intense level when working with code that goes into production. It\u2019s for reasons like these that most data scientist\u2019s code gets rewritten before it goes into production.\n\nWhen they design a distributed system\n\nOne way to know the difference between data scientists and data engineers is to see what happens when they write their own distributed systems. A data scientist will write one that is very math focused but performs terribly. A software engineer with a specialization in writing distributed systems will create one that performs well and is distributed (but seriously don\u2019t write your own). I\u2019ll share a few stories of my interactions with organizations where data scientists created a distributed system.\n\n\n\nA business unit that was made up of data scientists at my customer\u2019s company created a distributed system. I was sent in to talk to them and get an understanding of why they created their own system and what it could do. They were doing (distributed) image processing.\n\n\n\nI started out by asking them why they created their own distributed system. They responded that it wasn\u2019t possible to distribute the algorithm. To validate their findings, they contracted another data scientist with a specialty in image processing. The data scientist contractor confirmed that it wasn\u2019t possible to distribute the algorithm.\n\n\n\nIn the two hours I spent with the team, it was clear that the algorithm could be distributed on a general-purpose compute engine, like Spark. It was also clear that the distributed system they wrote wouldn\u2019t scale and had serious design flaws. By having another data scientist validate their findings instead of a qualified data engineer, they had another novice programmer validate their novice findings.\n\n\n\nAt another company run by mathematicians, they told me about the distributed system they wrote. It was written so that math problems could be run on other computers. A few things were clear after talking to them. They could have used a general-purpose compute engine and been better off. The way they were distributing and running jobs was inefficient. It was taking longer to do the RPC network traffic than it was to perform the calculation.\n\n\n\nThere are commonalities to all of these stories and others I didn\u2019t tell:\n\n\n\tData scientists focus on the math instead of the system. The system is there to run math instead of running math efficiently.\n\tData engineers know the tricks that aren\u2019t math. We\u2019re not trying to cancel out infinities.\n\tA data scientist asks, \u201chow can I get a computer to do my math problems?\u201d A data engineer asks, \u201chow can I get a computer to do my math problems as fast and efficiently as possible?\u201d\n\tThe organizations could have saved themselves time, money, and heartache by using a general-purpose engine instead of writing their own.\n\n\nWhat\u2019s the difference?\n\nYou\u2019ve made it this far and I hope I\u2019ve convinced you: data scientists are not data engineers. But really, what difference does all of this make?\n\n\n\nThe difference between a data scientist and a data engineer is the difference between an organization succeeding or failing in their big data project.\n\nData science from an engineering perspective\n\nWhen I first started to work with data scientists, I was surprised at how little they begged, borrowed, and stole from the engineering side. On the engineering front, we have some well-established best practices that weren\u2019t being used on the data science side. A few of these are:\n\n\n\tSource control\n\tContinuous integration\n\tProject management frameworks like Agile or Scrum\n\tIDEs\n\tBug tracking\n\tCode reviews\n\tCommenting code\n\n\n\n\nYou saw me offhandedly mention the technical debt I\u2019ve seen in data science teams. Let me elaborate on why I\u2019m so worried about this. When I start pushing on a data science team to use best practices, I get two answers: \u201cwe know and we\u2019re going to implement that later\u201d or \u201cwe don\u2019t need these heavyweight engineering practices. We\u2019re agile and nimble. These models won\u2019t go into production yet.\u201d The best practices never get implemented and that model goes straight into production. Each one of these issues leads to a compounding of technical debt.\n\nCode quality\n\nWould you put your intern\u2019s code into production? If you\u2019re in management, go ask your VP of engineering if they\u2019ll put a second-year computer science student\u2019s code into production. You might get a vehement no. Or they might say after the code was reviewed by other members of the team.\n\n\n\nAre you going to put your data scientist\u2019s code into production? Part of the thrust of this article is that data scientists are often novices at programming\u2014at best\u2014and their code is going into production. Take a look back up at the best practices that data science teams aren\u2019t doing. There are no checks and balances to keep amateur code from going into production.\n\nWhy did they get good?\n\nI want to end this by addressing the people who are still thinking their data scientists are data engineers. Or those data scientists who are also qualified data engineers. I want to restate that you can see from the figure it is possible, just not probable.\n\n\n\nIf this is true, I\u2019d like you to think about why this happened.\n\n\n\nIn my experience, this happens when the ratio of data scientists to data engineers is well out of alignment. This happens when the ratio is inverted and there are zero data engineers in the organization. There should be more like two to five data engineers per data scientist. This ratio is needed because more time goes into the data engineering side than the data science.\n\n\n\nWhen teams lack the right ratio, they\u2019re making poor use of their data scientists\u2019 time. Data scientists tend to get stuck on the programming parts that data engineers are proficient in. I\u2019ve seen too many data scientists spend days on something that would take a data engineer an hour. This incorrectly perceived and solved problem leads organizations to hire more data scientists instead of hiring the right people who make the process more efficient.\n\n\n\nOther times, they\u2019re misunderstanding what a data engineer is. Having unqualified or the wrong type of data engineer is just as bad. You need to make sure you\u2019re getting qualified help. This leads to the fallacy that you don\u2019t need a data engineer because the ones you\u2019ve worked with aren\u2019t competent.\n\n\n\nI\u2019m often asked by management how they should get their data scientists to be more technically proficient. I respond that this is more a question of should the data scientists become more technically proficient. This is important for several reasons:\n\n\n\tThere\u2019s a low point of diminishing returns for a data science team that isn\u2019t very technical to begin with. They can study for months, but may never get much better.\n\tIt assumes that a data scientist is a data engineer and that isn\u2019t correct. It would be better to target the one or two individuals on the data science team with the innate abilities to get better.\n\tIs there an ROI to this improvement? If the data science team gets better, what could it do better or different?\n\tIt assumes the highest value is to improve the data science team. The better investment may be in improving the data engineering team and facilitating better communication and relations between the data science and data engineering teams.\n\tIt assumes that the data scientists actually want to improve technically. I\u2019ve found that data scientists consider data engineering a means to an end. By doing the data engineering work, they get to do the fun data science stuff.\n\n\nWhat should we do?\n\nGiven that a data scientist is not a data engineer, what should we do? First and foremost, we have to understand what data scientists and data engineers do. We have to realize this isn\u2019t a focus on titles and limiting people based on that. This is about a fundamental difference in what each person is good at and their core strengths.\n\n\n\nHaving a data scientist do data engineering tasks is fraught with failure. Conversely, having a data engineer do data science is fraught with failure. If your organization is trying to do data science, you need both people. Each person fulfills a complementary and necessary role.\n\n\n\nFor larger organizations, you will start to see the need for people who split the difference between the data scientist and data engineer skill sets. I recommend the management team look at creating a machine learning engineer title and hiring for it.\n\nSuccess with big data\n\nAs you\u2019ve seen here, the path to success with big data isn\u2019t just technical\u2014there are critical management parts. Misunderstanding the nature of data scientists and data engineers is just one of those. If you\u2019re having trouble with your big data projects, don\u2019t just look for technical reasons. The underlying issue may be a management or team failure.\n\n\n\nAs you\u2019re doing a root cause analysis of why a big data project stalled or failed, don\u2019t just look at or blame the technology. Also, don\u2019t just take the data science team\u2019s explanation because they may not have enough experience to know or understand why it failed. Instead, you\u2019ll need to go deeper\u2014and often more painfully\u2014to look at the management or team failings that led to a project failure.\n\n\n\nFailures like these form a repeating and continuous pattern. You can move to the newest technology, but you\u2019re just fixing the systemic issues. Only by fixing the root issue can you start to be successful.\nContinue reading Why a data scientist is not a data engineer.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vRnLXKurjtg/why-a-data-scientist-is-not-a-data-engineer"
 },
 {
  "title": "Four short links: 8 April 2019",
  "content": "Chinese Livestreaming, Tech and Teens, YouTube Professionalizing, and Inclusive Meetings\n\nInside the Dystopian Reality of China's Livestreaming Craze -- Livestreaming exacts a huge mental toll on the people who do it. It\u2019s easy money, but also toxic. Overeggs the dystopia (all interaction is a performance, professional interaction no less so), but is still a quick precis of where livestreaming is at in China. As for the toxic money, just ask Justin Kan.\n\nScreens, Teens, and Psychological Well-Being: Evidence From Three Time-Use-Diary Studies -- We found little evidence for substantial negative associations between digital-screen engagement\u2014measured throughout the day or particularly before bedtime\u2014and adolescent well-being.\n\n\nThe Golden Age of YouTube is Over (The Verge) -- By promoting videos that meet certain criteria, YouTube tips the scales in favor of organizations or creators\u2014big ones, mostly\u2014that can meet those standards. My favorite part is where YouTube refers to the people who made it popular as our endemic creators, a phrase that'd make Orwell stabbier than usual.\n\nInclusive Scientific Meetings -- This document presents some concrete recommendations for how to incorporate inclusion and equity practices into scientific meetings, from the ground up. This document includes three sections: planning the meeting; during the meeting; and assessing the meeting. A great cheatsheet that applies to non-science meetings, too.\n\nContinue reading Four short links: 8 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-dO5qNzZZsk/four-short-links-8-april-2019"
 },
 {
  "title": "Four short links: 5 April 2019",
  "content": "DIY Bio, Perl, Knowledge Graph Learning, and Amazon Memos\n\nEngineering Proteins in the Cloud -- Amazingly, we're pretty close to being able to create any protein we want from the comfort of our Jupyter Notebooks, thanks to developments in genomics, synthetic biology, and most recently, cloud labs. In this article, I'll develop Python code that will take me from an idea for a protein all the way to expression of the protein in a bacterial cell, all without touching a pipette or talking to a human. The total cost will only be a few hundred dollars! Using Vijay Pande from A16Z's terminology, this is Bio 2.0.\n\n\n93% of Paint Splatters are Valid Perl Programs (Colin McMillen) -- tongue-in-cheek, but clever. I, of course, am fluent in those paint splatters. Have written a best-selling book on executable paint splatters. I should feel called-out, I guess, but it's too funny for me to feel much pain.\n\nAmpliGraph -- Python library for representation learning on knowledge graphs. [...] Use AmpliGraph if you need to: (1) Discover new knowledge from an existing knowledge graph. (2) Complete large knowledge graphs with missing statements. (3) Generate stand-alone knowledge graph embeddings. (4) Develop and evaluate a new relational model.\n\n\nWriting Docs at Amazon -- how to write those famous six-page narrative memos as preparation for meeting with Jeff Bezos, from someone who was there. As much about the meetings as the memos, as it should be.\n\nContinue reading Four short links: 5 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/b50pWTracQ4/four-short-links-5-april-2019"
 },
 {
  "title": "Four short links: 4 April 2019",
  "content": "Language Creators, Undersea Cable, Open Source Trends, Making Math Questions\n\nA Conversation with Language Creators: Guido, James, Anders, and Larry (YouTube) -- A lot of people make the mistake of thinking that languages move at the same speed as hardware or all of the other technologies we live with. But languages are much more like math and much more like the human brain, and they all have evolved slowly. And we're still programming in languages that were invented 50 years ago. All the principles of functional programming were thought of more than 50 years ago.\n\n\nUndersea Internet Cables and Big Internet Companies (APNIC) -- interesting numbers. Between 2016 and 2020, about 100 new cables have been laid or planned. [...] The unit cost is cheaper for new cables than old cables whose lit capacity is increased. [...] In the last five years, the cables that are partly owned by Google, Facebook, Microsoft, and Amazon have risen eight-fold, and there are more such cables in the pipeline. These content providers also consume over 50% of all international bandwidth, and TeleGeography projects that by 2027, they could consume over 80%.\n\n\nMaking Sense of a Crazy Year in Open Source -- if you haven't kept your eye on the latest weirdness in open source licensing (as companies attempt to squeeze commercial leverage from licenses), this is a great intro. Elastic CEO Shay Banon summed it up, saying: \u201cWe now have three tiers: open source and free, free but under a proprietary license, and paid under a proprietary license.\u201d\n\n\nMathematics Data Set (GitHub) -- This data set code generates mathematical question and answer pairs, from a range of question types at roughly school-level difficulty. This is designed to test the mathematical learning and algebraic reasoning skills of learning models. Not what Dan Meyer would call good problems, mind you!\n\nContinue reading Four short links: 4 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BqtxrWqe6x8/four-short-links-4-april-2019"
 },
 {
  "title": "150+ live online training courses opened for April and May",
  "content": "Get hands-on training in TensorFlow, cloud computing, blockchain, Python, Java, and many other topics.Learn new topics and refine your skills with more than 150 new live online training courses we opened up for April and May on the O'Reilly online learning platform.\n\nAI and machine learning\n\nDeep Learning from Scratch, April 19\nBeginning Machine Learning with Pytorch, May 1\nIntermediate Machine Learning with PyTorch, May 2\nNatural Language Processing (NLP) from Scratch, May 6\nDeep Learning Fundamentals, May 7\nHands-On Machine Learning with Python: Classification and Regression, May 9\nHands-On Machine Learning with Python: Clustering, Dimension Reduction, and Time Series Analysis, May 10\nSentiment Analysis for Chatbots in Python, May 14\nArtificial Intelligence: An Overview of AI and Machine Learning, May 15\nTensorFlow Extended: Data Validation and Transform, May 16\nDeep Learning for Natural Language Processing (NLP), May 16\nGetting Started with Machine Learning, May 20\nTensorFlow Extended: Model Build, Analysis, and Serving, May 21\nEssential Machine Learning and Exploratory Data Analysis with Python and Jupyter Notebook, May 28-29\nArtificial Intelligence: Real-World Applications, May 29\n\nBlockchain\n\nIntroducing Blockchain, May 3\nIntroduction to Distributed Ledger Technology for Enterprise, May 9\nBlockchain and Cryptocurrency Essentials, May 16\nCertified Blockchain Solutions Architect (CBSA) Certification Crash Course, May 22\nAn Introduction to Ethereum DApps, May 23\n\nBusiness\n\nSpotlight on Cloud: The Future of Internet Security with Bruce Schneier, April 4\nProduct Management in 90 Minutes, April 11\nSpotlight on Data: Creating Smart Products Requires Collaboration, with Gretchen Anderson , April 15\nAgile for Everybody, April 18\nDeveloping Your Coaching Skills, April 22\nSpotlight on Innovation: Building Resilient Systems with Heidi Waterhouse and Chris Guzikowski, April 23\nManaging Stress and Building Resiliency, May 2\nHaving Difficult Conversations , May 6\nUnlock Your Potential, May 7\nSpotlight on Innovation: A Trader\u2019s Journey to Python and Beyond, with David Bednarczyk , May 8\n60 Minutes to Better Product Metrics, May 9\nBusiness Fundamentals, May 10\nBuilding the Courage to Take Risks, May 13\nCompeting with Business Strategy, May 14\nBetter Business Writing, May 15\nLeadership Communication Skills for Managers, May 16\nPerformance Goals for Growth, May 21\nIntroduction to Critical Thinking, May 22\nIntroduction to Delegation Skills, May 22\nNegotiation Fundamentals, May 23\nGiving a Powerful Presentation, May 28\nEmotional Intelligence in the Workplace, May 31\nGetting Unstuck, June 3\nIntroduction to Google Cloud Platform, June 3-4\n\nData science and data tools\n\nApache Hadoop, Spark, and Big Data Foundations, April 22\nFraud Analytics Using Python, April 30\nHands-On Algorithmic Trading With Python, May 1\nData Structures in Java, May 1\nCleaning Data at Scale, May 13\nBig Data Modeling, May 13-14\nQuantitative Trading with Python, May 15\nFundamentals of Data Architecture, May 20-21\nTime Series Forecasting, May 22\nPractical Data Cleaning with Python, May 22-23\nIntroduction to Google Cloud Platform, June 3-4\n\nProgramming\n\nJava Full Throttle with Paul Deitel: A Code-Intensive One-Day Course, April 22\nScala Fundamentals: From Core Concepts to Real Code in 5 Hours, May 1\nHands-On Introduction to Apache Hadoop and Spark Programming, May 1-2\nC# Programming: A Hands-On Guide, May 2\nJava 8 Generics in 3 Hours, May 2\nLearning Python 3 by Example, May 2\nGetting Started with Java, May 3\nProgramming with Data: Foundations of Python and Pandas, May 6\nSOLID Principles of Object-Oriented and Agile Design, May 7\nIntroduction to the Go Programming Language, May 7\nBeginner\u2019s Guide to Writing AWS Lambda Functions in Python, May 7\nGetting Started with Pandas, May 7\nDesign Patterns Boot Camp, May 8-9\nModern JavaScript, May 13\nPython Full Throttle with Paul Deitel, May 13\nReactive Spring Boot, May 13\nReactive Programming with Java Completable Futures, May 13\nWhat's New In Java, May 14\nAdvanced SQL Series: Window Functions, May 14\nIntroduction to Python Programming, May 14\nNext-Generation Java Testing with JUnit 5, May 15\nIntro to Mathematical Optimization, May 15\nProgramming with Java Lambdas and Streams, May 16\nIoT Fundamentals, May 16-17\nWorking with Dataclasses in Python 3.7, May 21\nTest-Driven Development In Python, May 21\nRust Programming: A Crash Course, May 22\nPythonic Object-oriented Programming, May 22\nAutomating Go Projects, May 23\nPython: The Next Level, May 23-24\nGetting Started with OpenStack, May 24\nGround Zero Programming with JavaScript , May 28\nOCA Java SE 8 Programmer Certification Crash Course, May 28-30\nMastering the Basics of Relational SQL Querying, May 29-30\nScalable Concurrency with the Java Executor Framework, May 30\n\nSecurity\n\nCCNA Cyber Ops SECFND Crash Course 210-250, April 12\nCCNA Cyber Ops SECFND Crash Course 210-250, April 12\nCCNA Cyber Ops SECOPS crash course 210-255, April 22\nCertified Ethical Hacker (CEH) Crash Course, May 2-3\nSecurity Operation Center (SOC) Best Practices, May 3\nIntroduction to Encryption, May 22\nGetting Started with Cyber Investigations and Digital Forensics, May 23\nCyber Security Fundamentals, May 23-24\nCompTIA Security+ SY0-501 Crash Course , May 29-30\nEthical Hacking Bootcamp with Hands-on Labs, May 29-31\n\nSystems engineering and operations\n\nAnalyzing Software Architecture, April 16\nAutomating Architectural Governance Using Fitness Functions, April 22\nNext Level Git - Master Your Workflow, April 22\nContinuous Delivery with Jenkins and Docker, April 24\nBootiful Testing, April 29\nComparing Service-Based Architectures, April 30\nGetting Started with OpenShift, May 1\nAWS Certified Developer Associate Crash Course, May 1-2\nLinux Under the Hood, May 2\nAWS Certified Solutions Architect Associate Crash Course, May 6-7\nBuilding a Deployment Pipeline with Jenkins 2, May 8-9\nAWS Account Setup Best Practices, May 10\nHow Networks Really Work, May 10\nIntroduction to Docker CI/CD, May 13\nLinux Troubleshooting, May 13\nJenkins 2 - Up and Running, May 13\nIntroduction to Knative, May 14\nAWS for Mobile App Developer, May 14\nGoogle Cloud Certified Associate Cloud Engineer Crash Course, May 15-16\nGetting Started with Cloud Computing, May 16\nManaging Containers on Linux, May 16\nAWS Certified SysOps Administrator (Associate) Crash Course, May 16-17\nGoogle Cloud Platform (GCP) for AWS Professionals, May 17\nSoftware Architecture by Example, May 17\nMicroservices Caching Strategies, May 17\nKubernetes in 4 Hours, May 17\nIntroduction to Docker Images, May 20\nChaos Engineering: Planning and Running Your First Game Day, May 20\nAWS Managed Services, May 20-21\nKafka Fundamentals, May 20-21\nArchitecture for Continuous Delivery, May 21\n9 Steps to Awesome with Kubernetes, May 21\nAWS Certified Security - Specialty Crash Course, May 21-22\nIstio on Kubernetes: Enter the Service Mesh, May 22\nAWS Machine Learning Specialty Certification Crash Course, May 22-23\nGetting Started with Amazon Web Services (AWS), May 22-23\nAutomating Architectural Governance Using Fitness Functions, May 23\nChaos Engineering: Planning, Designing, and Running Automated Chaos Experiment, May 23\nIntroduction to Docker Containers, May 24\nBuilding and Managing Kubernetes Applications, May 24\nDocker for JVM projects, May 28\nCloud Complexity Management, May 28\nAnsible for Managing Network Devices, May 28\nUnderstanding AWS Cloud Compute Options, May 28-29\nRed Hat Certified System Administrator (RHCSA) Crash Course, May 28-31\nKubernetes Serverless with Knative, May 29\nNext Level Git - Master Your Content, May 30\nLearning MongoDB - A Hands-on Guide, May 30\nAWS Certified Cloud Practitioner Exam Crash Course, May 30-31\nComparing Service-Based Architectures, May 31\nAmazon Web Services (AWS) Technical Essentials, June 3\nImplementing and Troubleshooting TCP/IP, June 3\nIntroduction to Google Cloud Platform, June 3-4\nIntroduction to Google Cloud Platform, June 3-4\nContinue reading 150+ live online training courses opened for April and May.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/uZjs0zI3G_g/150-live-online-training-courses-opened-for-april-and-may"
 },
 {
  "title": "Four short links: 3 April 2019",
  "content": "HTML DRM, Toxic Incentives, Moral Crumple Zones, and Stats + Symbols\n\nThe Effects of HTML's DRM -- middlemen DRM vendors can say \"no\" to your software playing video.\n\nYouTube Executives Ignored Warnings, Letting Toxic Videos Run Rampant (Bloomberg) -- The company spent years chasing one business goal above others: \u201cEngagement,\u201d a measure of the views, time spent and interactions with online videos. Conversations with over 20 people who work at, or recently left, YouTube reveal a corporate leadership unable or unwilling to act on these internal alarms for fear of throttling engagement. How you incentivize your product managers matters.\n\nMoral Crumple Zones: Cautionary Tales in Human-Robot Interaction -- Just as the crumple zone in a car is designed to absorb the force of impact in a crash, the human in a highly complex and automated system may become simply a component\u2014accidentally or intentionally\u2014that bears the brunt of the moral and legal responsibilities when the overall system malfunctions.\n\n\nCombining Symbols and Statistics So Machines Can Reason About What They See (MIT) -- overview of a paper that combines reasoning (symbols) with perception (statistics). Combining the two is one piece of progressing AI.\n\nContinue reading Four short links: 3 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/eTZNYx6Hmyc/four-short-links-3-april-2019"
 },
 {
  "title": "De-biasing language",
  "content": "The toughest bias problems are often the ones you only think you\u2019ve solved.In a recent paper, Hila Gonen and Yoav Goldberg argue that methods for de-biasing language models aren\u2019t effective; they make bias less apparent, but don\u2019t actually remove it. De-biasing might even make bias more dangerous by hiding it, rather than leaving it out in the open. The toughest problems are often the ones you only think you\u2019ve solved.\n\n\n\nLanguage models are based on \u201cword embeddings,\u201d which are essentially lists of word combinations derived from human language. There are some techniques for removing bias from word embeddings. I won\u2019t go into them in detail, but for the sake of argument, imagine taking the texts of every English language newspaper and replacing \u201che,\u201d \u201cshe,\u201d and other gender-specific words with \u201cthey\u201d or \u201cthem.\u201d (Real techniques are more sophisticated, of course.) What Gonen and Goldberg show is that words still cluster the same way: stereotypically female professions still turn up as closely related (their example is nurse, caregiver, receptionist, and teacher).\n\n\n\nI\u2019m not at all surprised at the result. Stereotypes go deeper than pronouns and other obvious indications of gender, and are deeply embedded in the way we use language. Do nurse, teacher, and caregiver cluster together because they\u2019re all about \u201ccaring,\u201d and do traditionally masculine professions cluster differently? I suspect the connections are much more complex, but something along those lines is probably happening. It\u2019s not a problem if \u201ccaring\u201d professions cluster together\u2014but what about the connections between these words and other words?\n\n\n\nGonen and Goldberg point out that explicit male/female associations aren\u2019t really the issue: \u201calgorithmic discrimination is more likely to happen by associating one implicitly gendered term with other implicitly gendered terms, or by picking up on gender-specific regularities in the corpus by learning to condition on gender-biased words, and generalizing to other gender-biased words (i.e., a resume classifier that will learn to favor male over female candidates based on stereotypical cues in an existing\u2014and biased\u2014resume data set, despite being \u201coblivious\u201d to gender).\u201d That is, an AI that screens job applications for a \u201cprogramming\u201d position could be biased against women without knowing anything explicit about gender; it would just know that \u201cprogrammer\u201d clusters with certain words that happen to appear more often in resumes that come from men.\n\n\n\nSo let\u2019s ask some other difficult questions. Would de-biasing language around race and ethnicity achieve the same (lack of) result? I would like to see that studied; Robyn Speer\u2019s work, described in \u201cHow To Make a Racist AI Without Really Trying,\u201d suggests that de-biasing for race is at least partially successful; though, Speer asks: \u201cCan we stop worrying about algorithmic racism? No. Have we made the problem a lot smaller? Definitely.\u201d I hope she\u2019s right; I\u2019m less convinced now than I was a few months ago. I can certainly imagine racial stereotypes persisting even after bias has been removed. What about anti-semitic language? What about other stereotypes? One researcher I know discovered an enterprise security application that was biased against salespeople, who were considered more likely to engage in risky behavior.\n\n\n\nWe\u2019re uncovering biases that are basic to our use of language. It shouldn\u2019t surprise anyone that these biases are built in to the way we communicate, and that they go fairly deep. We can remove gender as a factor in word embeddings, but that doesn\u2019t help much. Turkish, for example, doesn\u2019t have gendered pronouns, a fact that has revealed gender bias in automated translation, where gender-neutral Turkish sentences are translated as gender-specific English sentences. But no one would claim that gender bias doesn\u2019t exist in Turkish; it\u2019s just encoded differently. Likewise, we can remove race and ethnicity as a factor in word embeddings, but that, at best, leaves us with a smaller problem. Language is only a symptom of bigger issues. These biases are part of what we are, and these word associations, including the associations we\u2019d prefer to disown, are part of what makes language work.\n\n\n\nThe problem we\u2019re facing in natural language processing (as in any application of machine learning) is that fairness is aspirational and forward looking; data can only be historical, and therefore necessarily reflects the biases and prejudices of the past. Learning how to de-bias our applications is progress, but the only real solution is to become better people. That\u2019s more easily said than done; it\u2019s not clear that being more conscious about how we talk will remove these hidden biases, any more than rewriting \u201che\u201d and \u201cshe\u201d as \u201cthey.\u201d And being too conscious of how we talk can easily lead to a constant state of self-censorship that makes conversation impossible, and specifically the kinds of conversations we need to make progress.\n\n\n\nIf there\u2019s any one thing that will remove those biases, it is being more aware of how we act. Returning to Gonem and Goldberg\u2019s study of professional stereotypes: the way to change those problematic word embeddings permanently isn\u2019t to tweak the data, but to make hiring decisions that aren\u2019t based on stereotypes, and to treat the people we hire fairly and with respect, regardless of gender (or race or ethnicity). If we act differently, our language will inevitably change to reflect our actions.\n\n\n\nI am hopeful that machine learning will help us leave behind a biased and prejudiced past, and build a future that\u2019s more fair and equitable. But machine learning won\u2019t make prejudice disappear by forcing us to rely on data when the data itself is biased. And the myth that the apparent abstraction and mathematical rationality of machine learning is unbiased only lends the prestige of math and science to prejudice and stereotype. If machine learning is going to help, we\u2019ll need to understand that progress is incremental, not absolute. Machine learning is a tool, not a magic wand, and it\u2019s capable of being misused. It can hold a mirror to our biases, or it can hide them. Real progress relies on us, and the road forward will be neither simple nor easy.\n\n\n\n\nContinue reading De-biasing language.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/PRwueIogrTo/de-biasing-language"
 },
 {
  "title": "Specialized tools for machine learning development and model governance are becoming essential",
  "content": "Why companies are turning to specialized machine learning tools like MLflow.A few years ago, we started publishing articles (see \u201cRelated resources\u201d at the end of this post) on the challenges facing data teams as they start taking on more machine learning (ML) projects. Along the way, we described a new job role and title\u2014machine learning engineer\u2014focused on creating data products and making data science work in production, a role that was beginning to emerge in the San Francisco Bay Area two years ago. At that time, there weren\u2019t any popular tools aimed at solving the problems facing teams tasked with putting machine learning into practice.\n\n\n\nAbout 10 months ago, Databricks announced MLflow, a new open source project for managing machine learning development (full disclosure: Ben Lorica is an advisor to Databricks). We thought that given the lack of clear open source alternatives, MLflow had a decent chance of gaining traction, and this has proven to be the case. Over a relatively short time period, MLflow has garnered more than 3,300 stars on GitHub and 80 contributors from more than 40 companies. Most significantly, more than 200 companies are now using MLflow.\n\n\n\nSo, why is this new open source project resonating with data scientists and machine learning engineers? Recall the following key attributes of a machine learning project:\n\n\n\tUnlike traditional software where the goal is to meet a functional specification, in ML the goal is to optimize a metric.\n\tQuality depends not just on code, but also on data, tuning, regular updates, and retraining.\n\tThose involved with ML usually want to experiment with new libraries, algorithms, and data sources\u2014and thus, one must be able to put those new components into production.\n\n\n\n\nMLflow\u2019s success can be attributed to a lightweight \u201copen interface\u201d that allows users to hook up their favorite machine learning libraries, and the availability of three components that users can pick and choose from (i.e., they can use one, two, or all three of the following):\n\nFigure 1. Image by Matei Zaharia; used with permission.\n\nThe fact that one can pick and choose which MLflow component(s) to use means the project is able to quickly serve the needs of a diverse set of users. Based on an upcoming survey we conducted of MLflow users, here are some of the most popular use cases:\n\n\n\tTracking and managing large numbers of machine learning experiments: MLflow is useful for an individual data scientist tracking his/her own experiments, but it is also designed to be used by companies with large teams of machine learning developers who are using it to track thousands of models.\n\tMLflow is being used to manage multi-step machine learning pipelines.\n\tModel packaging: companies are using MLflow to incorporate custom logic and dependencies as part of a model\u2019s package abstraction before deploying it to their production environment (example: a recommendation system might be programmed to not display certain images to minors).\n\n\n\n\nThe upcoming 0.9.0 release has many new features, including support for database stores for the MLflow Tracking Server, which will make it easier for large teams to query and track ongoing and past experiments.\n\n\n\nWe are still in the early days for tools supporting teams developing machine learning models. Besides MLflow, there are startups like Comet.ml and Verta.ai that are building tools to ease similar pain points. As software development begins to resemble ML development over the next few years, we expect to see more investments in tools.\n\n\n\nModel governance\n\nCompanies need to look seriously at the improved tools for developing machine learning models, many of which are part of more ambitious tools suites. Machine learning can\u2019t be limited to researchers with Ph.D.s; there aren\u2019t enough of them. Machine learning is in the process of democratization; tools that make it possible for software developers to build and train models are essential to this process.\n\n\n\nWe\u2019ve also said the number of machine learning models that are deployed in production will increase dramatically: many applications will be built from many models, and many organizations will want to automate many different aspects of their business. And those models will age and need to be re-trained periodically. We\u2019ve become accustomed to the need for data governance and provenance, understanding and controlling the many\ndatabases that are combined in a modern data-driven application. We\u2019re now realizing the same is true for models, too. Companies will need to track the models they\u2019re building and the models they have in production.\n\n\n\nStartups like Datatron are beginning to use the term \u201cmodel governance\u201d to describe the task of tracking and managing models, and they are beginning to build model governance tools into their product suites. This term describes the processes that enterprises and large companies are starting to use to understand the many ML initiatives and projects teams are working on. Regulators are also signalling their interest in products that rely on AI and machine learning, thus systems for managing ML development are going to be required to comply with future legislation. Here are some of the elements that are going to play a role in building a model governance solution:\n\n\n\n\n\tA database for authorization and security: who has read/write access to certain models\n\tA catalog or a database that lists models, including when they were tested, trained, and deployed\n\tMetadata and artifacts needed for audits: as an example, the output from the components of MLflow will be very pertinent for audits\n\tSystems for deployment, monitoring, and alerting: who approved and pushed the model out to production, who is able to monitor its performance and receive alerts, and who is responsible for it\n\tA dashboard that provides custom views for all principals (operations, ML engineers, data scientists, business owners)\n\n\n\n\nTraditional software developers have long had tools for managing their projects. These tools serve functions like version control, library management, deployment automation, and more. Machine learning engineers know and use all those tools, but they\u2019re not enough. We\u2019re beginning to see the tool suits that provide the features that machine learning engineers need, including tools for model governance, tracking experiments, and packaging models so that results are repeatable. The next big step in the democratization of machine learning is making it more manageable: not simply hand-crafted artisanal solutions, but solutions that make machine learning manageable and deployable at enterprise scale.\n\n\n\n\n\nRelated resources:\n\n\n\t\n\u201cWhat are machine learning engineers?\u201d: a new role focused on creating data products and making data science work in production\n\t\u201cWhat machine learning means for software development\u201d\n\t\u201cDeep automation in machine learning\u201d\n\t\n\u201cWhat is hardcore data science\u2014in practice?\u201d: the anatomy of an architecture to bring data science into production\n\t\u201cLessons learned turning machine learning models into real products and services\u201d\n\tHarish Doddi on \u201cSimplifying machine learning lifecycle management\u201d\n\n\tJesse Anderson and Paco Nathan on \u201cWhat machine learning engineers need to know\u201d\n\n\u201cData engineers vs. data scientists\u201d\n\nContinue reading Specialized tools for machine learning development and model governance are becoming essential.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4tBUslVoW9c/specialized-tools-for-machine-learning-development-and-model-governance-are-becoming-essential"
 },
 {
  "title": "Four short links: 2 April 2019",
  "content": "Content Moderation, Speech in 1.6kbps, Science is Hard, and Forensic Typography\n\nYour Speech, Their Rules: Meet the People Who Guard the Internet (Medium) -- Adam: \u201cSix months ago we told you, \u2018Don\u2019t pave the city with banana peels.\u2019 You decided, \u2018Let\u2019s see what happens if we pave the city with banana peels.\u2019 We are now here to clean up the injuries.\u201d\n\n\nA Real-Time Wideband Neural Vocoder at 1.6 kb/s Using LPCNet -- this is witchcraft. Skip straight to the demos and have your mind blown. 8kb/s used to be the norm for crappy audio, but this is better quality in 19% of the bandwidth.\n\nStatistically Controlling for Confounding Constructs Is Harder than You Think -- Counterintuitively, we find that error rates are highest\u2014in some cases approaching 100%\u2014when sample sizes are large and reliability is moderate. Our findings suggest that a potentially large proportion of incremental validity claims made in the literature are spurious.\n\n\nForensic DEC CRT Typography -- recreating the real look of a VT100.\n\nContinue reading Four short links: 2 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/xOw7QMc5awQ/four-short-links-2-april-2019"
 },
 {
  "title": "Four short links: 1 April 2019",
  "content": "Communist RuneScape, API Versioning, Computer Graphics, User Stories\n\nThe Communist Revolution inside RuneScape (Emilie R\u0101kete) -- In 2007, a communist RuneScape clan was formed to bring proletarian rule to Server 32 of the world of Gielinor. In a context of scattered clan infighting, the RuneScape communist party was a rampantly victorious social force. Under the wise leadership of SireZaros, the communists waged a revolutionary struggle against reactionary and bourgeois clans that saw more than 5,000 player characters killed in the fighting.\n\n\nBack-end/Front-end Versioning (Christian Findlay) -- A submission can be rejected [from Google/Apple App Store] for any number of reasons, and it can take up to several days for any one submission to reach the store.  On top of this, any user can choose to delay an upgrade, and many users will be on older phones that are not compatible with your current front-end API version. This leaves leaves a situation where front-end versions may be out of sync with each other, or out of sync with the latest back-end version. Here is a quick look at two patterns that might emerge as a strategy to solve the problem.\n\n\nIntroduction to Computer Graphics -- a free, online textbook covering the fundamentals of computer graphics and computer graphics programming.\n\n\nEngineering Guide to Writing User Stories -- the headings are: Using consistent language; Users do not want your stuff; Removing technical details; Clarifying roles; Making user stories verifiable; Spotting the incompleteness; Ranking user stories.\n\n\nContinue reading Four short links: 1 April 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/F0V6r4NdikY/four-short-links-1-april-2019"
 },
 {
  "title": "Four short links: 29 March 2019",
  "content": "Programming Languages, Asset Graphing, Statistical Tests, and Embeddable WebAssembly\n\t\nProgrammer Migration Patterns -- I made a little flow chart of mainstream programming languages and how programmers seem to move from one to another.\n\n\t\ncartography -- a Python tool that consolidates infrastructure assets and the relationships between them in an intuitive graph view powered by a Neo4j database. Video.\n\t\nCommon statistical tests are linear models (or: how to teach stats) -- the linear models underlying common parametric and non-parametric tests. Formulating all the tests in the same language highlights the many similarities between them.\n\n\t\nlucet -- a native WebAssembly compiler and runtime. It is designed to safely execute untrusted WebAssembly programs inside your application.. Open source, from Fastly. Announcement.\n\nContinue reading Four short links: 29 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/7JmTyZu4iZ0/four-short-links-29-march-2019"
 },
 {
  "title": "Highlights from the Strata Data Conference in San Francisco 2019",
  "content": "Watch highlights from expert talks covering AI, machine learning, data analytics, and more.\nPeople from across the data world came together in San Francisco for the Strata Data Conference. Below you'll find links to highlights from the event.\n\nHacking the vote: The neuropolitical universe\n\nElizabeth Svoboda explains how biosensors and predictive analytics are being applied by political campaigns and what they mean for the future of free and fair elections.\n\n\n\tWatch \"Hacking the vote: The neuropolitical universe.\"\n\n\nAI and cryptography: Challenges and opportunities\n\nShafi Goldwasser explains why the next frontier of cryptography will help establish safe machine learning.\n\n\n\tWatch \"AI and cryptography: Challenges and opportunities.\"\n\n\nLikewar: How social media is changing the world and how the world is changing social media\n\nPeter Singer explores the new rules of power in the age of social media and how we can navigate a world increasingly shaped by \"likes\" and lies.\n\n\n\tWatch \"Likewar: How social media is changing the world and how the world is changing social media.\"\n\n\nForecasting uncertainty at Airbnb\n\nTheresa Johnson outlines the AI powering Airbnb\u2019s metrics forecasting platform.\n\n\n\tWatch \"Forecasting uncertainty at Airbnb.\"\n\n\nChatting with machines: Strange things 60 billion bot logs say about human nature\n\nLauren Kunze discusses lessons learned from an analysis of interactions between humans and chatbots.\n\n\n\tWatch \"Chatting with machines: Strange things 60 billion bot logs say about human nature.\"\n\n\nThe journey to the data-driven enterprise from the edge to AI\n\nAmy O'Connor explains how Cloudera applies an \"edge to AI\" approach to collect, process, and analyze data.\n\n\n\tWatch \"The journey to the data-driven enterprise from the edge to AI.\"\n\n\nStreamlining your data assets: A strategy for the journey to AI\n\nDinesh Nirmal shares a data asset framework that incorporates current business structures and the elements you need for an AI-fluent data platform.\n\n\n\tWatch \"Streamlining your data assets: A strategy for the journey to AI.\"\n\n\nScoring your business in the AI matrix\n\nJed Dougherty plots AI examples on a matrix to clarify the various interpretations of AI.\n\n\n\tWatch \"Scoring your business in the AI matrix.\"\n\n\nData warehousing is not a use case\n\nGoogle BigQuery co-creator Jordan Tigani shares his vision for where cloud-scale data analytics is heading.\n\n\n\tWatch \"Data warehousing is not a use case.\"\n\n\nThe enterprise data cloud\n\nMike Olson describes the key capabilities an enterprise data cloud system requires, and why hybrid and multi-cloud is the future.\n\n\n\tWatch \"The enterprise data cloud.\"\n\n\nWinners of the Strata Data Awards 2019\n\nThe Strata Data Award is given to the most disruptive startup, the most innovative industry technology, the most impactful data science project, and the most notable open source contribution.\n\n\n\tWatch \"Winners of the Strata Data Awards 2019.\"\n\n\nContinue reading Highlights from the Strata Data Conference in San Francisco 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/z3IUlkWvDKU/highlights-from-strata-data-conference-sf-2019"
 },
 {
  "title": "Hacking the vote: The neuropolitical universe",
  "content": "Elizabeth Svoboda explains how biosensors and predictive analytics are being applied by political campaigns and what they mean for the future of free and fair elections.Continue reading Hacking the vote: The neuropolitical universe.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4CAdi9Vimy0/hacking-the-vote-the-neuropolitical-universe"
 },
 {
  "title": "The enterprise data cloud",
  "content": "Mike Olson describes the key capabilities an enterprise data cloud system requires, and why hybrid and multi-cloud is the future.Continue reading The enterprise data cloud.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5_mZ0xPeBYI/the-enterprise-data-cloud"
 },
 {
  "title": "Forecasting uncertainty at Airbnb",
  "content": "Theresa Johnson outlines the AI powering Airbnb\u2019s metrics forecasting platform.Continue reading Forecasting uncertainty at Airbnb.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Xa23d1Sb1wI/forecasting-uncertainty-at-airbnb"
 },
 {
  "title": "Likewar: How social media is changing the world and how the world is changing social media.",
  "content": "Peter Singer explores the new rules of power in the age of social media and how we can navigate a world increasingly shaped by \"likes\" and lies.Continue reading Likewar: How social media is changing the world and how the world is changing social media..",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/LcKgDnmEtlk/likewar-how-social-media-is-changing-the-world-and-how-the-world-is-changing-social-media."
 },
 {
  "title": "Data warehousing is not a use case",
  "content": "Google BigQuery co-creator Jordan Tigani shares his vision for where cloud-scale data analytics is heading.Continue reading Data warehousing is not a use case.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/AxnRl-4JCRY/data-warehousing-is-not-a-use-case"
 },
 {
  "title": "Chatting with machines: Strange things 60 billion bot logs say about human nature",
  "content": "Lauren Kunze discusses lessons learned from an analysis of interactions between humans and chatbots. Continue reading Chatting with machines: Strange things 60 billion bot logs say about human nature.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/dCPWMryIifs/chatting-with-machines-strange-things-60-billion-bot-logs-say-about-human-nature"
 },
 {
  "title": "Winners of the Strata Data Awards 2019",
  "content": "The Strata Data Award is given to the most disruptive startup, the most innovative industry technology, the most impactful data science project, and the most notable open source contribution.Continue reading Winners of the Strata Data Awards 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/8F30XRM0WAM/winners-of-the-strata-data-awards-2019"
 },
 {
  "title": "It\u2019s time for data scientists to collaborate with researchers in other disciplines",
  "content": "The O\u2019Reilly Data Show Podcast: Forough Poursabzi Sangdeh on the interdisciplinary nature of interpretable and interactive machine learning.In this episode of the Data Show, I spoke with Forough Poursabzi-Sangdeh, a postdoctoral researcher at Microsoft Research New York City. Poursabzi works in the interdisciplinary area of interpretable and interactive machine learning. As models and algorithms become more widespread, many important considerations are becoming active research areas: fairness and bias, safety and reliability, security and privacy, and Poursabzi\u2019s area of focus\u2014explainability and interpretability.Continue reading It\u2019s time for data scientists to collaborate with researchers in other disciplines.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/1uI0r8M-_do/its-time-for-data-scientists-to-collaborate-with-researchers-in-other-disciplines"
 },
 {
  "title": "Four short links: 28 March 2019",
  "content": "Data-Oriented Design, Time Zone Hell, Music Algorithms, and Fairness in ML\n\nData Oriented Design -- A curated list of data-oriented design resources.\n\n\nStoring UTC is Not a Silver Bullet -- time zones will drive you to drink.\n\nWarner Music Signed an Algorithm to a Record Deal (Verge) -- Although Endel signed a deal with Warner, the deal is crucially not for \u201can algorithm,\u201d and Warner is not in control of Endel\u2019s product. The label approached Endel with a distribution deal and Endel used its algorithm to create 600 short tracks on 20 albums that were then put on streaming services, returning a 50/50 royalty split to Endel. Unlike a typical major label record deal, Endel didn\u2019t get any advance money paid upfront, and it retained ownership of the master recordings.\n\n\n50 Years of Unfairness: Lessons for Machine Learning --  We trace how the notion of fairness has been defined within the testing communities of education and hiring over the past half century, exploring the cultural and social context in which different fairness definitions have emerged. In some cases, earlier definitions of fairness are similar or identical to definitions of fairness in current machine learning research, and foreshadow current formal work. In other cases, insights into what fairness means and how to measure it have largely gone overlooked. We compare past and current notions of fairness along several dimensions, including the fairness criteria, the focus of the criteria (e.g., a test, a model, or its use), the relationship of fairness to individuals, groups, and subgroups, and the mathematical method for measuring fairness (e.g., classification, regression). This work points the way toward future research and measurement of (un)fairness that builds from our modern understanding of fairness while incorporating insights from the past.\n\n\nContinue reading Four short links: 28 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zzCaNyPffqc/four-short-links-28-march-2019"
 },
 {
  "title": "AI and cryptography: Challenges and opportunities",
  "content": "Shafi Goldwasser explains why the next frontier of cryptography will help establish safe machine learning. Continue reading AI and cryptography: Challenges and opportunities.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0J7PxE8tFco/ai-and-cryptography-challenges-and-opportunities"
 },
 {
  "title": "The journey to the data-driven enterprise from the edge to AI",
  "content": "Amy O'Connor explains how Cloudera applies an \"edge to AI\" approach to collect, process, and analyze data.Continue reading The journey to the data-driven enterprise from the edge to AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2lE5nQqPPPA/the-journey-to-data-driven-enterprise-from-the-edge-to-ai"
 },
 {
  "title": "Streamlining your data assets: A strategy for the journey to AI",
  "content": "Dinesh Nirmal shares a data asset framework that incorporates current business structures and the elements you need for an AI-fluent data platform.Continue reading Streamlining your data assets: A strategy for the journey to AI.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/3CYnuvhyFtg/streamining-your-data-assets-a-strategy-for-the-journey-to-ai"
 },
 {
  "title": "Scoring your business in the AI matrix",
  "content": "Jed Dougherty plots AI examples on a matrix to clarify the various interpretations of AI.Continue reading Scoring your business in the AI matrix.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/PUF5eTIOJDc/scoring-your-business-in-the-ai-matrix"
 },
 {
  "title": "Four short links: 27 March 2019",
  "content": "Linkers and Loaders, Low-Low-Low Power Bluetooth, Voice, and NVC\n\nLinkers and Loaders -- the uncorrected manuscript chapters for my Linkers and Loaders, published by Morgan-Kaufman.\n\n\n<1mW Bluetooth LTE Transmitter -- Consuming just 0.6 milliwatts during transmission, it would broadcast for 11 years using a typical 5.8-mm coin battery. Such a millimeter-scale BLE radio would allow these ant-sized sensors to communicate with ordinary equipment, even a smartphone. Ingenious engineering hacks to make this work.\n\nMumble -- an open source, low-latency, high-quality voice chat software primarily intended for use while gaming.\n\n\nA Guide to Difficult Conversations (Dave Bailey) -- your quarterly reminder that non-violent communication exists and is a good thing.\n\nContinue reading Four short links: 27 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/1U9FbaxaEfY/four-short-links-27-march-2019"
 },
 {
  "title": "Four short links: 26 March 2019",
  "content": "Software Stack, Gig Economy, Simple Over Flexible, and Packet Radio\n\nThoughts on Conway's Law and the Software Stack (Jessie Frazelle) -- All these problems are not small by any means. They are miscommunications at various layers of the stack. They are people thinking an interface or feature is secure when it is merely a window dressing that can be bypassed with just a bit more knowledge about the stack. I really like the advice Lea Kissner gave: \u201ctake the long view, not just the broad view.\u201d We should do this more often when building systems.\n\n\nTroubles with the Open Source Gig Economy and Sustainability Tip Jar (Chris Aniszczyk) -- thoughtful long essay with a lot of links for background reading, on the challenges of sustainability via Patreon, etc., through to some signs of possibly-working models.\n\nChoose Simple Solutions Over Flexible Ones -- flexibility does not come for free.\n\nNew Packet Radio (Hackaday) -- a custom radio protocol, designed to transport bidirectional IP traffic over 430MHz radio links (ham radio). This protocol is optimized for \"point to multipoint\" topology, with the help of managed-TDMA.  Note that Hacker News commentors indicate some possible FCC violations; though, as the project comes from France, that's probably not a problem for the creators of the software.\n\nContinue reading Four short links: 26 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/bYYqSKUxKCc/four-short-links-26-march-2019"
 },
 {
  "title": "Four short links: 25 March 2019",
  "content": "Hiring for Neurodiversity, Reprogrammable Molecular Computing, Retro UUCP, and Industrial Go\n\nDell's Neurodiversity Program -- excellent work from Dell making themselves an attractive destination for folks on the autistic spectrum.\n\nReprogrammable Molecular Computing System (Caltech) -- The researchers were able to experimentally demonstrate 6-bit molecular algorithms for a diverse set of tasks. In mathematics, their circuits tested inputs to assess if they were multiples of three, performed equality checks, and counted to 63. Other circuits drew \"pictures\" on the DNA \"scarves,\" such as a zigzag, a double helix, and irregularly spaced diamonds. Probabilistic behaviors were also demonstrated, including random walks as well as a clever algorithm (originally developed by computer pioneer John von Neumann) for obtaining a fair 50/50 random choice from a biased coin. Paper.\n\nDataforge UUCP -- it's like Cory Doctorow guestwrote our timeline: UUCP over SSH to give decentralized comms for freedom fighters.\n\nGo for Industrial Programming (Peter Bourgon) -- I\u2019m speaking today about programming in an industrial context. By that I mean: in a startup or corporate environment; within a team where engineers come and go; on code that outlives any single engineer; and serving highly mutable business requirements. [...] I\u2019ve tried to select for areas that have routinely tripped up new and intermediate Gophers in organizations I\u2019ve been a part of, and particularly those things that may have nonobvious or subtle implications. (via ceej)\n\nContinue reading Four short links: 25 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/UoLBpxN27A8/four-short-links-25-march-2019"
 },
 {
  "title": "Four short links: 22 March 2019",
  "content": "Explainable AI, Product Management, REPL for Games, and Open Source Inventory\n\nXAI -- An explainability toolbox for machine learning. Follows the Ethical Institute for AI & Machine Learning's  8 principles.\n\nThe Producer Playbook -- Guidelines and best practices for producers and project managers.\n\n\nRepl.it Adds Graphics -- PyGame in the browser, in fast turnaround time.\n\nScanCode Toolkit -- detects licenses, copyrights, package manifests and dependencies, and more by scanning code ... to discover and inventory open source and third-party packages used in your code.\n\n\nContinue reading Four short links: 22 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/G9Y57Yvz_VQ/four-short-links-22-march-2019"
 },
 {
  "title": "Automating ethics",
  "content": "Machines will need to make ethical decisions, and we will be responsible for those decisions.We are surrounded by systems that make ethical decisions: systems approving loans, trading stocks, forwarding news articles, recommending jail sentences, and much more. They act for us or against us, but almost always without our consent or even our knowledge. In recent articles, I've suggested the ethics of artificial intelligence itself needs to be automated. But my suggestion ignores the reality that ethics has already been automated: merely claiming to make data-based recommendations without taking anything else into account is an ethical stance. We need to do better, and the only way to do better is to build ethics into those systems. This is a problematic and troubling position, but I don't see any alternative.\n\n\n\nThe problem with data ethics is scale. Scale brings a fundamental change to ethics, and not one that we're used to taking into account. That\u2019s important, but it\u2019s not the point I\u2019m making here. The sheer number of decisions that need to be made means that we can\u2019t expect humans to make those decisions. Every time data moves from one site to another, from one context to another, from one intent to another, there is an action that requires some kind of ethical decision.\n\n\n\nGmail\u2019s handling of spam is a good example of a program that makes ethical decisions responsibly. We\u2019re all used to spam blocking, and we don\u2019t object to it, at least partly because email would be unusable without it. And blocking spam requires making ethical decisions automatically: deciding that a message is spam means deciding what other people can and can\u2019t say, and who they can say it to.\n\n\n\nThere\u2019s a lot we can learn from spam filtering. It only works at scale; Google and other large email providers can do a good job of spam filtering because they see a huge volume of email. (Whether this centralization of email is a good thing is another question.) When their servers see an incoming message that matches certain patterns across their inbound email, that message is marked as spam and sorted into recipients\u2019 spam folders. Spam detection happens in the background; we don\u2019t see it. And the automated decisions aren\u2019t final: you can check the spam folder and retrieve messages that were spammed by mistake, and you can mark messages that are misclassified as not-spam.\n\n\n\nCredit card fraud detection is another system that makes ethical decisions for us. Most of us have had a credit card transaction rejected and, upon calling the company, found that the card had been cancelled because of a fraudulent transaction. (In my case, a motel room in Oklahoma.) Unfortunately, fraud detection doesn\u2019t work as well as spam detection; years later, when my credit card was repeatedly rejected at a restaurant that I patronized often, the credit card company proved unable to fix the transactions or prevent future rejections. (Other credit cards worked.) I\u2019m glad I didn\u2019t have to pay for someone else\u2019s stay in Oklahoma, but an implementation of ethical principles that can\u2019t be corrected when it makes mistakes is seriously flawed.\n\n\n\nSo, machines are already making ethical decisions, and often doing so badly. Spam detection is the exception, not the rule. And those decisions have an increasingly powerful effect on our lives. Machines determine what posts we see on Facebook, what videos are recommended to us on YouTube, what products are recommended on Amazon. Why did Google News suddenly start showing me alt-right articles about a conspiracy to deny Cornell University students\u2019 inalienable right to hamburgers? I think I know; I\u2019m a Cornell alum, and Google News \u201cthought\u201d I\u2019d be interested. But I\u2019m just guessing, and I have precious little control over what Google News decides to show me. Does real news exist if Google or Facebook decides to show me burger conspiracies instead? What does \u201cnews\u201d even mean if fake conspiracy theories are on the same footing? Likewise, does a product exist if Amazon doesn\u2019t recommend it? Does a song exist if YouTube doesn\u2019t select it for your playlist?\n\n\n\nThese data flows go both ways. Machines determine who sees our posts, who receives data about our purchases, who finds out what websites we visit. We\u2019re largely unaware of those decisions, except in the most grotesque sense: we read about (some of) them in the news, but we\u2019re still unaware of how they impact our lives.\n\n\n\nDon\u2019t misconstrue this as an argument against the flow of data. Data flows, and data becomes more valuable to all of us as a result of those flows. But as Helen Nissenbaum argues in her book Privacy in Context, those flows result in changes in context, and when data changes context, the issues quickly become troublesome. I am fine with medical imagery being sent to a research study where it can be used to train radiologists and the AI systems that assist them. I\u2019m not OK with those same images going to an insurance consortium, where they can become evidence of a \u201cpre-existing condition,\u201d or to a marketing organization that can send me fake diagnoses. I believe fairly deeply in free speech, so I\u2019m not too troubled by the existence of conspiracy theories about Cornell\u2019s dining service; but let those stay in the context of conspiracy theorists. Don\u2019t waste my time or my attention.\n\n\n\nI\u2019m also not suggesting that machines make ethical choices in the way humans do: ultimately, humans bear responsibility for the decisions their machines make. Machines only follow instructions, whether those instruction are concrete rules or the arcane computations of a neural network. Humans can\u2019t absolve themselves of responsibility by saying, \u201cThe machine did it.\u201d We are the only ethical actors, even when we put tools in place to scale our abilities.\n\n\n\nIf we\u2019re going to automate ethical decisions, we need to start from some design principles. Spam detection gives us a surprisingly good start. Gmail\u2019s spam detection assists users. It has been designed to happen in the background and not get into the user\u2019s way. That\u2019s a simple but important statement: ethical decisions need to stay out of the user\u2019s way. It\u2019s easy to think that users should be involved with these decisions, but that defeats the point: there are too many decisions, and giving permission each time an email is filed as spam would be much worse than clicking on a cookie notice for every website you visit. But staying out of the user's way has to be balanced against human responsibility: ambiguous or unclear situations need to be called to the users' attention. When Gmail can't decide whether or not a message is spam, it passes it on to the user, possibly with a warning.\n\n\n\nA second principle we can draw from spam filtering is that decisions can\u2019t be irrevocable. Emails tagged as spam aren\u2019t deleted for 30 days; at any time during that period, the user can visit the spam folder and say \u201cthat\u2019s not spam.\u201d In a conversation, Anna Lauren Hoffmann said it\u2019s less important to make every decision correctly than to have a means of redress by which bad decisions can be corrected. That means of redress must be accessible by everyone, and it needs to be human, even though we know humans are frequently biased and unfair. It must be possible to override machine-made decisions, and moving a message out of the spam folder overrides that decision.\n\n\n\nWhen the model for spam detection is systematically wrong, users can correct it. It\u2019s easy to mark a message as \u201cspam\u201d or \u201cnot spam.\u201d This kind of correction might not be appropriate for more complex applications. For example, we wouldn\u2019t want real estate agents \u201ccorrecting\u201d a model to recommend houses based on race or religion; and we could even discuss whether similar behavior would be appropriate for spam detection. Designing effective means of redress and correction may be difficult, and we\u2019ve only dealt with the simplest cases.\n\n\n\nEthical problems arise when a company\u2019s interest in profit comes before the interests of the users. We see this all the time: in recommendations designed to maximize ad revenue via \u201cengagement\u201d; in recommendations that steer customers to Amazon\u2019s own products, rather than other products on their platform. The customer\u2019s interest must always come before the company\u2019s. That applies to recommendations in a news feed or on a shopping site, but also how the customer\u2019s data is used and where it\u2019s shipped. Facebook believes deeply that \u201cbringing the world closer together\u201d is a social good but, as Mary Gray said on Twitter, when we say that something is a \u201csocial good,\u201d we need to ask: \u201cgood for whom?\u201d Good for advertisers? Stockholders? Or for the people who are being brought together? The answers aren\u2019t all the same, and depend deeply on who\u2019s connected and how.\n\n\n\nMany discussions of ethical problems revolve around privacy. But privacy is only the starting point. Again, Nissenbaum clarifies that the real issue isn\u2019t whether data should be private; it\u2019s what happens when data changes context. None of these privacy tools could have protected the pregnant Target customer who was outed to her parents. The problem wasn\u2019t with privacy technology, but with the intention: to use purchase data to target advertising circulars. How can we control data flows so those flows benefit, rather than harm, the user? \"Datasheets for datasets\" is a proposal for a standard way to describe data sets; model cards proposes a standard way to describe models. While neither of these is a complete solution, I can imagine a future version of these proposals that standardizes metadata so data routing protocols can determine which flows are appropriate and which aren't. It\u2019s conceivable that the metadata for data could describe what kinds of uses are allowable (extending the concept of informed consent), and metadata for models could describe how data might be used. That's work that hasn't been started, but it's work that needed.\n\n\n\nWhatever solutions we end up with, we must not fall in love with the tools. It\u2019s entirely too easy for technologists to build some tools and think they\u2019ve solved a problem, only to realize the tools have created their own problems. Differential privacy can safeguard personal data by adding random records to a database without changing its statistical properties, but it can also probably protect criminals by hiding evidence. Homomorphic encryption, which allows systems to do computations on encrypted data without first decrypting it, can probably be used to hide the real significance of computations. Thirty years of experience on the internet has taught us that routing protocols can be abused in many ways; protocols that use metadata to route data safely can no doubt be attacked. It's possible to abuse or to game any solution. That doesn\u2019t mean we shouldn\u2019t build solutions, but we need to build them knowing they aren\u2019t bulletproof, that they\u2019re subject to attack, and that we are ultimately responsible for their behavior.\n\n\n\nOur lives are integrated with data in ways our parents could never have predicted. Data transfers have gone way beyond faxing a medical record or two to an insurance company, or authorizing a credit card purchase over an analog phone line. But as Thomas Wolfe wrote, we can\u2019t go home again. There's no way back to some simpler world where your medical records were stored on paper in your doctor\u2019s office, your purchases were made with cash, and your smartphone didn\u2019t exist. And we wouldn\u2019t want to go back. The benefits of the new data-rich world are immense. Yet, we live in a \"data smog\" that contains everyone's purchases, everyone's medical records, everyone\u2019s location, and even everyone\u2019s heart rate and blood pressure.\n\n\n\nIt's time to start building the systems that will truly assist us to manage our data. These machines will need to make ethical decisions, and we will be responsible for those decisions. We can\u2019t avoid that responsibility; we must take it up, difficult and problematic as it is.\n\n\nContinue reading Automating ethics.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/WAd-Rg7FTOQ/automating-ethics"
 },
 {
  "title": "Four short links: 21 March 2019",
  "content": "Newsletters, Confidence Intervals, Reverse Engineering, and Human Scale\n\nEmail Newsletters: The New Social Media (NYT) -- \u201cWith newsletters, we can rebuild all of the direct connections to people we lost when the social web came along.\u201d\n\n\nScientists Rise Up Against Statistical Significance (Nature) -- want to replace p-values with confidence intervals, which are easier to interpret without special training. Sample intro to p-values and confidence intervals.\n\n\nCutter -- A Qt and C++ GUI for radare2 reverse engineering framework. Its goal is making an advanced, customizable, and FOSS reverse-engineering platform while keeping the user experience in mind. Cutter is created by reverse engineers for reverse engineers. \n\nComputer Latency at a Human Scale -- if a CPU cycle is 1 second, then SSD I/O takes 1.5-4 days, and rotational disk I/O takes 1-9 months. Also in the Hacker News thread, human-scale storage: if a byte is a letter, then a 4kb page of memory is 1 sheet of paper, a 256kb L2 cache is a 64-page binder on the desk, and a 1TB SSD is a warehouse of books.\n\nContinue reading Four short links: 21 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/CjeWe6k-EbA/four-short-links-21-march-2019"
 },
 {
  "title": "The fundamental problem with Silicon Valley\u2019s favorite growth strategy",
  "content": "Our entire economy seems to have forgotten that workers are also consumers, and suppliers are also customers.The pursuit of monopoly has led Silicon Valley astray.\n\nLook no further than the race between Lyft and Uber to dominate the online ride-hailing market. Both companies are gearing up for their IPOs in the next few months. Street talk has Lyft shooting for a valuation between $15 and $30 billion dollars, and Uber valued at an astonishing $120 billion dollars. Neither company is profitable; their enormous valuations are based on the premise that if a company grows big enough and fast enough, profits will eventually follow.\n\nMost monopolies or duopolies develop over time, and have been considered dangerous to competitive markets; now they are sought after from the start and are the holy grail for investors. If LinkedIn co-founder Reid Hoffman and entrepreneur Chris Yeh\u2019s new book Blitzscaling is to be believed, the Uber-style race to the top (or the bottom, depending on your point of view) is the secret of success for today\u2019s technology businesses.\n\nBlitzscaling promises to teach techniques that are \u201cthe lightning-fast path to building massively valuable companies.\u201d Hoffman and Yeh argue that in today\u2019s world, it\u2019s essential to \u201cachieve massive scale at incredible speed\u201d in order to seize the ground before competitors do. By their definition, blitzscaling (derived from the blitzkrieg or \u201clightning war\u201d strategy of Nazi general Heinz Guderian) \u201cprioritizes speed over efficiency,\u201d and risks \u201cpotentially disastrous defeat in order to maximize speed and surprise.\u201d\n\nMany of these businesses depend on network effects, which means the company that gets to scale first is likely to stay on top. So, for startups, this strategy typically involves raising a lot of capital and moving quickly to dominate a new market, even when the company\u2019s leaders may not know how they are going to make money in the long term.\n\nThis premise has become doctrine in Silicon Valley. But is it correct? And is it good for society? I have my doubts.\n\nImagine, for a moment, a world in which Uber and Lyft hadn\u2019t been able to raise billions of dollars in a winner-takes-all race to dominate the online ride-hailing market. How might that market have developed differently?\n\nBlitzscaling isn\u2019t really a recipe for success but rather survivorship bias masquerading as a strategy.\n\nUber and Lyft have developed powerful services that delight their users and are transforming urban transportation. But if they hadn\u2019t been given virtually unlimited capital to offer rides at subsidized prices taxicabs couldn\u2019t match in order to grow their user base at blitzscaling speed, would they be offering their service for less than it actually costs to deliver? Would each company be spending 55% of net revenue on driver incentives, passenger discounts, sales, and marketing to acquire passengers and drivers faster than the other? Would these companies now be profitable instead of hemorrhaging billions of dollars a year? Would incumbent transportation companies have had more time to catch up, leading to a more competitive market? Might drivers have gotten a bigger share of the pie? Would a market that grew more organically\u2014like the web, e-commerce, smartphones, or mobile mapping services\u2014have created more value over the long term?\n\nWe\u2019ll never know, because investors, awash in cheap capital, anointed the winners rather than letting the market decide who should succeed and who should fail. This created a de-facto duopoly long before either company had proven that it has a sustainable business model. And because these two giants are now locked in a capital-fueled deathmatch, the market is largely closed off to new ideas except from within the existing, well-funded companies.\n\nThe case for blitzscaling\n\nThere are plenty of reasons to believe that blitzscaling makes sense. The internet is awash in billionaires who made their fortunes by following a strategy summed up in Mark Zuckerberg\u2019s advice to \u201cmove fast and break things.\u201d Hoffman and Yeh invoke the storied successes of Apple, Microsoft, Amazon, Google, and Facebook, all of whom have dominated their respective markets and made their founders rich in the process, and suggest that it is blitzscaling that got them there. And the book tells compelling tales of current entrepreneurs who have outmaneuvered competitors by pouring on the gas and moving more quickly. Hoffman recalls his own success with the blitzscaling philosophy during the early days of Paypal. Back in 2000, the company was growing 5% per day, letting people settle their charges using credit cards while using the service for free. This left the company to absorb, ruinously, the 3% credit card charge on each transaction. He writes:\n\nI remember telling my old college friend and Paypal co-founder/CEO Peter Thiel, 'Peter, if you and I were standing on the roof of our office and throwing stacks of hundred-dollar bills off the edge as fast as our arms could go, we still wouldn\u2019t be losing money as quickly as we are right now.'\n\nBut it worked out. Paypal built an enormous user base quickly, giving the company enough market power to charge businesses to accept Paypal payments. They also persuaded most customers to make those payments via direct bank transfers, which have much lower fees than credit cards. If they\u2019d waited to figure out the business model, someone else might have beat them to the customer that made them a success: eBay, which went on to buy Paypal for $1.5 billion (which everyone thought was a lot of money in those days), launching Thiel and Hoffman on their storied careers as serial entrepreneurs and investors.\n\nOf course, for every company like Paypal that pulled off that feat of hypergrowth without knowing where the money would come from, there is a dotcom graveyard of hundreds or thousands of companies that never figured it out. That\u2019s the \u201crisks potentially disastrous defeat\u201d part of the strategy that Hoffman and Yeh talk about. A strong case can be made that blitzscaling isn\u2019t really a recipe for success but rather survivorship bias masquerading as a strategy.\n\nHowever, Hoffman and Yeh do a good job of explaining the conditions in which blitzscaling makes sense: The market has to be really big; there has to be a sustainable competitive advantage (e.g., network effects) from getting bigger faster than the competition; you have to have efficient means to bootstrap distribution; and you have to have high gross margins so the business will generate positive cash flow and profits when it does get to scale. This is good management advice for established companies as well as startups, and the book is chock full of it.\n\nHoffman and Yeh also make the point that what most often drives the need for blitzscaling is competition; an entrepreneur with a good idea can be too close to the center of the bullseye, inevitably drawing imitators. The book opens with an excellent tale of how Airbnb used blitzscaling to respond to the threat of a European copycat company by raising money to open and aggressively expand its own European operations years before the company would otherwise have chosen to do so.\n\nBut sometimes it isn\u2019t just the threat of competition that drives the need to turbocharge growth: it\u2019s the size and importance of the opportunity, and the need to get big fast enough to effect change. For example, you can make the case that if Uber and Lyft and Airbnb hadn\u2019t blitzscaled, they would have been tied up in bureaucratic red tape, and the future they are trying to build wouldn\u2019t just have happened more slowly; it would never have happened.\n\nThe strategic use of blitzscaling isn\u2019t limited to startups. It can also apply to large companies, governments, and even nonprofits. For example, we\u2019re facing a blitzscaling opportunity right now at Code for America, the non-profit founded and run by my wife Jennifer Pahlka, and on whose board I serve.\n\nOur mission is to use the principles and practices of the digital age to improve how government serves the American public, and how the public improves government. Since Code for America is a non-profit, we aren\u2019t trying to \u201ctake the market.\u201d There\u2019s no financial imperative to seize an opportunity before someone else does. Our goal is to show what\u2019s possible, to build a constituency and a consensus for a change in the way government does things, and to encourage the development of an ecosystem of new vendors who can work with government the same way we do. By demonstrating that the work of government can be done quickly and cheaply at massive scale using open source software, machine learning, and other 21st-century technology, we look to shape the expectations of the market.\n\nEmulating the tortoise, not the hare, has been our goal. We\u2019ve always preferred opportunities where time is an ally, not an enemy.\n\nSo why is blitzscaling relevant to us? It\u2019s not about making millions and snuffing out the competition\u2014as in many of the most compelling cases for blitzscaling\u2014it\u2019s about building enough momentum to break through the stone walls of an old established order. In our case, we are attempting to save taxpayers money and radically alter the lives of millions of Americans.\n\nHere\u2019s a concrete example: one of the areas we\u2019ve gotten deeply involved in is criminal justice reform. Specifically, we\u2019re helping governments implement new laws and initiatives to redress 30 years of over-aggressive policy that has left almost 70 million Americans with some kind of criminal record and 2.2 million behind bars. (That\u2019s the highest percentage in the world.) A broad consensus is emerging on both left and right that it\u2019s time to rethink our criminal justice system.\n\nToo often, though, those passing new laws have given insufficient thought to their implementation, leaving existing bureaucratic processes in place. For example, to clear a criminal record under 2014\u2019s California Proposition 47, which reduced the penalty for many crimes by reclassifying them as misdemeanors rather than felonies, a person must go to the District Attorney\u2019s office in each jurisdiction where he or she has a record, ask the DA to download their rap sheet, determine eligibility by assessing the obscure codes on the rap sheet, and, if eligible, petition the court for clearance. Facing such a cumbersome, expensive process, only a few thousands of those eligible were able to clear their records.\n\nAfter the passage in 2016 of California Proposition 64, which decriminalized marijuana and added millions to the rolls of those who had criminal records eligible to be expunged, San Francisco District Attorney George Gascon announced a program for automatic expungement. The DA\u2019s office would not wait for petitioners to appear, but would preemptively download and evaluate all eligible records. Unfortunately, lacking technology expertise, Gascon\u2019s office set out to do it with manual labor, hiring paralegals to download and evaluate the rap sheets and file the necessary paperwork with the courts.\n\nWhen we demonstrated that we could download the records in bulk and automate the evaluation of rap sheets, working through thousands of records in minutes and automatically generating the paperwork for clearance, they were all in. True automatic expungement looks like a real possibility. Now we aim to scale up our team to support the entire state in this ambitious program.\n\nSo what\u2019s the rush? The first reason for urgency is the human toll we can alleviate by getting the work done more quickly. When people can clear their records, it gives them better access to jobs, subsidized housing, and many other benefits.\n\nThe second reason is that many other states are also reducing sentences and pushing for record clearance. While we\u2019ve already got our Clear My Record project well underway in California, other states are turning to legacy vendors working through legacy procurement processes. If existing vendors exploit this opportunity and persuade states to sign traditional contracts before we show how cheaply and effectively the job can be done, millions of dollars in public money may be wasted doing it the old way, and years of delay in implementation are at stake. (These contracts typically cost hundreds of millions of dollars and take years to deliver on.)\n\nSo we\u2019re asking ourselves, is it enough to show what\u2019s possible and hope that others do the right thing? Or might we get to our desired outcome more effectively if we scale up our own capability to address the problem? The key question we are wrestling with is \u201chow can we move faster?\u201d\u2014which is exactly the question that Hoffman and Yeh\u2019s book seeks to answer.\n\nIn short, there are compelling reasons to blitzscale, and the book provides a great deal of wisdom for those facing a strategic inflection point where success depends on moving much faster. But I worry that the book oversells the idea, and that too many entrepreneurs will believe this is the only way to succeed.\n\nWhy I\u2019m skeptical of blitzscaling\n\nTo understand why I\u2019m skeptical about blitzscaling, you have to understand a bit about my own entrepreneurial history. I started my company, O\u2019Reilly Media, 40 years ago with a $500 initial investment. We took in no venture capital, but despite that have built a business that generates hundreds of millions of dollars of profitable annual revenue. We got there through steady, organic growth, funded by customers who love our products and pay us for them.\n\nEmulating the tortoise, not the hare, has been our goal. We\u2019ve always preferred opportunities where time is an ally, not an enemy. That\u2019s not to say that we haven\u2019t had our share of blitzscaling opportunities, but in each of them, we kickstarted a new market and then let others take the baton.\n\nIn 1993, my company launched GNN, the Global Network Navigator, which was the first advertising-supported site on the World Wide Web, and the first web portal. We were so early that we had to persuade the world that advertising was the natural business model for this new medium. We plowed every penny we were making from our business writing and publishing computer-programming manuals into GNN\u2014our own version of throwing hundred dollar bills off the rooftop. And for two years, from 1993 until we sold GNN to AOL in mid-1995, we were the place where people went to go to find new websites.\n\nAs the commercial web took off, however, it became clear that we couldn\u2019t play by the same rules as we had in the publishing market, where a deep understanding of what customers were looking for, superior products, innovative marketing, and fair prices gave us all the competitive advantages we needed. Here, the market was exploding, and unless we could grow as fast or faster than the market, we\u2019d soon be left behind. And we could see the only way to do that would be to take in massive amounts of capital, with the price of chasing the new opportunity being the loss of control over the company.\n\nWanting to build a company that I would own and control for the long term, I decided instead to spin out GNN and sell it to AOL. Jerry Yang and David Filo made a different decision at Yahoo!, founded a year after GNN. They took venture capital, blitzscaled their company, and beat AOL to the top of the internet heap\u2014before being dethroned in their turn by Google.\n\nIt happened again in 1996 when O\u2019Reilly launched a product called Website, the first commercial Windows-based web server. We\u2019d been drawn to the promise of a web in which everyone was a publisher; instead, websites were being built on big centralized servers, and all most people could do was consume web content via ubiquitous free browsers. So we set out to democratize the web, with the premise that everyone who had a browser should also be able to have a server. Website took off, and became a multimillion-dollar product line for us. But our product was soon eclipsed by Netscape, which had raised tens of millions of dollars of venture capital, and eventually had a multibillion-dollar IPO\u2014before being crushed in turn by Microsoft.\n\nIn the case of both GNN and Website, you can see the blitzscaling imperative: a new market is accelerating, and there is a clear choice between keeping up and being left behind. In those two examples, I made a conscious choice against blitzscaling because I had a different vision for my company. But far too many entrepreneurs don\u2019t understand the choice, and are simply overtaken by better-funded competitors who seize the opportunity more boldly. And in too many markets, in the absence of antitrust enforcement, there is always the risk that no matter how much money you raise and how fast you go, the entrenched giants will be able to leverage their existing business to take over the new market you\u2019ve created. That\u2019s what Microsoft did to Netscape, and what Facebook did to Snapchat.\n\nHad we followed Hoffman and Yeh\u2019s advice, we would have taken on a contest we were very unlikely to win, no matter how much money we raised or how fast we moved. And even though we abandoned these two opportunities when the blitzscalers arrived, O\u2019Reilly Media has had enormous financial success, becoming a leader in each of our chosen markets.\n\nWinning at all costs\n\nThere\u2019s another point that Hoffman and Yeh fail to address. It matters what stories we tell ourselves about what success looks like. Blitzscaling can be used by any company, but it can encourage a particular kind of entrepreneur: hard-charging, willing to crash through barriers, and often ruthless.\n\nWe see the consequences of this selection bias in the history of the on-demand ride-hailing market. Why did Uber emerge the winner in the ride-hailing wars? Sunil Paul, the founder of Sidecar, was the visionary who came up with the idea of a peer-to-peer taxi service provided by people using their own cars. Logan Green, the co-founder of Lyft, was the visionary who had set out to reinvent urban transportation by filling all the empty cars on the road. But Travis Kalanick, the co-founder and CEO of Uber, was the hyper-aggressive entrepreneur who raised money faster, broke the rules more aggressively, and cut the most corners in the race to the top.\n\nIn 2000, a full eight years before Uber was founded, Sunil Paul filed a patent that described many of the possibilities that the newly commercialized GPS capabilities would provide for on-demand car sharing. He explored founding a company at that time, but realized that GPS-enabled phones weren\u2019t common enough. It was just too early for his ideas to take hold.\n\nThe market Paul had envisioned began, in fits and starts, around 2007. That year, Logan Green and John Zimmer, the founders of Lyft, started a company called Zimride that was inspired by the bottom-up urban jitneys Green had fallen in love with during a trip to Zimbabwe. They began with a web app to match up college students making long-distance trips with others going in the same direction. In 2008, Garrett Camp and Travis Kalanick founded Uber as a high-end service using SMS to summon black-car drivers.\n\nNeither Zimride nor Uber had yet realized the full idea we now think of as smartphone-enabled ride hailing, and each was working toward it from a different end\u2014peer-to-peer, and mobile on-demand respectively. The two ideas were about to meet in an explosive combination, fueled by the wide adoption of GPS-enabled smartphones and app marketplaces. Following the 2007 introduction of the iPhone and, at least as importantly, the 2008 introduction of the iPhone App Store, the iPhone became a platform for mobile applications.\n\nOnce again, it was Paul who first saw the future. Inspired by Airbnb\u2019s success in getting ordinary people to rent out their homes, he realized people might also be willing to rent out their cars. He worked on several versions of this idea. In 2009, while working with the founders of what became Getaround in a class he was teaching at Singularity University, Paul explored peer-to-peer fractional car rental. Then, in 2012, he launched a new company, Sidecar, to provide the equivalent of peer-to-peer taxi service, with ordinary people providing the service using their own cars. He set out to get permission from regulatory agencies for this new approach.\n\nThere are few small wins for the entrepreneur; only the big bets pay off. And, as in Las Vegas, only the house always wins.\n\nGreen and Zimmer heard about Paul\u2019s work on Sidecar and realized immediately that this model could help them realize their original vision for Zimride. They pivoted quickly from their original vision, launching Lyft as a project within Zimride about three months after Sidecar. When Lyft took off, they sold the original Zimride product and went all-in on the new offering. (That\u2019s blitzscaling for you. Seize the ground first.)\n\nUber was an even more aggressive blitzscaler. Hearing about Lyft\u2019s plans, Uber announced UberX, its down-market version of Uber using ordinary people driving their own cars instead of chauffeurs with limousines, the day before Lyft launched, even though all that it had developed in the way of a peer-to-peer driver platform was a press release. In fact, Kalanick, the co-founder and CEO, had been skeptical about the legality of the peer-to-peer driver model, telling Jason Calacanis, the host of the podcast This Week in Startups, \u201cIt would be illegal.\u201d\n\nAnd the race was on. Despite his earlier reservations about the legality of the model, Uber out-executed its smaller rivals, in part by ignoring regulation while they attempted to change the rules, and became the market leader. Uber also followed the blitzscaling playbook more closely, raising far more money than its rivals, and growing far faster. Lyft managed to become a strong number two. But by 2015, Sidecar was a footnote in history, going out of business after having raised only $35.5 million to Uber\u2019s $7.3 billion and Lyft\u2019s $2 billion. To date, Uber has raised a total of $24.3 billion, and Lyft $4.9 billion.\n\nHoffman and Yeh embrace this dark pattern as a call to action. Early in their book, Blake, the cynical sales manager played by Alec Baldwin in the movie Glengarry Glen Ross, appears as an oracle dispensing wise advice:\n\nAs you all know, first prize is a Cadillac Eldorado. Anyone wanna see second prize? Second prize is a set of steak knives. Third prize is you\u2019re fired. Get the picture?\n\nIn the real world, though, while Sunil Paul\u2019s company went out of business, it was Travis Kalanick of Uber who got fired. Stung by scandal after scandal as Uber deceived regulators, spied on passengers, and tolerated a culture of sexual harassment, the board eventually asked for Kalanick\u2019s resignation. Not only that, Uber\u2019s worldwide blitzscaling attempts\u2014competing in ride-hailing not only with Lyft in the U.S. but with Didi in China and with Grab and Ola in Southeast Asia, and with Google on self-driving cars\u2014eventually spread the company too thin, just as Guderian\u2019s blitzkrieg techniques, which had worked so well against France and Poland, failed during the invasion of Russia.\n\nMeanwhile, the forced bloom of Uber\u2019s market share lead became a liability even in the U.S. Even though Uber had far more money, the price war between the two companies cost Uber far more in markets where its share was large and Lyft\u2019s was small. Lyft focused on the U.S. market and began to chip away at Uber\u2019s early lead. It also made significant gains on Uber as passengers and drivers, stung by the sense that Uber was an amoral company, began to abandon the service. Uber is still the larger and more valuable company, and Dara Khosrowshahi, the new CEO, has made enormous progress in stabilizing its business and restoring its reputation. But Lyft\u2019s gains appear to be sustainable.\n\nBlitzscaling\u2014or sustainable scaling?\n\nWhile Hoffman and Yeh\u2019s book claims that companies like Google, Facebook, Microsoft, Apple, and Amazon are icons of the blitzscaling approach, this idea is plausible only with quite a bit of revisionist history. Each of these companies achieved profitability (or in Amazon\u2019s case, positive cash flow) long before its IPO, and growth wasn\u2019t driven by a blitzkrieg of spending to acquire customers below cost but by breakthrough products and services, and by strategic business model innovations that were rooted in a future the competition didn\u2019t yet understand. These companies didn\u2019t blitzscale; they scaled sustainably.\n\nGoogle raised only $36 million before its IPO\u2014an amount that earned Sidecar\u2019s Sunil Paul the dismal third prize of going out of business. For that same level of investment, Google was already hugely profitable.\n\nFacebook\u2019s rise to dominance was far more capital-intensive than Google\u2019s. The company raised $2.3 billion before its IPO, but it too was already profitable long before it went public; according to insiders, it ran close to breakeven from fairly early in its life. The money raised was strategic, a way of hedging against risk, and of stepping up the valuation of the company while delaying the scrutiny of a public offering. As Hoffman and Yeh note in their book, in today\u2019s market, \u201cEven if the money doesn\u2019t prove to be necessary, a major financing round can have positive signaling effects\u2014it helps convince the rest of the world that your company is likely to emerge as the market leader.\u201d\n\nEven Amazon, which lost billions before achieving profitability, raised only $108 million in venture capital before its IPO. How was this possible? Bezos realized his business generated enormous positive cash flow that he could borrow against. It was his boldness in taking the risk of borrowing billions (preserving a larger ownership stake for himself and his team than if he had raised billions in equity), not just Amazon\u2019s commitment to growth over profits, that helped make him the world\u2019s richest man today.\n\nWinners-take-all is an investment philosophy perfectly suited for our age of inequality and economic fragility.\n\nIn short, none of these companies (except arguably Amazon) followed the path that Hoffman and Yeh lay out as a recipe for today\u2019s venture-backed companies. Venture-backed blitzscaling was far less important to their success than product and business-model innovation, brilliant execution, and relentless strategic focus. Hypergrowth was the result rather than the cause of these companies\u2019 success.\n\nIronically, Hoffman and Yeh\u2019s book is full of superb practical advice about innovation, execution, and strategic focus, but it\u2019s wrapped in the flawed promise that startups can achieve similar market dominance as these storied companies by force-feeding inefficient growth.\n\nFor a company like Airbnb, a company with both strong network effects and a solid path to profitability, blitzscaling is a good strategy. But blitzscaling also enables too many companies like Snap, which managed to go public while still losing enormous amounts of money, making its founders and investors rich while passing on to public market investors the risk that the company will never actually become a profitable business. Like Amazon and Airbnb, some of these companies may become sustainable, profitable businesses and grow into their valuation over time, but as of now, they are still bleeding red ink.\n\nSustainability may not actually matter, though, according to the gospel of blitzscaling. After all, the book\u2019s marketing copy does not promise the secret of building massively profitable or enduring companies, but merely \u201cmassively valuable\u201d ones.\n\nWhat is meant by value? To too many investors and entrepreneurs, it means building companies that achieve massive financial exits, either by being sold or going public. And as long as the company can keep getting financing, either from private or public investors, the growth can go on.\n\nBut is a business really a business if it can\u2019t pay its own way?\n\nIs it a business or a financial instrument?\n\nBenjamin Graham, the father of value investing, is widely reported to have said: \u201cIn the short run, the market is a voting machine. In the long run, it\u2019s a weighing machine.\u201d That is, in the short term, investors vote (or more accurately, place bets) on the present value of the future earnings of a company. Over the long term, the market discovers whether they were right in their bets. (That\u2019s the weighing machine.)\n\nBut what is happening today is that the market has almost entirely turned into a betting machine. Not only that, it\u2019s a machine for betting on a horse race in which it\u2019s possible to cash your winning ticket long before the race has actually finished. In the past, entrepreneurs got rich when their companies succeeded and were able to sell shares to the public markets. Increasingly, though, investors are allowing insiders to sell their stock much earlier than that. And even when companies do reach the point of a public offering, these days, many of them still have no profits.\n\nAccording to University of Florida finance professor Jay Ritter, 76% of all IPOs in 2017 were for companies with no profits. By October 2018, the percentage was 83%, exceeding even the 81% seen right before the dotcom bust in 2000.\n\n\n\nWould profitless companies with huge scale be valued so highly in the absence of today\u2019s overheated financial markets?\n\nToo many of the companies likely to follow Hoffman and Yeh\u2019s advice are actually financial instruments instead of businesses, designed by and for speculators. The monetization of the company is sought not via the traditional means of accumulated earnings and the value of a continuing business projecting those earnings into the future, but via the exit, that holy grail of today\u2019s Silicon Valley. The hope is that either the company will be snapped up by another company that does have a viable business model but lacks the spark and sizzle of internet-fueled growth, or will pull off a profitless IPO, like Snap or Box.\n\nThe horse-race investment mentality has a terrible side effect: companies that are not contenders to win, place, or show are starved of investment. Funding dries up, and companies that could have built a sustainable niche if they\u2019d grown organically go out of business instead. \u201cGo big or go home\u201d results in many companies that once would have been members of a thriving business ecosystem indeed going home. As Hoffman and Yeh put it:\n\nHere is one of the ruthless practices that has helped make Silicon Valley so successful: investors will look at a company that is on an upward trajectory but doesn\u2019t display the proverbial hockey stick of exponential growth and conclude that they need to either sell the business or take on additional risk that might increase the chances of achieving exponential growth... Silicon Valley venture capitalists want entrepreneurs to pursue exponential growth even if doing so costs more money and increases the chances that the business will fail.\n\nBecause this blitzscaling model requires raising ever more money in pursuit of the hockey stick venture capitalists are looking for, the entrepreneur\u2019s ownership is relentlessly diluted. Even if the company is eventually sold, unless the company is a breakout hit, most of the proceeds go to investors whose preferred shares must be repaid before the common shares owned by the founders and employees get anything. There are few small wins for the entrepreneur; only the big bets pay off. And, as in Las Vegas, the house always wins.\n\nBryce Roberts, my partner at O\u2019Reilly AlphaTech Ventures (OATV), recently wrote about the probability of winning big in business:\n\n\nTimely reminder that the VCs aren\u2019t even in the home run business.\n\nThey\u2019re in the grand slam business.\n\nInterestingly, odds of hitting a grand slam (.07%) are uncannily similar to odds of backing a unicorn (.07% of VC backed startups) https://t.co/O0VgCeuAe3\n\u2014indievc (@indievc) December 20, 2018\n\n\nThis philosophy has turned venture capitalists into movie studios, financing hundreds of companies in pursuit of the mega-hit that will make their fund, and at its worst turns entrepreneurs into the equivalent of Hollywood actors, moving from one disposable movie to another. (\u201cThe Uber of Parking\u201d is sure to be a hit! And how about \u201cthe Uber of Dry Cleaning\u201d?)\n\nThe losses from the blitzscaling mentality are felt not just by entrepreneurs but by society more broadly. When the traditional venture-capital wisdom is to shutter companies that aren\u2019t achieving hypergrowth, businesses that would once have made meaningful contributions to our economy are not funded, or are starved of further investment once it is clear that they no longer have a hope of becoming a home run.\n\nWinners-take-all is an investment philosophy perfectly suited for our age of inequality and economic fragility, where a few get enormously rich, and the rest get nothing. In a balanced economy, there are opportunities for success at all scales, from the very small, through successful mid-size companies, to the great platforms.\n\nIs Glengarry Glen Ross\u2019s sales competition really the economy we aspire to?\n\nThere is another way\n\nThere are business models, even in the technology sector, where cash flow from operations can fund the company, not venture capitalists gripping horse-race tickets.\n\nConsider these companies: Mailchimp, funded by $490 million in annual revenue from its 12 million customers, profitable from day one without a penny of venture capital; Atlassian, bootstrapped for eight years before raising capital in 2010 after it had reached nearly $75 million in self-funded annual revenue; and Shutterstock, which took in venture capital only after it had already bootstrapped its way to becoming the largest subscription-based stock photo agency in the world. (In the case of both Atlassian and Shutterstock, outside financing was a step toward liquidity through a public offering, rather than strictly necessary to fund company growth.) All of these companies made their millions through consumer-focused products and patience, not blitzscaling.\n\nJason Fried and David Heinemeier Hansson, the founders of Basecamp, a 20-year-old, privately held, profitable Chicago company whose core product is used by millions of software developers, have gone even further: they entirely abandoned the growth imperative, shedding even successful products to keep their company under 50 people. Their book about their approach, It Doesn\u2019t Have to Be Crazy At Work, should be read as an essential counterpoint to Blitzscaling.\n\nAnother story of self-funded growth I particularly like is far from tech. RxBar, a Chicago-based nutrition bar company with $130 million of self-funded annual revenue, was acquired last year by Kellogg for $600 million. Peter Rahal, one of the two founders, recalls that he and co-founder Jared Smith were in his parents\u2019 kitchen, discussing how they would go about raising capital to start their business. His immigrant father said something like, \u201cYou guys need to shut the fuck up and just sell a thousand bars.\u201d\n\nAnd that\u2019s exactly what they did, putting in $5,000 each, and hustling to sell their bars via Crossfit gyms. It was that hustle and bias toward customers, rather than outside funding, that got them their win. Their next breakthrough was in their distinctive \u201cNo BS\u201d packaging, which made the ingredients, rather than extravagant claims about them, the centerpiece of the message.\n\nThe exit for RxBar, when it came, was not the objective, but a strategy for growing a business that was already working. \u201cJared and I never designed the business to sell it; we designed it to solve a problem for customers,\u201d Rahal recalled. \u201cIn January 2017, Jared and I were sitting around and asked what do we want to do with this business? Do we want to continue and make it a family business? Or do we want to roll it up into a greater company, really scale this thing and take it to the next level? We wanted to go put fire on this thing.\u201d\n\nThey could have raised growth capital at that point, like Mike Cannon-Brooks of Atlassian or Jon Oringer of Shutterstock did, but acquisition provided a better path to sustainable impact. Kellogg brought them not just an exit, but additional capabilities to grow their business. Rahal continues to lead the brand at Kellogg, still focusing on customers.\n\nRaise less, own more\n\nThe fact that the Silicon Valley blitzscaling model is not suited for many otherwise promising companies has led a number of venture capitalists, including my partner Bryce Roberts at OATV, to develop an approach for finding, inspiring, and financing cash-flow positive companies.\n\nIndie.vc, a project at OATV, has developed a new kind of venture financing instrument. It\u2019s a convertible loan designed to be repaid out of operating cash flow rather than via an exit, but that can convert to equity if the company, having established there is a traditional venture business opportunity, decides to go that route. This optionality effectively takes away the pressure for companies to raise ever more money in pursuit of the hypergrowth that, as Hoffman and Yeh note, traditional venture capitalists are looking for. The program also includes a year of mentorship and networking, providing access to experienced entrepreneurs and experts in various aspects of growing a business.\n\nIn the Indie.vc FAQ, Bryce wrote:\n\nWe believe deeply that there are hundreds, even thousands, of businesses that could be thriving, at scale, if they focused on revenue growth over raising another round of funding. On average, the companies we\u2019ve backed have increased revenues over 100% in the first 12 months of the program and around 300% after 24 months post-investment. We aim to be the last investment our founders NEED to take. We call this Permissionless Entrepreneurship.\n\nThis is a bit like the baseball scouting revolution that Michael Lewis chronicled in Moneyball. While all the other teams were looking for home-run hitters, Oakland A\u2019s manager Billy Beane realized that on-base percentage was a far more important statistic for actually winning. He took that insight all the way from the league basement to the playoffs, winning against far richer teams despite the A\u2019s own low-salary budget.\n\nOne result of an investment model looking for the equivalent of on-base percentage\u2014that is, the ability to deliver a sustainable business for as little money as possible\u2014is that many entrepreneurs can do far better than they can in the VC blitzscale model. They can build a business that they love, like I did, and continue to operate it for many years. If they do decide to exit, they will own far more of the proceeds.\n\nEven successful swing-for-the fences VCs like Bill Gurley of Benchmark Capital agree. As Gurley, an early Uber investor and board member, tweeted recently:\n\n\n100% agree with this article, & have voiced this opinion my whole career. The vast majority of entrepreneurs should NOT take venture capital. Why? Article nails it: it is a binary \"swing for the fences\" exercise. Bootstrapping more likely to lead to individual financial success. https://t.co/s1mAOKwz6m\n\u2014 Bill Gurley (@bgurley) January 11, 2019\n\n\nIndie.vc\u2019s search for profit-seeking rather than exit-seeking companies has also led to a far more diverse venture portfolio, with more than half of the companies led by women and 20% by people of color. (This is in stark contrast to traditional venture capital, where 98% of venture dollars go to men.) Many are from outside the Bay Area or other traditional venture hotbeds. The 2019 Indie.vc tour, in which Roberts looks for startups to join the program, hosts stops in Kansas City, Boise, Detroit, Denver, and Salt Lake City, as well as the obligatory San Francisco, Seattle, New York, and Boston.\n\nWhere conventional startup wisdom would suggest that aiming for profits, not rounds of funding, will lead to plodding growth, many of our Indie.vc companies are growing just as fast as those from the early-stage portfolios in our previous OATV funds.\n\nNice Healthcare is a good example. Its founder, Thompson Aderinkomi, had been down the traditional blitzscaling path with his prior venture and wanted to take a decidedly different approach to funding and scaling his new business. Seven months post-investment by Indie.vc, Nice was able to achieve 400% revenue growth, over $1 million in annual recurring revenue, and is now profitable. All while being run by a black founder in Minneapolis. Now that\u2019s a real unicorn! Some of the other fast-growing companies in the Indie.VC portfolio include The Shade Room, Fohr, Storq, re:3d, and Chopshop.\n\nOATV has invested in its share of companies that have gone on to raise massive amounts of capital\u2014Foursquare, Planet, Fastly, Acquia, Signal Sciences, Figma, and Devoted Health for example\u2014but we\u2019ve also funded companies that were geared toward steady growth, profitability, and positive cash flow from operations, like Instructables, SeeClickFix, PeerJ, and OpenSignal. In our earlier funds, though, we were trying to shoehorn these companies into a traditional venture model when what we really needed was a new approach to financing. So many VCs throw companies like these away when they discover they aren\u2019t going to hit the hockey stick. But Roberts kept working on the problem, and now his approach to venture capital is turning into a movement.\n\nA recent New York Times article, \u201cMore Startups Have an Unfamiliar Message for Venture Capitalists: Get Lost,\u201d describes a new crop of venture funds with a philosophy similar to Indie.vc. Some entrepreneurs who were funded using the old model are even buying out their investors using debt, like video-hosting company Wistia, or their own profits, like social media management company Buffer.\n\nSweet Labs, one of OATV\u2019s early portfolio companies, has done the same. With revenues in the high tens of millions, the founders asked themselves why they should pursue risky hypergrowth when they already had a great business they loved and that already had enough profit to make them rich. They offered to buy out their investors at a reasonable multiple of their investment, and the investors agreed, giving back control over the company to its founders and employees. What Indie.vc has done is to build in this optionality from the beginning, reminding founders that an all-or-nothing venture blitzscale is not their only option.\n\nThe responsibility of the winners\n\nI\u2019ve talked so far mainly about the investment distortions that blitzscaling introduces. But there is another point I wish Hoffman and Yeh had made in their book.\n\nAssume for a moment that blitzscaling is indeed a recipe for companies to achieve the kind of market dominance that has been achieved by Apple, Amazon, Facebook, Microsoft, and Google. Assume that technology is often a winner-takes-all market, and that blitzscaling is indeed a powerful tool in the arsenal of those in pursuit of the win.\n\nWhat is the responsibility of the winners? And what happens to those who don\u2019t win?\n\nWe live in a global, hyperconnected world. There is incredible value to companies that operate at massive scale. But those companies have responsibilities that go with that scale, and one of those responsibilities is to provide an environment in which other, smaller companies and individuals can thrive. Whether they got there by blitzscaling or other means, many of the internet giants are platforms, something for others to build on top of. Bill Gates put it well in a conversation with Chamath Palihapitiya when Palihapitiya was the head of platform at Facebook: \u201cA platform is when the economic value of everybody that uses it exceeds the value of the company that creates it.\u201d\n\nFor every company that pulled off that feat of hypergrowth, there is a dotcom graveyard of hundreds of companies that never figured it out.\n\nThe problem with the blitzscaling mentality is that a corporate DNA of perpetual, rivalrous, winner-takes-all growth is fundamentally incompatible with the responsibilities of a platform. Too often, once its hyper-growth period slows, the platform begins to compete with its suppliers and its customers. Gates himself faced (and failed) this moral crisis when Microsoft became the dominant platform of the personal computer era. Google is now facing this same moral crisis, and also failing.\n\nWindows, the web, and smartphones such as the iPhone succeeded as platforms because a critical mass of third-party application developers added value far beyond what a single company, however large, could provide by itself. Nokia and Microsoft were also-rans in the smartphone platform race not just because they couldn\u2019t get customers to buy their phones, but because they couldn\u2019t get enough developers to build applications for them. Likewise, Uber and Lyft need enough drivers to pick people up within a few minutes, wherever they are and whenever they want a ride, and enough passengers to keep all their drivers busy. Google search and Amazon commerce succeed because of all that they help us find or buy from others. Platforms are two-sided marketplaces that have to achieve critical mass on both the buyer and the seller sides.\n\nYet despite the wisdom Gates expressed in his comments to Palihapitiya about the limitations of Facebook as a platform, he clearly didn\u2019t go far enough in understanding the obligations of a platform owner back when he was Microsoft\u2019s CEO.\n\nMicrosoft was founded in 1975, and its operating systems\u200a\u2014first MS-DOS, and then Windows\u2014became the platform for a burgeoning personal computer industry, supporting hundreds of PC hardware companies and thousands of software companies. Yet one by one, the most lucrative application categories\u2014word processing, spreadsheets, databases, presentation software\u2014came to be dominated by Microsoft itself.\n\nOne by one, the once-promising companies of the PC era\u2014Micropro, Ashton-Tate, Lotus, Borland\u2014went bankrupt or were acquired at bargain-basement prices. Developers, no longer able to see opportunity in the personal computer, shifted their attention to the internet and to open source projects like Linux, Apache, and Mozilla. Having destroyed all its commercial competition, Microsoft sowed the dragon\u2019s teeth, raising up a new generation of developers who gave away their work for free, and who enabled the creation of new kinds of business models outside Microsoft\u2019s closed domain.\n\nThe government also took notice. When Microsoft moved to crush Netscape, the darling of the new internet industry, by shipping a free browser as part of its operating system, it had gone too far. In 1994, Microsoft was sued by the U.S. Department of Justice, signed a consent decree that didn\u2019t hold, and was sued again in 1998 for engaging in anti-competitive practices. A final settlement in 2001 gave enough breathing room to the giants of the next era, most notably Google and Amazon, to find their footing outside Microsoft\u2019s shadow.\n\nThat story is now repeating itself. I recently did an analysis of Google\u2019s public filings since its 2004 IPO. One of the things those filings report is the share of the ad business that comes from ads on Google\u2019s own properties (Google Ads) versus from ads it places on its partner sites (AdSense). While Google has continued to grow the business for its partners, the company has grown its own share of the market far, far faster. As shown on the chart below, when Google went public in 2004, 51% of ad revenue came from Google\u2019s own search engine while 49% came from ads on third-party websites served up by Google. But by 2017, revenue from Google properties was up to 82%, with only 18% coming from ads on third-party websites.\n\n\n\nWhere once advertising was relegated to a second-class position on Google search pages, it now occupies the best real estate. Ads are bigger, they now appear above organic results rather than off to the side, and there are more of them included with each search. Even worse, organic clicks are actually disappearing. In category after category\u2014local search, weather, flights, sports, hotels, notable people, brands and companies, dictionary and thesaurus, movies and TV, concerts, jobs, the best products, stock prices, and more\u2014Google no longer sends people to other sites: it provides the information they are looking for directly in Google. This is very convenient for Google\u2019s users, and very lucrative for Google, but very bad for the long-term economic health of the web.\n\nIn a recent talk, SEO expert Rand Fishkin gave vivid examples of the replacement of organic search traffic with \u201cno click\u201d searches\u00a0(especially on mobile) as Google has shifted from providing links to websites to providing complete answers on the search page itself. Fishkin\u2019s statistical view is even more alarming than his anecdotal evidence. He claims that in February 2016, 58% of Google searches on mobile resulted in organic clicks, and 41% had no clicks. (Some of these may have been abandoned searches, but most are likely satisfied directly in the Google search results.) By February 2018, the number of organic clicks had dropped to 39%, and the number of no click searches had risen to 61%. It isn\u2019t clear what proportion of Google searches his data represents, but it suggests the cannibalization is accelerating.\n\nGrowth for growth\u2019s sake seems to have replaced the mission that made Google great.\n\nGoogle might defend itself by saying that providing information directly in its search results is better for users, especially on mobile devices with much more limited screen real estate. But search is a two-sided marketplace, and Google, now effectively the marketplace owner, needs to look after both sides of the market, not just its users and itself. If Google is not sending traffic to its information suppliers, should it be paying them for their content?\n\nThe health of its supplier ecosystem should be of paramount concern for Google. Not only has the company now drawn the same kind of antitrust scrutiny that once dogged Microsoft, it has weakened its own business with a self-inflicted wound that will fester over the long term. As content providers on the web get less traffic and less revenue, they will have fewer resources to produce the content that Google now abstracts into its rich snippets. This will lead to a death spiral in the content ecosystem on which Google depends, much as Microsoft\u2019s extractive dominion over PC software left few companies to develop innovative new applications for the platform.\n\nIn his book Hit Refresh, Satya Nadella, Microsoft\u2019s current CEO, reflected on the wrong turn his company had taken:\n\nWhen I became CEO, I sensed we had forgotten how our talent for partnerships was a key to what made us great. Success caused people to unlearn the habits that made them successful in the first place.\n\nI asked Nadella to expand on this thought in an interview I did with him in April 2017:\n\nThe creation myth of Microsoft is what should inspire us. One of the first things the company did, when Bill and Paul got together, is that they built the BASIC interpreter for the ALTAIR. What does that tell us today, in 2017? It tells us that we should build technology so that others can build technology. And in a world which is going to be shaped by technology, in every part of the world, in every sector of the economy, that\u2019s a great mission to have. And, so, I like that, that sense of purpose, that we create technology so that others could create more technology.\n\nNow that they\u2019ve gone back to enabling others, Microsoft is on a tear.\n\nWe might ask a similar question: what was the creation myth of Google? In 1998, Larry Page and Sergey Brin set out to \u201corganize the world\u2019s information and make it universally accessible and useful.\u201d Paraphrasing Nadella, what does that tell us today, in 2019? It tells us that Google should build services that help others to create the information that Google can then organize, make accessible, and make more useful. That\u2019s a mission worth blitzscaling for.\n\nGoogle is now 20 years old. One reason for its extractive behavior is that it is being told (now by Wall Street rather than venture investors) that it is imperative to keep growing. But the greenfield opportunity has gone, and the easiest source of continued growth is cannibalization of the ecosystem of content suppliers that Google was originally created to give users better access to. Growth for growth\u2019s sake seems to have replaced the mission that made Google great.\n\nThe true path to prosperity\n\nLet\u2019s circle back to Uber and Lyft as they approach their IPOs. Travis Kalanick and Garrett Camp, the founders of Uber, are serial entrepreneurs who set out to get rich. Logan Green and John Zimmer, the founders of Lyft, are idealists whose vision was to reinvent public transportation. But having raised billions using the blitzscaling model, both companies are subject to the same inexorable logic: they must maximize the return to investors.\n\nThis they can do only by convincing the market that their money-losing businesses will be far better in the future than they are today. Their race to monopoly has ended up instead with a money-losing duopoly, where low prices to entice ever more consumers are subsidized by ever more capital. This creates enormous pressure to eliminate costs, including the cost of drivers, by investing even more money in technologies like autonomous vehicles, once again \u201cprioritizing speed over efficiency,\u201d and \u201crisking potentially disastrous defeat\u201d while blitzscaling their way into an unknown future.\n\nUnfortunately, the defeat being risked is not just theirs, but ours. Microsoft and Google began to cannibalize their suppliers only after 20 years of creating value for them. Uber and Lyft are being encouraged to eliminate their driver partners from the get-go. If it were just these two companies, it would be bad enough. But it isn\u2019t. Our entire economy seems to have forgotten that workers are also consumers, and suppliers are also customers. When companies use automation to put people out of work, they can no longer afford to be consumers; when platforms extract all the value and leave none for their suppliers, they are undermining their own long-term prospects. It\u2019s two-sided markets all the way down.\n\nThe goal for Lyft and Uber (and for all the entrepreneurs being urged to blitzscale) should be to make their companies more sustainable, not just more explosive\u2014more equitable, not more extractive.\n\nAs an industry and as a society, we still have many lessons to learn, and, apologies to Hoffman and Yeh, I fear that how to get better at runaway growth is far from the most important one.\nContinue reading The fundamental problem with Silicon Valley\u2019s favorite growth strategy.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vaPTQ57sDNA/the-fundamental-problem-with-silicon-valleys-favorite-growth-strategy"
 },
 {
  "title": "Velocity 2019 will focus on the rise of cloud native infrastructure",
  "content": "Organizations that want all of the speed, agility, and savings the cloud provides are embracing a cloud native approach. Nearly all organizations today are doing some of their business in the cloud, but the push for increased feature performance and reliability has sparked a growing number to embrace a cloud native infrastructure. In Capgemini\u2019s survey of more than 900 executives, adoption of cloud native apps is set to jump from 15% to 32% by 2020. The strong combination of growth in cloud native adoption and the considerable opportunities it creates for organizations is why we\u2019re making cloud native a core theme at the O\u2019Reilly Velocity Conference this year.\n\n\n\nWhat\u2019s the appeal of cloud native? These days consumers demand instant access to services, products, and data across any device, at any time. This 24/7 expectation has changed how companies do business, forcing many to move their infrastructure to the cloud to provide the fast, reliable, always-available access on which we\u2019ve come to rely.\n\n\n\nYet, merely packaging your apps and moving them to the cloud isn\u2019t enough. To harness the cloud\u2019s cost and performance benefits, organizations have found that a cloud native approach is a necessity. Cloud native applications are specifically designed to scale and provision resources on the fly in response to business needs. This lets your apps run efficiently, saving you money. These apps are also more resilient, resulting in less downtime and happier customers. And as you develop and improve your applications, a cloud native infrastructure makes it possible for your company to deploy new features faster, more affordably, and with less risk.\n\n\n\nCloud native considerations\n\n\n\nThe Cloud Native Computing Foundation (CNCF) defines cloud native as a set of technologies designed to:\n\n...empower organizations to build and run scalable applications in modern, dynamic environments such as public, private, and hybrid clouds. Containers, service meshes, microservices, immutable infrastructure, and declarative APIs exemplify this approach.\n\nThese techniques enable loosely coupled systems that are resilient, manageable, and observable. Combined with robust automation, they allow engineers to make high-impact changes frequently and predictably with minimal toil.\n\n\nThe alternative to being cloud native is to either retain your on-premises infrastructure or merely \"lift and shift\" your current infrastructure to the cloud. Both options result in your existing applications being stuck with their legacy modes of operation and unable to take advantage of the cloud's built-in benefits.\n\n\n\nWhile \u201clift and shift\u201d is an option, it\u2019s become clear as enterprises struggle to manage cloud costs and squeeze increased performance from their pipelines that it\u2019s not enough to simply move old architectures to new locations. To remain competitive, companies are being forced to adopt new patterns, such as DevOps and site reliability engineering, and new tools like Kubernetes, for building and maintaining distributed systems that often span multiple cloud providers. Accordingly, use of cloud native applications in production has grown more than 200% since December 2017.\n\n\n\nAnd the number of companies contributing to this space keeps growing. The CNCF, home to popular open source tools like Kubernetes, Prometheus, and Envoy, has grown to 350 members compared to fewer than 50 in early 2016. The community is extremely active\u2014the CNCF had more than 47,000 contributors work on its projects in 2018. \"This is clearly a sign that the cloud native space is a place companies are investing in, which means increased demand for resources,\" said Boris Scholl, product architect for Microsoft Azure, in a recent conversation.\n\n\n\nBut going cloud native is not all sunshine and roses; it\u2019s hard work. The systems are inherently complex, difficult to monitor and troubleshoot, and require new tools that are constantly evolving and not always easy to learn. Vendor lock-in is a concern as well, causing many companies to adopt either a multi-cloud approach (where they work with more than one public cloud vendor) or a hybrid cloud approach (a combination of on-premises private cloud and third-party public cloud infrastructure, managed as one), which adds complexity in exchange for flexibility. Applications that are developed specifically to take advantage of one cloud provider\u2019s infrastructure are not very portable.\n\n\n\nThe challenges are not all technical, either. Going cloud native requires new patterns of working and new methods of collaborating, such as DevOps and site reliability engineering. To be successful, these shifts need buy-in from every part of the business.\n\n\n\nIn Solstice\u2019s Cloud Native Forecast for 2019, the authors highlight the challenges of change as a top trend facing the cloud community this year. \u201cOne of the most challenging aspects of cloud-native modernization is transforming an organization\u2019s human capital and culture,\u201d according to the report. \u201cThis can involve ruthless automation, new shared responsibilities between developers and operations, pair programming, test-driven development, and CI/CD. For many developers, these changes are simply hard to implement.\u201d\n\n\n\nCloud native and the evolution of the O\u2019Reilly Velocity Conference\n\n\n\nWe know businesses are turning to cloud native infrastructure because it helps them meet and exceed the expectations of their customers. We know cloud native methods and tools are expanding and maturing. And we know adoption of cloud native infrastructure is not an easy task. These factors mean systems engineers and operations professionals\u2014the audience Velocity serves\u2014are being asked to learn new techniques and best practices for building and managing the cloud native systems their companies need.\n\n\n\nEvolving toward cloud native is a natural step for Velocity because it has a history of shifting as technology shifts. The event's original focus on WebOps grew to encompass a broader audience: systems engineers. Our community today has emerged from their silos to take part in cross-functional teams, building and maintaining far more interconnected, distributed systems, most of which are hosted, at least in part, on the cloud. Our attendees have experienced first-hand the raft of new challenges and opportunities around performance, security, and reliability in building cloud native systems.\n\n\n\nAt Velocity, our mission is to provide our audience with the educational resources and industry connections they need to successfully build and maintain modern systems, which means turning the spotlight to cloud native infrastructure. We hope you\u2019ll join us as we explore cloud native in depth at our 2019 events in San Jose (June 10-13, 2019) and Berlin (November 4-7, 2019).\nContinue reading Velocity 2019 will focus on the rise of cloud native infrastructure.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/AGxTc2f9Pm8/velocity-2019-will-focus-on-the-rise-of-cloud-native-infrastructure"
 },
 {
  "title": "Proposals for model vulnerability and security",
  "content": "Apply fair and private models, white-hat and forensic model debugging, and common sense to protect machine learning models from malicious actors.Like many others, I\u2019ve known for some time that machine learning models themselves could pose security risks. A recent flourish of posts and papers has outlined the broader topic, listed attack vectors and vulnerabilities, started to propose defensive solutions, and provided the necessary framework for this post. The objective here is to brainstorm on potential security vulnerabilities and defenses in the context of popular, traditional predictive modeling systems, such as linear and tree-based models trained on static data sets. While I\u2019m no security expert, I have been following the areas of machine learning debugging, explanations, fairness, interpretability, and privacy very closely, and I think many of these techniques can be applied to attack and defend predictive modeling systems. \n\nIn hopes of furthering discussions between actual security experts and practitioners in the applied machine learning community (like me), this post will put forward several plausible attack vectors for a typical machine learning system at a typical organization, propose tentative defensive solutions, and discuss a few general concerns and potential best practices.\n\n1. Data poisoning attacks\n\nData poisoning refers to someone systematically changing your training data to manipulate your model\u2019s predictions. (Data poisoning attacks have also been called \u201ccausative\u201d attacks.) To poison data, an attacker must have access to some or all of your training data. And at many companies, many different employees, consultants, and contractors have just that\u2014and with little oversight. It\u2019s also possible a malicious external actor could acquire unauthorized access to some or all of your training data and poison it. A very direct kind of data poisoning attack might involve altering the labels of a training data set. So, whatever the commercial application of your model is, the attacker could dependably benefit from your model\u2019s predictions\u2014for example, by altering labels so your model learns to award large loans, large discounts, or small insurance premiums to people like themselves. (Forcing your model to make a false prediction for the attacker\u2019s benefit is sometimes called a violation of your model\u2019s \u201cintegrity\u201d.) It\u2019s also possible that a malicious actor could use data poisoning to train your model to intentionally discriminate against a group of people, depriving them the big loan, big discount, or low premiums they rightfully deserve. This is like a denial-of-service (DOS) attack on your model itself. (Forcing your model to make a false prediction to hurt others is sometimes called a violation of your model\u2019s \u201cavailability\u201d.) While it might be simpler to think of data poisoning as changing the values in the existing rows of a data set, data poisoning can also be conducted by adding seemingly harmless or superfluous columns onto a data set. Altered values in these columns could then trigger altered model predictions.\n\n\n\nNow, let\u2019s discuss some potential defensive and forensic solutions for data poisoning:\n\n\n\n\nDisparate impact analysis: Many banks already undertake disparate impact analysis for fair lending purposes to determine if their model is treating different types of people in a discriminatory manner. Many other organizations, however, aren't yet so evolved. Disparate impact analysis could potentially discover intentional discrimination in model predictions. There are several great open source tools for detecting discrimination and disparate impact analysis, such as Aequitas, Themis, and AIF360.\n\n\nFair or private models: Models such as learning fair representations (LFR) and private aggregation of teacher ensembles (PATE) try to focus less on individual demographic traits to make predictions. These models may also be less susceptible to discriminatory data poisoning attacks.\n\n\nReject on Negative Impact (RONI): RONI is a technique that removes rows of data from the training data set that decrease prediction accuracy. See \u201cThe Security of Machine Learning\u201d in section 8 for more information on RONI.\n\n\nResidual analysis: Look for strange, prominent patterns in the residuals of your model predictions, especially for employees, consultants, or contractors.\n\n\nSelf-reflection: Score your models on your employees, consultants, and contractors and look for anomalously beneficial predictions.\n\n\n\nDisparate impact analysis, residual analysis, and self-reflection can be conducted at training time and as part of real-time model monitoring activities.\n\n2. Watermark attacks\n\nWatermarking is a term borrowed from the deep learning security literature that often refers to putting special pixels into an image to trigger a desired outcome from your model. It seems entirely possible to do the same with customer or transactional data. Consider a scenario where an employee, consultant, contractor, or malicious external actor has access to your model\u2019s production code\u2014that makes real-time predictions. Such an individual could change that code to recognize a strange, or unlikely, combination of input variable values to trigger a desired prediction outcome. Like data poisoning, watermark attacks can be used to attack your model\u2019s integrity or availability. For instance, to attack your model\u2019s integrity, a malicious insider could insert a payload into your model\u2019s production scoring code that recognizes the combination of age of 0 and years at an address of 99 to trigger some kind of positive prediction outcome for themselves or their associates. To deny model availability, an attacker could insert an artificial, discriminatory rule into your model\u2019s scoring code that prevents your model from producing positive outcomes for a certain group of people.\n\n\n\nDefensive and forensic approaches for watermark attacks might include:\n\n\n\n\nAnomaly detection: Autoencoders are a fraud detection model that can identify input data that is strange or unlike other input data, but in complex ways. Autoencoders could potentially catch any watermarks used to trigger malicious mechanisms.\n\n\nData integrity constraints: Many databases don\u2019t allow for strange or unrealistic combinations of input variables and this could potentially thwart watermarking attacks. Applying data integrity constraints on live, incoming data streams could have the same benefits.\n\n\nDisparate impact analysis: see section 1.\n\n\n Version control: Production model scoring code should be managed and version-controlled\u2014just like any other mission-critical software asset.\n\n\n\nAnomaly detection, data integrity constraints, and disparate impact analysis can be used at training time and as part of real-time model monitoring activities.\n\n3. Inversion by surrogate models\n\nInversion basically refers to getting unauthorized information out of your model\u2014as opposed to putting information into your model. Inversion can also be an example of an \u201cexploratory reverse-engineering\u201d attack. If an attacker can receive many predictions from your model API or other endpoint (website, app, etc.), they can train their own surrogate model. In short, that\u2019s a simulation of your very own predictive model! An attacker could conceivably train a surrogate model between the inputs they used to generate the received predictions and the received predictions themselves. Depending on the number of predictions they can receive, the surrogate model could become quite an accurate simulation of your model. Once the surrogate model is trained, then the attacker has a sandbox from which to plan impersonation (i.e., \u201cmimicry\u201d) or adversarial example attacks against your model\u2019s integrity, or the potential ability to start reconstructing aspects of your sensitive training data. Surrogate models can also be trained using external data sources that can be somehow matched to your predictions, as ProPublica famously did with the proprietary COMPAS recidivism model.\n\n\n\nTo protect your model against inversion by surrogate model, consider the following approaches:\n\n\n\n\nAuthorized access: Require additional authentication (e.g., 2FA) to receive a prediction.\n\n\nThrottle predictions: Restrict high numbers of rapid predictions from single users; consider artificially increasing prediction latency.\n\n\nWhite-hat surrogate models: As a white-hat hacking exercise, try this: train your own surrogate models between your inputs and the predictions of your production model and carefully observe:\n\n\n\tthe accuracy bounds of different types of white-hat surrogate models; try to understand the extent to which a surrogate model can really be used to learn unfavorable knowledge about your model.\n\tthe types of data trends that can be learned from your white-hat surrogate model, like linear trends represented by linear model coefficients.\n\tthe types of segments or demographic distributions that can be learned by analyzing the number of individuals assigned to certain white-hat surrogate decision tree nodes.\n\tthe rules that can be learned from a white-hat surrogate decision tree\u2014for example, how to reliably impersonate an individual who would receive a beneficial prediction.\n\n\n\n4. Adversarial example attacks\n\nA motivated attacker could theoretically learn, say by trial and error (i.e., \u201cexploration\u201d or \u201csensitivity analysis\u201d), surrogate model inversion, or by social engineering, how to game your model to receive their desired prediction outcome or to avoid an undesirable prediction. Carrying out an attack by specifically engineering a row of data for such purposes is referred to as an adversarial example attack. (Sometimes also known as an \u201cexploratory integrity\u201d attack.) An attacker could use an adversarial example attack to grant themselves a large loan or a low insurance premium or to avoid denial of parole based on a high criminal risk score. Some people might call using adversarial examples to avoid an undesirable outcome from your model prediction \u201cevasion.\u201d\n\n\n\nTry out the techniques outlined below to defend against or to confirm an adversarial example attack:\n\n\n\n\nActivation analysis: Activation analysis requires benchmarking internal mechanisms of your predictive models, such as the average activation of neurons in your neural network or the proportion of observations assigned to each leaf node in your random forest. You then compare that information against your model\u2019s behavior on incoming, real-world data streams. As one of my colleagues put it, \u201cthis is like seeing one leaf node in a random forest correspond to 0.1% of the training data but hit for 75% of the production scoring rows in an hour.\u201d Patterns like this could be evidence of an adversarial example attack.\n\n\nAnomaly detection: see section 2.\n\n\nAuthorized access: see section 3.\n\n\nBenchmark models: Use a highly transparent benchmark model when scoring new data in addition to your more complex model. Interpretable models could be seen as harder to hack because their mechanisms are directly transparent. When scoring new data, compare your new fancy machine learning model against a trusted, transparent model or a model trained on a trusted data source and pipeline. If the difference between your more complex and opaque machine learning model and your interpretable or trusted model is too great, fall back to the predictions of the conservative model or send the row of data for manual processing. Also record the incident. It could be an adversarial example attack.\n\n\nThrottle predictions: see section 3.\n\n\nWhite-hat sensitivity analysis: Use sensitivity analysis to conduct your own exploratory attacks to understand what variable values (or combinations thereof) can cause large swings in predictions. Screen for these values, or combinations of values, when scoring new data. You may find the open source package cleverhans helpful for any white-hat exploratory analyses you conduct.\n\n\nWhite-hat surrogate models: see section 3.\n\n\n\nActivation analysis and benchmark models can be used at training time and as part of real-time model monitoring activities.\n\n5. Impersonation\n\nA motivated attacker can learn\u2014say, again, by trial and error, surrogate model inversion, or social engineering\u2014what type of input or individual receives a desired prediction outcome. The attacker can then impersonate this input or individual to receive their desired prediction outcome from your model. (Impersonation attacks are sometimes also known as \u201cmimicry\u201d attacks and resemble identity theft from the model\u2019s perspective.) Like an adversarial example attack, an impersonation attack involves artificially changing the input data values to your model. Unlike an adversarial example attack, where a potentially random-looking combination of input data values could be used to trick your model, impersonation implies using the information associated with another modeled entity (i.e., convict, customer, employee, financial transaction, patient, product, etc.) to receive the prediction your model associates with that type of entity. For example, an attacker could learn what characteristics your model associates with awarding large discounts, like comping a room at a casino for a big spender, and then falsify their information to receive the same discount. They could also share their strategy with others, potentially leading to large losses for your company.\n\n\n\nIf you are using a two-stage model, be aware of an \u201callergy\u201d attack. This is where a malicious actor may impersonate a normal row of input data for the first stage of your model in order to attack the second stage of your model.\n\n\n\nDefensive and forensic approaches for impersonation attacks may include:\n\n\n\nActivation analysis: see section 4.\n\n\nAuthorized access: see section 3.\n\n\nScreening for duplicates: At scoring time track the number of similar records your model is exposed to, potentially in a reduced-dimensional space using autoencoders, multidimensional scaling (MDS), or similar dimension reduction techniques. If too many similar rows are encountered during some time span, take corrective action.\n\n\nSecurity-aware features: Keep a feature in your pipeline, say num_similar_queries, that may be useless when your model is first trained or deployed but could be populated at scoring time (or during future model retrainings) to make your model or your pipeline security-aware. For instance, if at scoring time the value of num_similar_queries is greater than zero, the scoring request could be sent for human oversight. In the future, when you retrain your model, you could teach it to give input data rows with high num_similar_queries values negative prediction outcomes.\n\n\n\nActivation analysis, screening for duplicates, and security-aware features can be used at training time and as part of real-time model monitoring activities.\n\n6. General concerns\n\nSeveral common machine learning usage patterns also present more general security concerns.\n\n\n\nBlackboxes and unnecessary complexity: Although recent developments in interpretable models and model explanations have provided the opportunity to use accurate and also transparent nonlinear classifiers and regressors, many machine learning workflows are still centered around blackbox models. Such blackbox models are only one type of often unnecessary complexity in a typical commercial machine learning workflow. Other examples of potentially harmful complexity could be overly exotic feature engineering or large numbers of package dependencies. Such complexity can be problematic for at least two reasons:\n\n\n\nA dedicated, motivated attacker can, over time, learn more about your overly complex blackbox modeling system than you or your team knows about your own model. (Especially in today\u2019s overheated and turnover-prone data \u201cscience\u201d market.) To do so, they can use many newly available model-agnostic explanation techniques and old-school sensitivity analysis, among many other more common hacking tools. This knowledge imbalance can potentially be exploited to conduct the attacks described in sections 1 \u2013 5 or for other yet unknown types of attacks.\n\n\n\nMachine learning in the research and development environment is highly dependent on a diverse ecosystem of open source software packages. Some of these packages have many, many contributors and users. Some are highly specific and only meaningful to a small number of researchers or practitioners. It\u2019s well understood that many packages are maintained by brilliant statisticians and machine learning researchers whose primary focus is mathematics or algorithms, not software engineering, and certainly not security. It\u2019s not uncommon for a machine learning pipeline to be dependent on dozens or even hundreds of external packages, any one of which could be hacked to conceal an attack payload.\n\n\n\nDistributed systems and models: For better or worse, we live in the age of big data. Many organizations are now using distributed data processing and machine learning systems. Distributed computing can provide a broad attack surface for a malicious internal or external actor in the context of machine learning. Data could be poisoned on only one or a few worker nodes of a large distributed data storage or processing system. A back door for watermarking could be coded into just one model of a large ensemble. Instead of debugging one simple data set or model, now practitioners must examine data or models distributed across large computing clusters.\n\n\n\nDistributed denial of service (DDOS) attacks: If a predictive modeling service is central to your organization\u2019s mission, ensure you have at least considered more conventional distributed denial of service attacks, where attackers hit the public-facing prediction service with an incredibly high volume of requests to delay or stop predictions for legitimate users.\n\n7. General solutions\n\nSeveral older and newer general best practices can be employed to decrease your security vulnerabilities and to increase fairness, accountability, transparency, and trust in machine learning systems.\n\n\n\nAuthorized access and prediction throttling: Standard safeguards such as additional authentication and throttling may be highly effective at stymieing a number of the attack vectors described in sections 1\u20135.\n\n\n\nBenchmark models: An older or trusted interpretable modeling pipeline, or other highly transparent predictor, can be used as a benchmark model from which to measure whether a prediction was manipulated by any number of means. This could include data poisoning, watermark attacks, or adversarial example attacks. If the difference between your trusted model\u2019s prediction and your more complex and opaque model\u2019s predictions are too large, record these instances. Refer them to human analysts or take other appropriate forensic or remediation steps. (Of course, serious precautions must be taken to ensure your benchmark model and pipeline remains secure and unchanged from its original, trusted state.)\n\n\n\nInterpretable, fair, or private models: The techniques now exist (e.g., monotonic GBMs (M-GBM), scalable Bayesian rule lists (SBRL), eXplainable Neural Networks (XNN)), that can allow for both accuracy and interpretability. These accurate and interpretable models are easier to document and debug than classic machine learning blackboxes. Newer types of fair and private models (e.g., LFR, PATE) can also be trained to essentially care less about outward visible, demographic characteristics that can be observed, socially engineered into an adversarial example attack, or impersonated. Are you considering creating a new machine learning workflow in the future? Think about basing it on lower-risk, interpretable, private, or fair models. Models like this are more easily debugged and potentially robust to changes in an individual entity\u2019s characteristics.\n\n\n\nModel debugging for security: The newer field of model debugging is focused on discovering errors in machine learning model mechanisms and predictions, and remediating those errors. Debugging tools such a surrogate models, residual analysis, and sensitivity analysis can be used in white-hat exercises to understand your own vulnerabilities or for forensic exercises to find any potential attacks that may have occurred or be occurring.\n\n\n\nModel documentation and explanation techniques: Model documentation is a risk-mitigation strategy that has been used for decades in banking. It allows knowledge about complex modeling systems to be preserved and transferred as teams of model owners change over time. Model documentation has been traditionally applied to highly transparent linear models. But with the advent of powerful, accurate explanatory tools (such as tree SHAP and derivative-based local feature attributions for neural networks), pre-existing blackbox model workflows can be at least somewhat explained, debugged, and documented. Documentation should obviously now include all security goals, including any known, remediated, or anticipated security vulnerabilities.\n\n\n\nModel monitoring and management explicitly for security: Serious practitioners understand most models are trained on static snapshots of reality represented by training data and that their prediction accuracy degrades in real time as present realities drift away from the past information captured in the training data. Today, most model monitoring is aimed at discovering this drift in input variable distributions that will eventually lead to accuracy decay. Model monitoring should now likely be designed to monitor for the attacks described in sections 1 \u2013 5 and any other potential threats your white-hat model debugging exercises uncover. (While not always directly related to security, my opinion is that models should also be evaluated for disparate impact in real time as well.) Along with model documentation, all modeling artifacts, source code, and associated metadata need to be managed, versioned, and audited for security like the valuable commercial assets they are.\n\n\n\nSecurity-aware features: Features, rules, and pre- or post-processing steps can be included in your models or pipelines that are security-aware, such as the number of similar rows seen by the model, whether the current row represents an employee, contractor, or consultant, or whether the values in the current row are similar to those found in white-hat adversarial example attacks. These features may or may not be useful when a model is first trained. But keeping a placeholder for them when scoring new data, or when retraining future iterations of your model, may come in very handy one day.\n\n\n\nSystemic anomaly detection: Train an autoencoder\u2013based anomaly detection metamodel on your entire predictive modeling system\u2019s operating statistics\u2014the number of predictions in some time period, latency, CPU, memory, and disk loads, the number of concurrent users, and everything else you can get your hands on\u2014and then closely monitor this metamodel for anomalies. An anomaly could tip you off that something is generally not right in your predictive modeling system. Subsequent investigation or specific mechanisms would be needed to trace down the exact problem.\n\n8. References and further reading\n\nA lot of the contemporary academic machine learning security literature focuses on adaptive learning, deep learning, and encryption. However, I don\u2019t know many practitioners who are actually doing these things yet. So, in addition to recently published articles and blogs, I found papers from the 1990s and early 2000s about network intrusion, virus detection, spam filtering, and related topics to be helpful resources as well. If you\u2019d like to learn more about the fascinating subject of securing machine learning models, here are the main references\u2014past and present\u2014that I used for this post. I\u2019d recommend them for further reading, too.\n\n\n\n\n\tBarreno, Marco, et al. \u201cThe Security of Machine Learning.\u201d Machine Learning 81.2 (2010): 121-148. URL: https://people.eecs.berkeley.edu/~adj/publications/paper-files/SecML-MLJ2010.pdf\n\n\n\tKumar, Ajitesh. \u201cSecurity Attacks: Analysis of Machine Learning Models.\u201d DZone (2018). URL: https://dzone.com/articles/security-attacks-analysis-of-machine-learning-mode\n\n\n\tLorica, Ben and Loukides, Mike. \u201cYou created a machine learning application. Now make sure it\u2019s secure\u201d. O\u2019Reilly Ideas (2019). URL: https://www.oreilly.com/ideas/you-created-a-machine-learning-application-now-make-sure-its-secure\n\n\n\tPapernot, Nicolas. \u201cA Marauder\u2019s Map of Security and Privacy in Machine Learning: An overview of current and future research directions for making machine learning secure and private. \u201d Proceedings of the 11th ACM Workshop on Artificial Intelligence and Security. ACM (2018). URL: https://arxiv.org/pdf/1811.01134.pdf\n\n\n\nConclusion\n\nI care very much about the science and practice of machine learning, and I am now concerned that the threat of a terrible machine learning hack, combined with growing concerns about privacy violations and algorithmic discrimination, could increase burgeoning public and political skepticism about machine learning and AI. We should all be mindful of AI winters in the not-so-distant past. Security vulnerabilities, privacy violations, and algorithmic discrimination could all potentially combine to lead to decreased funding for machine learning research or draconian over-regulation of the field. Let\u2019s continue discussing and addressing these important problems to preemptively prevent a crisis, as opposed to having to reactively respond to one.\n\n\nAcknowledgements\n\nThanks to Doug Deloy, Dmitry Larko, Tom Kraljevic, and Prashant Shuklabaidya for their insightful comments and suggestions.\n\nContinue reading Proposals for model vulnerability and security.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/r7qc1PVJCcU/proposals-for-model-vulnerability-and-security"
 },
 {
  "title": "Four short links: 20 March 2019",
  "content": "Embedded Computer Vision, Unix History, Unionizing Workforce, and Text Adventure AI\n\nSOD -- an embedded, modern cross-platform computer vision and machine learning software library that exposes a set of APIs for deep learning, advanced media analysis and processing, including real-time, multi-class object detection and model training on embedded systems with limited computational resource and IoT devices. Open source.\n\nUnix History Repo -- Continuous Unix commit history from 1970 until today.\n\n\nKickstarter's Staff is Unionizing -- early days for the union, but I'm keen to see how this plays out. (I'm one of the founding signatories to the Aotearoa Tech Union, though our countries have different workplace laws.)\n\nTextworld -- Microsoft Research project, it's an open source, extensible engine that both generates and simulates text games. You can use it to train reinforcement learning (RL) agents to learn skills such as language understanding and grounding, combined with sequential decision-making. Cue \"Microsoft teaches AI to play Zork\" headlines. And they have a competition.\n\nContinue reading Four short links: 20 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/kx47dUvg_w4/four-short-links-20-march-2019"
 },
 {
  "title": "Four short links: 19 March 2019",
  "content": "Digital Life, Information Abundance, Quantum Computing, Language Design\n\nTimeliner -- All your digital life on a single timeline, stored locally. Great idea; I hope its development continues.\n\nWhat's Wrong with Blaming \"Information\" for Political Chaos (Cory Doctorow) -- a response to yesterday's \"What The Hell is Going On?\" link. I think Perell is wrong. His theory omits the most salient, obvious explanation for what's going on (the creation of an oligarchy that has diminished the efficacy of public institutions and introduced widespread corruption in every domain), in favor of rationalizations that let the wealthy and their enablers off the hook, converting a corrupt system with nameable human actors who have benefited from it and who spend lavishly to perpetuate it into a systemic problem that emerges from a historical moment in which everyone is blameless, prisoners of fate and history. I think it's both: we have far more of every medium than we can consume because the information industrial engines are geared to production and distraction not curation for quality. This has crippled the internet's ability to be a fightback mechanism. My country's recent experiences with snuff videos and white supremacist evangelicals doesn't predispose me to think as Perell does that the deluge of undifferentiated information is a marvelous thing, so I think Cory and I have a great topic of conversation the next time we're at the same conference together.\n\nQuantum Computing for the Very Curious (Michael Nielsen) -- an explanation of quantum computing with built-in spaced repetition testing of key concepts. Clever!\n\n3 Things I Wish I Knew When I Began Designing Languages (Peter Alvaro) -- when I presented at my job talk at Harvard, a systems researcher who I admire very much, said something along the lines of, \"Yes, this kind of reminds me of a Racket, and in Racket everything is a parenthesis. So, in your language, what is the thing that is everything that I don't buy?\" That was nice.\n\n\nContinue reading Four short links: 19 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-EcbrNi0eHU/four-short-links-19-march-2019"
 },
 {
  "title": "Four short links: 18 March 2019",
  "content": "Information Abundance, Meritocracy Considered Harmful, Tracking, and Blockchain Basics\n\nWhat the Hell is Going On? -- I\u2019ll show how the shift from information scarcity to information abundance is transforming commerce, education, and politics. The structure of each industry was shaped by the information-scarce, mass media environment. First, we\u2019ll focus on commerce. Education will be second. Then, we\u2019ll zoom out for a short history of America since World War II. We\u2019ll see how information scarcity creates authority and observe the effects of the internet on knowledge. Finally, we\u2019ll return to politics and tie these threads together.\n\n\nMeritocracy (Fast Company) -- in companies that explicitly held meritocracy as a core value, managers assigned greater rewards to male employees over female employees with identical performance evaluations. This preference disappeared where meritocracy was not explicitly adopted as a value.\n\n\nClient-Side Instrumentation for Under $1/Month -- open source JavaScript tracker, AWS Lambda collecting it, Cloudflare logs into S3, AWS Athena integrating. (via Simon Willison)\n\nCrypto Canon -- A16Z recommended reading list to come up to speed in cryptocurrency/blockchain. Contains an awful lot of Medium posts.\n\nContinue reading Four short links: 18 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/71v9GAHwv_U/four-short-links-18-march-2019"
 },
 {
  "title": "Four short links: 15 March 2019",
  "content": "Monopsony, Debugging Neural Nets, Future of Wearables, and Event Audio\n\nFacebook is Not a Monopoly, But Should Be Broken Up (Wired) -- Demand monopsonists integrate horizontally, acquiring or copying user demand adjacent to their existing demand and gaining leverage over their suppliers (and advertisers, if that\u2019s the model). Facebook is unlikely to ever own a media production company, just as Airbnb and Uber will not soon own a hotel or a physical taxi company. But if they can, they\u2019ll own every square foot of demand that feeds those industries. (via Cory Doctorow)\n\nDebugging Neural Networks -- 1. Start simple; 2. Confirm your loss; 3. Check intermediate outputs and connections; 4. Diagnose parameters; 5. Tracking your work.\n\n\nA Peek into the Future of Wearables (IEEE) -- Mind reading glasses, goggles that erase chronic pain, a wristband that can hear what the wearer can\u2019t, and more futuristic wearables are on the horizon.\n\n\nEvent Audio -- I wrote up a guide for event organizers to providing microphones so all the speakers can give their best performance.\n\nContinue reading Four short links: 15 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Wa2xlhrLdR0/four-short-links-15-march-2019"
 },
 {
  "title": "Algorithms are shaping our lives\u2014here\u2019s how we wrest back control",
  "content": "The O\u2019Reilly Data Show Podcast: Kartik Hosanagar on the growing power and sophistication of algorithms.In this episode of the Data Show, I spoke with Kartik Hosanagar, professor of technology and digital business, and professor of marketing at The Wharton School of the University of Pennsylvania. \u00a0Hosanagar is also the author of a newly released book, A Human\u2019s Guide to Machine Intelligence, an interesting tour through the recent evolution of AI applications that draws from his extensive experience at the intersection of business and technology.Continue reading Algorithms are shaping our lives\u2014here\u2019s how we wrest back control.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/qaPy2IjbvQA/algorithms-are-shaping-our-lives-heres-how-we-wrest-back-control"
 },
 {
  "title": "Four short links: 14 March 2019",
  "content": "Ethical Data, Iodide Notebook, Alexa Discovery, and Game Engine\n\nChanging Contexts and Intents (O'Reilly) -- context and intent as framing mechanisms for determining whether a use of data is appropriate.\n\nIodide (Mozilla) -- notebook, but with multiple languages (eventually) compiling down to WebAssembly. Create, share, collaborate, and reproduce powerful reports and visualizations with tools you already know.\n\n\nAmazon's Alexa: 80,000 Apps and No Runaway Hit (Bloomberg) -- voice has a massive discoverability problem. As Alan Cooper said, I really have no idea what the boundaries of the domains are, because I would have to go experiment endlessly with Siri and Alexa and all the others, and I don\u2019t have the patience. But that\u2019s the point: I have no idea even roughly what I\u2019m likely to be able to ask about. And it\u2019s a moving target because the platform makers assume that more content is better, so they shovel new content into the system as fast as they can. So in a very real sense, the burden of memorizing the list of commands is increasing over time, as the system \u201cimproves.\u201d\n\n\nESP Little Game Engine -- Game engine with web emulator and compiler.  [...] The game engine has a virtual screen resolution of 128x128 pixels, 16 colors, one background layer, 32 soft sprites with collision tracking and rotation, 20kb of memory for the game and variables. The virtual machine performs approximately 900,000 operations per second at a drawing rate of 20 frames per second. Control of eight buttons. Built for the ESP8266 chipset.\n\nContinue reading Four short links: 14 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BPbIL9r5bgg/four-short-links-14-march-2019"
 },
 {
  "title": "Changing contexts and intents",
  "content": "The internet itself is a changing context\u2014we\u2019re right to worry about data flows, but we also have to worry about the context changing even when data doesn\u2019t flow.Every day, someone comes up with a new use for old data. Recently, IBM scraped a million photos from Flickr and turned them into a training data set for an AI project intending to reduce bias in facial recognition. That\u2019s a noble goal, promoted to researchers as an opportunity to make more ethical AI.\n\nYet, the project raises numerous ethical questions of its own. Photographers and subjects weren\u2019t asked if their photos could be included; while the photos are all covered by a Creative Commons non-commercial license, one of the photographers quoted in an NBC article about the project asks by what rationale anything IBM does with his photographs can be considered \u201cnon-commercial.\u201d It\u2019s almost impossible to get your photographs removed from the database; it\u2019s possible in principle, but IBM requires you to have the URL of the original photograph\u2014which means you have to know which photographs were included in the first place. (NBC provides a tool to check whether your photos are in the database.) And there are plenty of questions about how people will make use of this data, which has been annotated with many measurements that are useful for face recognition systems.\n\nNot only that, photographic subjects were, in effect, turned into research subjects without their consent. And even though their photos were public on Flickr, a strong case can be made that the new context violates their privacy.\n\nCornell Tech professor Helen Nissenbaum, author of the book Privacy in Context, reminds us that we need to think about privacy in terms of when data moves from one context to another, rather than in absolute terms. Thinking about changes in context is difficult, but essential: we\u2019ve long passed the point where any return to absolute privacy was possible\u2014if it ever was possible in the first place.\n\nMeredith Whittaker, co-director of the AI Now Institute, made a striking extension to this insight in a quote from the same NBC article: \u201cPeople gave their consent to sharing their photos in a different internet ecosystem.\u201d\n\nWe do indeed live in a different internet ecosystem than the one many of our original privacy rules were invented for. The internet is not what it was 30 years ago. The web is not what it was 30 years ago, when it was invented. Flickr is not what it was when it was founded. 15 or 20 years ago, we had some vague ideas about face recognition, but it was a lot closer to science fiction. People weren\u2019t actually automating image tagging, which is creepy enough; they certainly weren\u2019t building applications to scan attendees at concerts or sporting events.\n\nIBM\u2019s creation of a new database obviously represents a change of context. But Whittaker is saying that the internet itself is a changing context. It isn\u2019t what it has been in the past; it probably never could have stayed the same; but regardless of what it is now, the data\u2019s context has changed, without the data moving. We\u2019re right to worry about data flows, but we also have to worry about the context changing even when data doesn\u2019t flow. It\u2019s easy to point fingers at IBM for using Flickr\u2019s data irresponsibly\u2014as we read it, we\u2019re sympathetic with that position. But the real challenge is that the meaning of the images on Flickr has changed. They're not just photos: they're a cache of data for training machine learning systems.\n\nWhat do we do when the contexts themselves change? That\u2019s a question we must work hard to answer. Part of the problem is that contexts change slowly, and that changes in a context are much easier to ignore than a new data-driven application.\n\nSome might argue that data can never be used without consent. But that has led us down a path of over-broad clickwrap agreements that force people to give consent for things that are not yet even imagined in order to use a valuable service.\n\nOne special type of meta-context to consider is intent. While context may change, it is possible to look through that changing context to the intent of a user\u2019s consent to the use of their data. For example, when someone uses Google maps, they implicitly consent to Google using location data to guide them from place to place. When Google then provides an API that allows Uber or Lyft or Doordash to leverage that data to guide a driver to them, the context has changed but the intent has not. The data was part of a service transaction, and the intent can be \"well-intentionedly\" transferred to a new context, as long as it is still in service of the user, rather than simply for the advantage of the data holder.\n\nWhen Google decides to use your location to target advertisements, that's not only a different context but a different intent. As it turns out, Google actually expresses the intent of their data collection very broadly, and asks its users to consent to Google\u2019s use of their data in many evolving contexts as the cost of providing free services. There would surely be value in finer grained expression of intent. At the same time, you can make the case that there was a kind of meta-intent expressed and agreed to, which can survive the context transition to new services. What we still need in a case like this is some mechanism for redress, for users to say \u201cin this case, you went too far\u201d even as they often are delighted by other new and unexpected uses for their data.\n\nThere are other cases where the breach of intent is far clearer. For example, when my cell phone provider gets my location as a byproduct of connecting me to a cell tower, other use of my data was never part of the transaction, and when they resell it (as they do), that is a breach not only of context but of intent.\n\nData ethics raises many a hard problem. Fortunately, the framing of context as a guiding principle by Nissenbaum and Whittaker gives us a powerful way to work toward solving them.\nContinue reading Changing contexts and intents.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/DuKQbMq5ONk/changing-contexts-and-intents"
 },
 {
  "title": "Four short links: 13 March 2019",
  "content": "O'Reilly Radar, Speech Recognition, Super Sensors, Burnout\n\t\nWhat is O'Reilly Radar? -- trends we see breaking: Next Economy; Future of the Firm; Machine Learning/AI; Next Architecture; Responding to Disruption. Report on Future of the Firm is already out.\n\t\nAn All-Neural On-Device Speech Recognizer (Google) -- on-device is the important bit here: no more uploading all your ambient audio to the cloud. After compression, the final model is 80MB. That's impressive too.\n\t\nSuper Sensors -- a single, highly capable sensor can indirectly monitor a large context, without direct instrumentation of objects.\n\n\t\nObservations on Burnout -- I\u2019ll add some data points, go in-depth on what I think causes it, and attempt to offer some advice for engineers and managers.\n\n\nContinue reading Four short links: 13 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/8o2JRERt-oY/four-short-links-13-march-2019"
 },
 {
  "title": "Four short links: 12 March 2019",
  "content": "Digital Music, Smart Camera, Cell Network Software, and Gender Equity\n\nProtocols: Duty, Despair and Decentralization (Mat Dryhurst) -- Another cold-light-of-day re-reading of the surge of poptimism in the press over the past decade is to see it as the bargaining stage of grief over the seemingly inexorable charge of bot-like popular figures who hoover up ideas from the margins and deploy significant resources to capture a moment with music fortified from any potentially critical angle one might level at it. Pop stars are better understood as monarchic CEO\u2019s of content production studios atop a feudal, trickle up, creative economy. They have adapted to the online ecosystem far faster than the critical systems that might have one day raised objection to them. A fascinating and energetic stream of consciousness about the internet-disabled/enabled music industry.\n\nUnder the Hood: Portal's Smart Camera (Facebook) -- how it follows you as you move around the room, with interesting pictures of the prototypes and how they automated what directors do (in some cases).\n\nMagma -- Magma is an open source software platform that gives network operators an open, flexible, and extendable mobile core network solution. Magma enables better connectivity by: (1) Allowing operators to offer cellular service without vendor lock-in with a modern, open source core network; (2) Enabling operators to manage their networks more efficiently with more automation, less downtime, better predictability, and more agility to add new services and applications; (3) Enabling federation between existing MNOs and new infrastructure providers for expanding rural infrastructure; (4) Allowing operators who are constrained with licensed spectrum to add capacity and reach by using Wi-Fi and CBRS. Want to spin up your own LTE network? (via Facebook blog)\n\nGender Equity Resources (NAVA) -- is for the GLAM (Galleries, Libraries, Archives, Museums) sector, but there's a lot to adapt for your tech workplace, too. (via Courtney Johnston)\n\nContinue reading Four short links: 12 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/aMCNOZf70PU/four-short-links-12-march-2019"
 },
 {
  "title": "Future of the firm",
  "content": "Mapping the complex forces that are reshaping organizations and changing the employee/employer relationship.The \u201cfuture of the firm\u201d is a big deal. As jobs become more automated, and people more often work in teams, with work increasingly done on a contingent and contract basis, you have to ask: \u201cWhat does a firm really do?\u201d Yes, successful businesses are increasingly digital and technologically astute. But how do they attract and manage people in a world where two billion people work part-time? How do they develop their workforce when automation is advancing at light speed? And how do they attract customers and full-time employees when competition is high and trust is at an all-time low?\n\nWhen thinking about the big-picture items affecting the future of the firm, we identified several topics that we discuss in detail in this report:Continue reading Future of the firm.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/S9x_QsWTirc/future-of-the-firm"
 },
 {
  "title": "What is O'Reilly Radar?",
  "content": "Radar spots and explores emerging technology themes so organizations can succeed amid constant change.O\u2019Reilly Radar is a process that assimilates signals and data to track, map, and name technology trends that impact many aspects of modern business and living. Almost as old as O\u2019Reilly itself, Radar has a history of playing a key role in the development and amplification of influential themes, including open source, Web 2.0, big data, DevOps, Next Economy, and others.\n\nO'Reilly applies the Radar approach to spot what's coming next and show how technology is changing our world. Using reports, conferences, and conversations, the Radar group provides decision-makers with the tools and connections they need to thrive during these dynamic times.\n\nOur insights come from many sources: our own reading of the industry tea leaves, our many contacts in the industry, our analysis of usage on the O\u2019Reilly online learning platform, and data we assemble on technology trends. After we track and identify emergent trends, we map them into themes that address their broader impact on employees, organizations, and society at large.\n\nThe process is not easy or simplistic. It requires follow-up analysis, much internal debate, and a healthy dose of realism in response to industry hype. Our goal is to provide insights and confidence to folks making decisions about technology, strategy, purpose, and mission.\n\nWe take a few fundamental approaches to exploring technology adoption, each building on the other:\n\n\n\tWe convene communities of interest through our conferences, summits, and our Foo Camps (short for \u201cFriends of O\u2019Reilly,\u201d these are invite-only, unconference-style events).\n\tWe conduct qualitative research, taking advantage of our ability to convene communities, using surveys, interviews, and salons to gain intimate access to thought leaders, business leaders, and those in the trenches who are wrestling with technology and change.\n\tWe conduct quantitative analysis to track technology adoption, from the esoteric and emergent to the everyday world of developers, designers, administrators, managers, and architects. The O\u2019Reilly online learning platform is one of the quantitative tools we use; it serves as a massive sensor that we analyze for insights into users\u2019 engagement with technology.\n\n\nThrough this process, we\u2019ve identified the following five themes business and technology leaders should consider. These themes are not discrete; we see much bleed between topics and how they interact\u2014a characteristic of the current technology environment that affects, well, nearly everything organizations touch.\n\nNext Economy\n\nNext Economy defines how business leaders, policymakers, and technologists can chart a course from the economy we experience today to a better future for all, acknowledging the wonders and challenges that we have collectively wrought.\n\nThis research area focuses on big picture economic trends that nearly all organizations face, including:\n\n\n\tHow is technology changing the shape of the corporation and the nature of work?\n\tWhat skills become more valuable as more types of work are subject to automation?\n\tWhat economic incentives encourage businesses to treat people as costs to be eliminated rather than as assets to invest in? How do we change those incentives?\n\tHow do algorithmic systems drive value, manifest bias, and affect fairness\u2014particularly in closed platforms with their own economics?\n\tWhat is the impact of behavioral economics?\n\tHow does diversity improve all aspects of decision-making?\n\tDo we need a new model and way of assessing antitrust in the age of internet-scale platforms?\n\tWhat does technology let us do now that was formerly impossible?\n\n\nThis Radar theme offers a new and empowering perspective on creating value and success, leveraging innovation, and embracing disruption and change.\n\nRadar has been looking at the Next Economy for the last five years, including running Next:Economy conferences in 2015 and 2016. Tim O\u2019Reilly\u2019s book WTF?: What\u2019s the Future and Why It\u2019s Up to Us provides a deeper dive into Next Economy topics. Lately, we have sharpened our attention on Next Economy topics into a focus on the Future of the Firm, as covered in the next section and in our \u201cFuture of the Firm\u201d report.\n\nFuture of the Firm\n\nAs jobs become more automated and work is increasingly done on a contingent and contract basis, you have to ask: what does a firm really do?\n\nYes, successful businesses are increasingly digital and technologically astute. But how do they attract, retain, incent, and manage people in a world where by choice or by circumstance two billion people work part-time? How do they develop their workforce when automation is advancing at light speed? And how do they attract customers and full-time employees when competition is high and trust is at an all-time low?\n\nModern businesses are being reshaped by a number of factors, including:\n\n\n\tIncreasing demand for trust, responsibility, credibility, honesty, and transparency in organizations.\n\tEmployees\u2019 search for meaning.\n\tNew leadership models with networks replacing hierarchies\u2014the recognition that the top-down approach is too slow, catalyzing a move toward decentralization and teams.\n\tThe impact of generational change on employee and customer expectations.\n\tBig systemic thinking\u2014the need to understand and consider organizations as operating in complex, interconnected environments.\n\tAutomation creating new kinds of partnerships between people and machines.\n\tFree agency, personal brands, and the evolving employer/employee relationship.\n\tCompensation beyond pay.\n\tDiversity, inclusion, and fairness at work.\n\tGovernance and the case for cognitive and experiential diversity.\n\n\nWe explore each of these trends in our report \u201cFuture of the Firm.\u201d\n\nMachine Learning / Artificial Intelligence\n\nFew technologies have the potential to change the nature of work, of the firm, and how we live as machine learning (ML) and artificial intelligence (AI). The impact of ML/AI on the future of our economy is both uncertain and undeniable. With new tools and computing power, ML/AI has become more effective at predictions, recommendations, certain types of pattern matching, and optimizing processes. And while the space has matured quickly, organizations continue to grapple with how to best apply ML/AI models\u2014we are still in early days with much to learn.\n\nWe see a lot of the effort around ML/AI aimed at improving or reframing the customer experience\u2014i.e., making processes simpler, faster, more convenient, more intuitive, and anticipating requests or actions. However, results don\u2019t always meet expectations due to a number of factors:\n\n\n\tThe need for large quantities of well-organized, accurate data.\n\tThe time and resources required to train machine learning models.\n\tAlgorithms that are difficult or too complex to understand.\n\tBias and fairness issues.\n\tThe need for constant monitoring.\n\tUnreliable accuracy.\n\n\nThe result: ML/AI work requires a different approach and different perspective from how we develop software. Managing risk and measuring success means more unpredictable schedules and more tolerance for failure\u2014think of ML/AI projects as a portfolio of experiments to monitor and evaluate.\n\nLearn more about how organizations are evaluating and implementing ML/AI in our report \u201cAI Adoption in the Enterprise.\u201d\n\nNext Architecture\n\nAnalysis of the O\u2019Reilly online learning platform shows growing engagement with cloud, orchestration, and microservices topics. When coupled with continued interest in containers, this engagement paints a picture of increased use of a new kind of software architecture for building an organization\u2019s digital presence. This architecture, which we call the Next Architecture, is cloud based, with functionality decomposed into microservices that are modularized into containers and managed and monitored by dynamic orchestration. Conversations with thought leaders across many industries confirm that the combination of cloud, containers, orchestration, and decomposition does indeed represent the path many organizations are taking for their next architecture.\n\nWhy the change? Organizations see the need to support agility, flexibility, scaling, resiliency, and productivity in building their digital properties as intrinsic to their value propositions and their ability to compete. The Next Architecture is not a cure-all or magic bullet. It\u2019s a way of thinking about and designing systems that promises to be more flexible and adaptable than traditional monolith approaches.\n\nMoving to the Next Architecture is not to be taken lightly, requiring new skills and the ability to manage complexity, including the particularly difficult task of turning complex functionality into modular, stand-alone services that can be easily upgraded or replaced. For most organizations, these challenges are worth confronting as a more flexible, agile, scalable architecture becomes essential for their digital properties.\n\nWe plan to release a report covering the Next Architecture in more detail in the coming months.\n\nResponding to Innovation & Disruption\n\nHow do you run a business when everything is always changing, and innovation and disruption have become the new norms? Once you acknowledge that change is inevitable, how do you embrace it? Moonshots? Incremental change? Innovation centers? Skunkworks? Key hires? Some mix? It\u2019s confusing, and each approach has its benefits and risks.\n\nAt a fundamental level, the best way to thrive in a world of constant innovation and disruption is to constantly reinvent yourself\u2014to pay attention to technology, to your customers, to thought leaders, and adapt. Leaders and staff at all levels need to embrace continuous learning to avoid surprising and existential threats. History is littered with organizations that failed to adapt: look at Digital Equipment Corporation. Look at Kodak. Look at Sears.\n\nBut we also see companies making profound turnarounds. Five years ago, Microsoft looked stagnant and irrelevant. Nobody would say that now. Microsoft adapted to a future that looks different from the past, embracing change, embracing open source, and embracing the cloud.\n\nTaking a note from Microsoft, what are the adaptations your organization needs to make? What technologies and shifts do you see on the horizon that will need to be addressed through innovation and disruption?\n\nFor example, blockchain, the distributed trust data structure, offers the potential for great disruption. And, our analysis shows organizations using our online learning platform paying increasing and considerable attention to blockchain as a topic.\n\nWhile banks and other financial institutions are trialing blockchain applications, we see a wealth of possibilities beyond finance to apply blockchain\u2019s encrypted and distributed data structure: supply chain / asset tracking, customer loyalty, identity management, government records, educational credentials, and distributed energy generation. Blockchain brings incredible disruptive potential to the future\u2014or, it may not. It\u2019s too early to tell. Nonetheless, if you\u2019re not paying attention to blockchain, you can be sure someone you compete with is.\n\nGet an introduction to blockchain\u2019s components and uses in \u201cWhat is a blockchain?\u201d\n\nMore to come from O\u2019Reilly Radar\n\nOur first report of 2019, \u201cFuture of the Firm,\u201d is now available.\n\nOver the coming months we\u2019ll explore each of these themes through reports and analytic studies, event tracks, interviews with leaders and experts, and through other content and activities. We hope you\u2019ll join us.\nContinue reading What is O'Reilly Radar?.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FPUk9vHx6kE/what-is-oreilly-radar"
 },
 {
  "title": "Four short links: 11 March 2019",
  "content": "Health Care NLP, Same Sex Databases, Apple Maps RE, and Science Articles\n\nLessons Learned Building Natural Language Processing Systems in Health Care -- The next mistake I made, like many others, was building models that \u201csolve health care.\u201d Amazon\u2019s Comprehend Medical is now taking this approach with a universal medical-NLP-as-a-service. This assumes that health care is one language. In reality, every sub-specialty and form of communication is fundamentally different.\n\n\nObergefell v. Hodges: The Database Engineering Perspective -- discussion of the database implications of same-sex marriage. But the more interesting thing is that you just incidentally let in a whole bunch of edge cases. Up until now, it wasn't possible for an individual to marry themself. Now it is, and you need a new check constraint to ensure that partner_1_id and partner_2_id are different. Regardless of concerns about duplicate rows/couples remarrying, you also now have to contend with swapped partners: Alice marries Eve, and also Eve marries Alice, resulting in two rows recording the same marriage.\n\n\nApple Maps Flyover Reverse Enginering -- This is an attempt to reverse-engineer Flyover (= 3D satellite mode) from Apple Maps. The main goal is to document the results and to provide code that emerges.\n\n\nbullets.tech -- The best articles for science lovers shortened to five bullet points or less. At first, I was sniffy at the \"five bullet points,\" but then I realized my modal amount retained from reading science articles is 0, so...\n\nContinue reading Four short links: 11 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/M6qlzz6QHjg/four-short-links-11-march-2019"
 },
 {
  "title": "International Women's Day at O'Reilly",
  "content": "At O\u2019Reilly, we seek to foster a culture that creates opportunity, rewards and recognizes accomplishments, and treats everyone with respect.On this International Women\u2019s Day, I want to call out the simple truth that actions speak louder than words. That respect and doing the right thing always matter regardless of gender, and that treating people with respect and equality are the constant responsibility of everyone. I'm proud to say at O\u2019Reilly our actions do speak for themselves and we have a record of taking action to promote equality.\n\nO\u2019Reilly in a nutshell\n\n\n\t50% of the executive team are women.\n\t47% of our current workforce are women.\n\tOf the 86 management positions in the company, 50% are held by women.\n\t45% of our promotions last year went to 31 women who assumed new positions of responsibility within the company.\n\tWe have a firm commitment to job and salary parity across all divisions, departments, and roles, evidenced by the fact that O'Reilly joined many other companies in signing the White House Equal Pay Pledge in 2016 (and we ensure we are always in compliance).\n\tWe've created event scholarship and diversity programs to provide opportunities for women and recognize people of all races, ethnicities, genders, ages, abilities, religions, sexual orientation, and military service.\n\tWe developed clear and specific anti-harassment and code of conduct policies, which are in force at all of our events (and widely used as a model by other tech events.)\n\tWe are actively committed to increasing the diversity of our conference speakers, which helps highlight women and members of other under-represented groups as visible leaders in the tech industry. 37% of our keynote speakers in 2018 were women, up from 32% in 2017. We also donated $36,710 to organizations that support women in tech throughout 2018.\n\n\nI\u2019m incredibly proud of these statistics, but they cannot stand in isolation. We must continue to push the envelope and strive for diversity and inclusion, not just for women, but for everyone. And, while our internal efforts are solid, we still have a ways to go regarding diversity and inclusion in our own hiring practices.\u00a0\n\nWhat we truly seek to accomplish at O\u2019Reilly is to make sure we foster a culture that creates opportunity for everyone, rewards and recognizes accomplishments, and treats everyone with respect regardless of their gender. Now that's something to celebrate.\n\nHappy International Women's Day!\nContinue reading International Women's Day at O'Reilly.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5I-dMH0QsoM/international-womens-day-at-oreilly"
 },
 {
  "title": "Four short links: 8 March 2019",
  "content": "Ethics and Skepticism, Corporate Open Source, Detecting Fake Text, and DNA Computation\n\nRound Up: Ethics and Skepticism -- There are a whole lot of different ways to misunderstand or be duped by data. This is my round up of good links that illustrate some of the most common problems with relying on data. Real-world examples. (via Elizabeth Goodman)\n\noss-contributors -- Adobe's s/w for measuring companies' contributions to GitHub, released as open source natch. The BigQuery table is also open. (via Fil Maj)\n\ngltr.io -- inspect the visual footprint of automatically generated text. It enables a forensic analysis of how likely an automatic system generated text. (via @harvardnlp and Miles Brundage)\n\nProtocells use DNA Logic to Communicate and Compute (Bristol) -- a new approach called BIO-PC (Biomolecular Implementation Of Protocell communication) based on communities of semi-permeable capsules (proteinosomes) containing a diversity of DNA logic gates that together can be used for molecular sensing and computation.\n\n\nContinue reading Four short links: 8 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FAplJJB2iUw/four-short-links-8-march-2019"
 },
 {
  "title": "Lessons learned building natural language processing systems in health care",
  "content": "NLP systems in health care are hard\u2014they require broad general and medical knowledge, must handle a large variety of inputs, and need to understand context.We\u2019re in an exciting decade for natural language processing (NLP). Computers will get as good as humans in complex tasks like reading comprehension, language translation, and creative writing. Language understanding benefits from every part of the fast-improving ABC of software: AI (freely available deep learning libraries like PyText and language models like BERT), big data (Hadoop, Spark, and Spark NLP), and cloud (GPU's on demand and NLP-as-a-service from all the major cloud providers).\n\n\n\nIn health care, several applications have already moved from science fiction to reality. AI systems passed the medical licensing exams in both China and England\u2014doing better than average doctors. A new system diagnoses 55 pediatric conditions better than junior doctors. These systems are harder to build than some of the first computer vision deep learning applications (i.e., study one image)\u2014they require a broader general and medical knowledge, handle a bigger variety of inputs, and must understand context.\n\n\n\nI\u2019ve been lucky to be involved in building NLP systems in health care for the past seven years. The goal of this article is to share key lessons I learned along the way to help you build similar systems faster and better.\n\n\n\nMeet the language of emergency room triage notes\n\n\n\nMany people, me included, make the mistake of assuming that clinical notes in the U.S. are written in English. That happens because that\u2019s how doctors will answer if you ask them what language they use. However, consider this example of three de-identified triage notes taken from emergency room visits:\n\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\tTriage Notes\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\tstates started last night, upper abd, took alka seltzer approx 0500, no relief. nausea no vomiting\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\tSince yesterday 10/10 \"constant Tylenol 1 hr ago. +nausea. diaphoretic. Mid abd radiates to back\n\t\t\t\n\t\t\n\t\t\n\t\t\t\n\t\t\tGeneralized abd radiating to lower x 3 days accompanied by dark stools. Now with bloody stool this am. Denies dizzy, sob, fatigue.\n\t\t\t\n\t\t\n\t\n\n\n\n\nMost people without a medical education do not understand the meaning of these typical sentences. Here are a few things to note:\n\n\n\n\n\tNone of these sentences are grammatically correct sentences in English.\n\tNone of them use the words \u201cpatient\u201d or \u201cpain.\u201d They don\u2019t have a subject.\n\tThey use a lot of jargon: 10/10 refers to the intensity of pain. \u201cGeneralized abd radiating to lower\u201d refers to general abdominal (stomach) pain that radiates to the lower back.\n\n\n\n\nER doctors I\u2019ve shown these notes to, though, consider them useful\u2014they\u2019re concise and focus on what matters. They would consider these as common and not \u201cbad\u201d examples of ER triage notes.\n\n\n\nYes, emergency rooms have their own language\n\n\n\nAs a philosopher or linguist, you might argue that this still does not constitute a \u201cdifferent language\u201d in the typical sense of the word. However, if you\u2019re a data scientist or NLP practitioner, there shouldn\u2019t be any doubt that it is:\n\n\n\n\n\t\nIt has a different vocabulary. The Unified Medical Language System (UMLS) includes more than 200 vocabularies for English alone, covering more than three million terms. In contrast, the Oxford English Dictionary of 1989 had 171,476 words (although, that should be roughly tripled to include derivatives that UMLS directly lists).\n\t\nIt has a different grammar. The text has its own definition of what sentences are and what parts of speech are. Statements like \u201c+nausea\u201d and \u201csince yesterday 10/10\u201d are grammatical structures that don\u2019t exist anywhere else.\n\t\nIt has different semantics. \u201cSob\u201d means \u201cshortness of breath\u201d (and not the other meaning you had in mind). \u201cDenies\u201d means the patient says they don\u2019t have the symptom, although the clinician thinks they might.\n\t\nIt goes beyond jargon. Jargon refers to the 100-200 new words you learn in the first month after you join a new school or workplace. In contrast, understanding health care language takes people as long as it takes to master day-to-day Italian or Portuguese.\n\n\n\n\nLesson #1: Off-the-shelf NLP models don\u2019t work\n\n\n\nIn practice, off-the-shelf NLP libraries and algorithms built for English will fail miserably on this \u201cdifferent language\u201d in the health care industry. Not only will named entity recognition or entity resolution models fail, but even basic tasks such as tokenization, part of speech tagging, and sentence segmentation will fail for the majority of sentences.\n\n\n\nIf you don\u2019t believe me, feel free to test it yourself with the six popular NLP cloud services and libraries listed below. All but Amazon Comprehend provide a web user interface so you can copy and paste sentences to see how the service would analyze it:\n\n\n\n\nGoogle Cloud Natural Language\nIBM Watson NLU\nAzure Text Analytics\nspaCy Named Entity Visualizer\n\nAmazon Comprehend (offline)\nStanford Core NLP\n\n\nIn a test done during December 2018, of the six engines, the only medical term (which only two of them recognized) was Tylenol as a product.\n\n\n\nHealth care has hundreds of languages\n\n\n\nThe next mistake I made, like many others, was building models that \u201csolve health care.\u201d Amazon\u2019s Comprehend Medical is now taking this approach with a universal medical-NLP-as-a-service. This assumes that health care is one language. In reality, every sub-specialty and form of communication is fundamentally different. Here\u2019s a handful of de-identified examples:\n\n\n\nPathology (Surgical pathology, cancer):\n\nPart #1 which is labeled \"? metastatic tumor in jugular vein lymph node\" consists of an elliptical fragment of light whitish-tan tissue which measures approximately 0.3 x 0.2 x 0.2 cm.\n\n\n\nRadiology (MRI Cervical Spine):\n\nC6-7: There is a diffuse disc osteophyte which results in flattening of the ventral thecal sac with a mild spinal canal stenosis and moderate to severe bilateral neural foraminal narrowing. OTHER FINDINGS: No paraspinal soft tissue mass.\n\n\n\nPre-authorization:\n\nBased on the outcome of the Phase I trial, the patient will receive permanent implantation of the stimulator. Specifically, this patient will receive a Spinal Cord Stimulator System, made by Boston Scientific Neuromodulation Corporation. This SCS System includes a re-chargeable battery within the implanted stimulator, allowing the physician and patient to control pain at the most optimal settings without compromising battery life compared to non-rechargeable SCS systems. The Boston Scientific SCS System is FDA-approved.\n\n\n\nPostop (from \"Objective\" section of a SOAP note):\n\nVitals- Tmax: 99.8, BP- 128/82, P- 82, R-18 I/O- 3000ml NS IV / 200ml out via foley, 800ml on own, in past 24 hours\n\nGeneral- laying in bed, appears comfortable\n\nSkin- Surgical incision margins have minimal erythema and are well approximated with staples, no dehiscence, no drainage. No signs of hematoma or seroma formation. No jaundice\n\n\n\nDental (Anesthetic, Specific Tooth):\n\nBenzocaine was placed on the palate, adjacent to tooth 1. A total of 0 .00  carpules of Articaine, 4% with Epinephrine 1:100,000 was injected into the palate using a long, 25-gauge needle.\n\n\n\nMedications (Dosage, route, frequency, duration, form)\n\naspirin is required 20 mg po daily for 2 times as tab\n\n\n\nNeed more examples? Take some time to learn about deciphering your lab reports. Or consider that medical students starting a specialty in dermatology need to master the aptly named Dermatology\u2014learning the language. Even Identifying Patient Smoking Status from medical discharge records is complex enough to be an active area of academic research.\n\n\n\nThen, there are many variants within each medical specialty. For example, deciding whether or not to approve a pre-authorization request for an MRI versus, say, an implantable spinal cord stimulator requires extracting completely different items from the pre-authorization forms. As another example, within pathology, different terms are used to discuss different types of cancer. This has a real-world impact: the company I work for is undertaking a project that requires training separate NLP models for extracting facts about lung, breast, and colon cancer from pathology reports.\n\n\n\nAmazon\u2019s Comprehend Medical has, so far, only focused on normalizing medication values (see that last \u201caspirin\u201d example in the above table). The service also comes with standard medical named entity recognition\u2014which doesn\u2019t address any specific application\u2019s needs. Please do not take my word for it\u2014try it yourself on the examples above or on your own text. Such NLP services are mostly used nowadays as a means to attract customers into professional services engagements. Other companies like 3M and Nuance that sell \u201chealth care NLP\u201d are more up front about this in their marketing.\n\n\n\nLesson #2: Build trainable NLP pipelines\n\n\n\nIf you need to build NLP systems in health care yourself, you\u2019ll need to train NLP models that are specific to the application you\u2019re building. This doesn\u2019t mean you cannot reuse existing software\u2014there is a lot you can reuse:\n\n\n\n\n\t\n\t\t\n\t\t\t\n\t\t\tReuse:\n\n\t\t\tMedical terminologies\n\n\t\t\tMedical embeddings\n\n\t\t\tNeural network graphs\n\n\t\t\tNLP Pipeline API\u2019s\n\n\t\t\tTraining & inference framework\n\t\t\t\n\t\t\t\n\t\t\tTo build:\n\n\t\t\tWhat medications is this patient taking?\n\n\t\t\tDoes this patient require a chest CT scan?\n\n\t\t\tWhat\u2019s the right E/M billing code for this visit?\n\n\t\t\tHas this patient been pregnant before?\n\n\t\t\tDo they have known allergies?\n\t\t\t\n\t\t\n\t\n\n\n\n\nWhen we built Spark NLP for Healthcare\u2014an extension of the open source NLP library for Apache Spark\u2014the goal was to provide as many reusable out-of-the-box components as possible. These include, for example, production-grade implementations of the state-of-the-art academic papers for clinical named entity recognition and de-identification, biomedical entity normalization, and assertion status (i.e., negation) detection. Using these implementations doesn\u2019t require learning to use TensorFlow (or any other framework), since the deep learning framework is embedded in the library under easy-to-use Python, Java, and Scala APIs. The library itself is a native extension of Spark ML and reuses its Pipeline class for building, serializing, and extending NLP, ML, and DL flows.\n\n\n\nMaking this library perform in real-world projects taught us a lot about just how different \u201chealth care languages\u201d are from human ones. Here are some of the things we had to build:\n\n\n\n\n\t\nDeep learning-based sentence segmentation. While splitting sentences in Wikipedia articles often can be done just using regular expressions, handling multi-page clinical documents was a bigger challenge. In particular, the algorithms had to deal with headers and footers, lists, enumerations, call-outs, two-column documents, and other formatting.\n\t\nHealth care-specific part-of-speech tagging. Not only was a different model required, but additional parts of speech are used for health care models. This was done because it actually improves the accuracy of clinical named entity recognition.\n\t\nHealth care-specific entity normalization algorithms. Named entity recognition by itself is often useless in practice: annotating from \u201cboth eyes seem to be infected\u201d that \u201ceye\u201d and \u201cinfection\u201d are medical terms doesn\u2019t help much. In contrast, marking the whole chunk of text as code 312132001 from the standard SNOMED-CT clinical terminology, while normalizing for the different ways to describe the same finding, is much more useful. It enables your application to base business logic based on this code, no matter how it was normalized or how, exactly, it was expressed in the free-form text it came from.\n\n\n\n\nIn short: the deeper we go into treating health care texts as different languages, the closer we get to matching and exceeding human accuracy on the same tasks.\n\n\n\nLesson #3: Start with labeling ground truth\n\n\n\nSo, how do you start your own project? How do you know how far off you are and whom to trust? One way is to start by building a labeled validation set. For example, if you are interested in automating ICD-10 coding from outpatient notes, have clinicians define a representative sample of such records, de-identify them, and have professional clinical coders label them (by assigning the correct codes). If you are interested in extracting critical events from radiology reports or missed safety events from in-patient notes, have clinicians define the sample and label them correctly first.\n\n\n\nThis will often uncover blockers you need to address before involving (and wasting the time of) your data science team. If you don\u2019t have access to enough data, or can\u2019t de-identify it at scale, then there\u2019s no way to build a reliable model anyway. If clinicians cannot consistently agree on the correct labels in some cases, then the first problem to solve is to agree on clinical guidelines instead of involving data scientists to try to automate a disagreement. Finally, if you find you\u2019re facing highly unbalanced classes (i.e., you are looking for something that happens to a handful of patients per year), it may be wise to change the definition of the problem before calling in the data scientists.\n\n\n\nOnce you have a representative and an agreed upon and correctly labeled validation set, you can start testing existing libraries and cloud providers. Most likely, the first test will immediately uncover the gaps between each offering and your needs. The smartest teams we\u2019ve worked with have set up week-long or two-week-long test projects, in which the goal is to use a library or service to reach the maximum level of accuracy for your specific needs. Doing this enables you to evaluate how easy each service is to train custom models, define domain-specific features and pipeline steps that your solution requires, and explain the results back to you.\n\n\n\nSuch an approach can be a great education opportunity for your team. It tests both the packaged software and the support/consulting aspects of the services you\u2019ll evaluate. It will show you how far you are from achieving a level of accuracy that\u2019s in line with your business needs. Finally, this third lesson enables you to validate lessons #1 and #2 on your own, without taking my word for them.\n\n\n\nBest of luck and success in your projects. Since this is health care we\u2019re talking about, the world needs you to succeed!\n\n\n\n\n\nRelated resources:\n\n\n\tAn upcoming Strata Data London tutorial: \u201cNatural language understanding at scale with Spark NLP\u201d\n\n\t\u201cLessons learned turning machine learning models into real products and services\u201d\n\tDavid Talby on \u201cBuilding a natural language processing library for Apache Spark\u201d\n\n\t\u201cComparing production-grade NLP libraries\u201d\n\tMaryam Jahanshahi on \u201cUsing machine learning and analytics to attract and retain employees\u201d\n\n\tAlan Nichol on \u201cUsing machine learning to improve dialog flow in conversational applications\u201d\n\n\tDavid Ferrucci on why \u201cLanguage understanding remains one of AI\u2019s grand challenges\u201d\n\n\tDavid Blei on \u201cTopic models: Past, present, and future\u201d\n\n\nContinue reading Lessons learned building natural language processing systems in health care.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/xE_zpM_o_P4/lessons-learned-building-natural-language-processing-systems-in-health-care"
 },
 {
  "title": "Four short links: 7 March 2019",
  "content": "Privacy Exercise, Tragedy of the Commons, Program Repair, and Privacy != Safety\n\nPrivacy Exercise (Twitter) -- Professor Kate Klonick gave her students a great exercise to teach them about deanonymization, \"if you have nothing to hide,\" etc. (via BoingBoing)\n\nThe Tragedy of the Tragedy of the Commons (Twitter) -- Matto Mildenberger succinctly points not only to the critiques of Hardin's \"Tragedy of the Commons\" idea, but also to the other ideas of the man himself. Now, lots of awful people have left noble ideas that outlive them. But in Hardin\u2019s case, the intellectual legacy is largely built on top of his racist, flawed science that we still treat as gospel and uncritically assign in undergraduate courses year after year. \n\n\nSimFix -- Automatically fix programs by leveraging existing patches from other projects and similar code snippets from the faulty project. If automated program repair interests you, you should know about program-repair.org. (via Hacker News)\n\nZuck Thinks Encrypted Message Will Save Facebook -- no mention of misinformation, doxxing, or any of the other evils on Facebook's core platform. Instead, a move to end-to-end encrypted communication, where Facebook can't monitor the contents. Feels like The Problem was defined as \"we have to police content\" instead of the pain actually felt by users.\n\nContinue reading Four short links: 7 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Nb3z3TLuWjs/four-short-links-7-march-2019"
 },
 {
  "title": "Four short links: 6 March 2019",
  "content": "Reverse Engineering, Public Policy, New Editor, and Burnout\n\nGhidra -- software reverse-engineering tool, rival for IDAPro. Open source, released by NSA.\n\nCybersecurity in the Public Interest (Bruce Schneier) -- We need public-interest technologists in policy discussions. We need them on congressional staff, in federal agencies, at non-governmental organizations (NGOs), in academia, inside companies, and as part of the press. In our field, we need them to get involved in not only the Crypto Wars, but everywhere cybersecurity and policy touch each other: the vulnerability equities debate, election security, cryptocurrency policy, Internet of Things safety and security, big data, algorithmic fairness, adversarial machine learning, critical infrastructure, and national security.\n\n\nKakoune -- a code editor that implements vi\u2019s \"keystrokes as a text editing language\" model. As it\u2019s also a modal editor, it is somewhat similar to the Vim editor (after which Kakoune was originally inspired). In the words of a Hacker News commenter, it's trying to ditch some of the historical ed/ex syntax and thought patterns that make vi weirdly inconsistent.\n\nBurnout Self Test -- This tool can help you check yourself for burnout. It helps you look at the way you feel about your job and your experiences at work, so you can get a feel for whether you are at risk of burnout.\n\n\nContinue reading Four short links: 6 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/j7pPQCv4IIk/four-short-links-6-march-2019"
 },
 {
  "title": "Four short links: 5 March 2019",
  "content": "CEO Personality, Adult Development, Big Idea Famine, and Neuron Imaging\n\nStrategic Decisions: Behavioral Differences Between CEOs and Others -- All subjects participated in three incentivized games\u2014Prisoner\u2019s Dilemma, Chicken, Battle-of-the-Sexes. Beliefs were elicited for each game. We report substantial and robust differences in both behavior and beliefs between the CEOs and the control group. The most striking results are that CEOs do not best respond to beliefs; they cooperate more, play less hawkish, and thereby earn much more than the control group. (via Marginal Revolution)\n\nRobert Keegan's Theory of Adult Development -- interesting set of stages: selfish/transaction; weak sense of self/strongly influenced by others; emotionally aware and strong sense of personal value and belief system; and able to continuously grow by adopting, adapting, and discarding mental models and \"identities.\" Found it via YC Startup School's excellent How to Win lecture by Daniel Gross, which also included the mantra that \"sleep is a nootropic.\"\n\nBig Idea Famine (Nicolas Negroponte) -- I believe that 30 years from now people will look back at the beginning of our century and wonder what we were doing and thinking about big, hard, long-term problems, particularly those of basic research. They will read books and articles written by us in which we congratulate ourselves about being innovative. The self-portraits we paint today show a disruptive and creative society, characterized by entrepreneurship, startups, and big company research advertised as moonshots. Our great-grandchildren are certain to read about our accomplishments, all the companies started, and all the money made. At the same time, they will experience the unfortunate knock-on effects of an historical (by then) famine of big thinking. (via Daniel G. Siegel)\n\nCalmAn -- an open source library for calcium imaging data analysis. [...] CaImAn is suitable for two-photon and one-photon imaging, and also enables real-time analysis on streaming data. [...] We demonstrate that CaImAn achieves near-human performance in detecting locations of active neurons.\n\n\nContinue reading Four short links: 5 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jNCW4UP2EE0/four-short-links-5-march-2019"
 },
 {
  "title": "Four short links: 4 March 2019",
  "content": "Open Source Chat, Assistant Shim, Data Science, and Quantum Computing\n\nZulip 2.0 Out -- open source chat, a-la Slack. Better support for thread-like multiple conversations in a channel.\n\nProject Alias -- a teachable \u201cparasite\u201d that is designed to give users more control over their smart assistants, both when it comes to customization and privacy. Through a simple app, the user can train Alias to react on a custom wake-word/sound, and once trained, Alias can take control over your home assistant by activating it for you. There's no problem in technology that can't be solved by the addition of another layer of indirection. Interesting to see attempts to make third-party improvements to these things in our house that we have no control over except that which The Maker has given us.\n\nPython Data Science in Jupyter Notebooks -- full text.\n\nWhen Will Quantum Computing Have Real Commercial Value? -- Nobody really knows. The field sits behind a number of difficult science and engineering breakthroughs before we get to the equivalent of a UNIVAC, let alone a PDP-1, Altair 8800, or iPhone. My read is that the military's hopes and fears for quantum crypto and comms have buoyed VCs' hopes, so they're wading in before the fundamental research is done and are just hoping there'll be a breakthrough within the lifetime of their fund.\n\nContinue reading Four short links: 4 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/SI-8GCt237A/four-short-links-4-march-2019"
 },
 {
  "title": "Four short links: 1 March 2019",
  "content": "Voice Data, Lunar Library, IP Rights, Computational Photography\n\t\nCommon Voice Data (Mozilla) -- largest dataset of human voices available for use, including 18 different languages, adding up to almost 1,400 hours of recorded voice data from more than 42,000 contributors. (via Mozilla blog)\n\t\nThe Lunar Library -- The Arch Mission Foundation is nano-etching 30,000,000 pages' worth of \"archives of human history and civilization, covering all subjects, cultures, nations, languages, genres, and time periods\" onto 25 DVD-sized, 40-micron-thick discs that will be deposited on the surface of the moon in 2019 by the Beresheet lander. (that's from BoingBoing's coverage)\n\t\nIP Rights of Hackathons (OKFN) -- The tool is a set of documents. [...] The \u201cdefault settings\u201d of the documents are such that participants\u2019 intellectual property rights stay with participants. [....] These settings are in the pre-participation document, and the post-participation document is for writing down an agreement among the contributing group members over their rights, permissions, terms, etc. so that they know what they can do with the output of their team. \n\n\t\nSpectre App -- Spectre lets you erase moving tourists from busy locations or capture light trails and water movements from the camera on your iPhone. Computational photography is awesome, but I feel like all these apps are implementing what will become features of The Camera App on your phone. Unless, that is, the UI for a camera app that can do everything is Too Much For Mortals and people actually want a half-dozen different camera apps.\n\nContinue reading Four short links: 1 March 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/H5KXrRMTyFs/four-short-links-1-march-2019"
 },
 {
  "title": "Why your attention is like a piece of contested territory",
  "content": "The O\u2019Reilly Data Show Podcast: P.W. Singer on how social media has changed, war, politics, and business.In this episode of the Data Show, I spoke with P.W. Singer, strategist and senior fellow at the New America Foundation, and a contributing editor at Popular Science. He is co-author of an excellent new book, LikeWar: The Weaponization of Social Media, which explores how social media has changed war, politics, and business. The book is essential reading for anyone interested in how social media has become an important new battlefield in a diverse set of domains and settings.Continue reading Why your attention is like a piece of contested territory.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Vjv09c_RJAo/why-your-attention-is-like-a-piece-of-contested-territory"
 },
 {
  "title": "You created a machine learning application. Now make sure it\u2019s secure.",
  "content": "The software industry has demonstrated, all too clearly, what happens when you don\u2019t pay attention to security.In a recent post, we described what it would take to build a sustainable machine learning practice. By \u201csustainable,\u201d we mean projects that aren\u2019t just proofs of concepts or experiments. A sustainable practice means projects that are integral to an organization\u2019s mission: projects by which an organization lives or dies. These projects are built and supported by a stable team of engineers, and supported by a management team that understands what machine learning is, why it\u2019s important, and what it\u2019s capable of accomplishing. Finally, sustainable machine learning means that as many aspects of product development as possible are automated: not just building models, but cleaning data, building and managing data pipelines, testing, and much more. Machine learning will penetrate our organizations so deeply that it won\u2019t be possible for humans to manage them unassisted.\n\nOrganizations throughout the world are waking up to the fact that security is essential to their software projects. Nobody wants to be the next Sony, the next Anthem, or the next Equifax. But while we know how to make traditional software more secure (even though we frequently don\u2019t), machine learning presents a new set of problems. Any sustainable machine learning practice must address machine learning\u2019s unique security issues. We didn\u2019t do that for traditional software, and we\u2019re paying the price now. Nobody wants to pay the price again. If we learn one thing from traditional software\u2019s approach to security, it\u2019s that we need to be ahead of the curve, not behind it. As Joanna Bryson writes, \u201cCyber security and AI are inseparable.\u201d\n\nThe presence of machine learning in any organization won\u2019t be a single application, a single model; it will be many applications, using many models\u2014perhaps thousands of models, or tens of thousands, automatically generated and updated. Machine learning on low-power edge devices, ranging from phones to tiny sensors embedded in assembly lines, tools, appliances, and even furniture and building structures, increases the number of models that need to be monitored. And the advent of 5G mobile services, which significantly increases the network bandwidth to mobile devices, will make it much more attractive to put machine learning at the edge of the network. We anticipate billions of machines, each of which may be running dozens of models.\u00a0At this scale, we can't assume that we can deal with security issues manually. We need tools to assist the humans responsible for security. We need to automate as much of the process as possible, but not too much, giving humans the final say.\n\nIn \u201cLessons learned turning machine learning models into real products and services,\u201d David Talby writes that \u201cthe biggest mistake people make with regard to machine learning is thinking that the models are just like any other type of software.\u201d Model development isn\u2019t software development. Models are unique\u2014the same model can\u2019t be deployed twice; the accuracy of any model degrades as soon as it is put into production; and the gap between training data and live data, representing real users and their actions, is huge. In many respects, the task of modeling doesn\u2019t get started until the model hits production, and starts to encounter real-world data.\n\nUnfortunately, one characteristic that software development has in common with machine learning is a lack of attention to security. Security tends to be a low priority. It gets some lip service, but falls out of the picture when deadlines get tight. In software, that\u2019s been institutionalized in the \u201cmove fast and break things\u201d mindset. If you\u2019re building fast, you\u2019re not going to take the time to write sanitary code, let alone think about attack vectors. You might not \u201cbreak things,\u201d but you\u2019re willing to build broken things; the benefits of delivering insecure products on time outweigh the downsides, as Daniel Miessler has written. You might be lucky; the vulnerabilities you create may never be discovered. But if security experts aren\u2019t part of the development team from the beginning, if security is something to be added on at the last minute, you\u2019re relying on luck, and that\u2019s not a good position to be in. Machine learning is no different, except that the pressure of delivering a product on time is even greater, the issues aren\u2019t as well understood, the attack surface is larger, the targets are more valuable, and companies building machine learning products haven\u2019t yet engaged with the problems.\n\nWhat kinds of attacks will machine learning systems see, and what will they have to defend against? All of the attacks we have been struggling with for years, but there are a number of vulnerabilities that are specific to machine learning. Here\u2019s a brief taxonomy of attacks against machine learning:\n\nPoisoning, or injecting bad (\u201cadversarial\u201d) data into the training data. We\u2019ve seen this many times in the wild. Microsoft\u2019s Tay was an experimental chatbot that was quickly taught to spout racist and anti-semitic messages by the people who were chatting with it. By inserting racist content into the data stream, they effectively gained control over Tay\u2019s behavior. The appearance of \u201cfake news\u201d in channels like YouTube, Facebook, Twitter, and even Google searches, was similar: once fake news was posted, users were attracted to it like flies, and the algorithms that made recommendations \u201clearned\u201d to recommend that content. danah boyd has argued that these incidents need to be treated as security issues, intentional and malicious corruption of the data feeding the application, not as isolated pranks or algorithmic errors.\n\nAny machine learning system that constantly trains itself is vulnerable to poisoning. Such applications could range from customer service chat bots (can you imagine a call center bot behaving like Tay?) to recommendation engines (real estate redlining might be a consequence) or even to medical diagnosis (modifying recommended drug dosages). To defend against poisoning, you need strong control over the training data. Such control is difficult (if not impossible) to achieve. \u201cBlack hat SEO\u201d to improve search engine rankings is nothing if not an early (and still very present) example of poisoning. Google can\u2019t control the incoming data, which is everything that is on the web. Their only recourse is to tweak their search algorithms constantly and penalize abusers for their behavior. In the same vein, bots and troll armies have manipulated social media feeds to spread views ranging from opposition to vaccination to neo-naziism.\n\nEvasion, or crafting input that causes a machine learning system to misclassify it. Again, we\u2019ve seen this both in the wild and in the lab. CV Dazzle uses makeup and hair styles as \u201ccamouflage against face recognition technology.\u201d Other research projects have shown that it\u2019s possible to defeat image classification by changing a single pixel in an image: a ship becomes a car, a horse becomes a frog. Or, just as with humans, image classifiers can miss an unexpected object that\u2019s out of context: an elephant in the room, for example. It\u2019s a mistake to think that computer vision systems \u201cunderstand\u201d what they see in ways that are similar to humans. They\u2019re not aware of context, they don\u2019t have expectations about what\u2019s normal; they\u2019re simply doing high-stakes pattern matching. Researchers have reported similar vulnerabilities in natural language processing, where changing a word, or even a letter, in a way that wouldn\u2019t confuse human researchers causes machine learning to misunderstand a phrase.\n\nAlthough these examples are often amusing, it\u2019s worth thinking about real-world consequences: could someone use these tricks to manipulate the behavior of autonomous vehicles? Here\u2019s how that could work: I put a mark on a stop sign\u2014perhaps by sticking a fragment of a green sticky note at the top. Does that make an autonomous vehicle think the stop sign is a flying tomato, and if so, would the car stop? The alteration doesn\u2019t have to make the sign \u201clook like\u201d a tomato to a human observer; it just has to push the image closer to the boundary where the model says \u201ctomato.\u201d Machine learning has neither the context nor the common sense to understand that tomatoes don\u2019t appear in mid-air. Could a delivery drone be subverted to become a weapon by causing it to misunderstand its surroundings? Almost certainly. Don\u2019t dismiss these examples as academic. A stop sign with a few pixels changed in the lab may not be different from a stop sign that has been used for target practice during hunting season.\n\nImpersonation attacks attempt to fool a model into misidentifying someone or something. The goal is frequently to gain unauthorized access to a system. For example, an attacker might want to trick a bank into misreading the amount written on a check. Fingerprints obtained from drinking glasses, or even high resolution photographs, can be used to fool fingerprint authentication. South Park trolled Alexa and Google Home users by using the words \u201cAlexa\u201d and \u201cOK Google\u201d repeatedly in an episode, triggering viewers\u2019 devices; the devices weren\u2019t able to distinguish between the show voices and real ones. The next generation of impersonation attacks will be \u201cdeep fake\u201d videos that place words in the mouths of real people.\n\nInversion means using an API to gather information about a model, and using that information to attack it. Inversion can also mean using an API to obtain private information from a model, perhaps by retrieving data and de-anonymizing it. In \u201cThe Secret Sharer: Measuring Unintended Neural Network Memorization & Extracting Secrets,\u201d the authors show that machine learning models tend to memorize all their training data, and that it\u2019s possible to extract protected information from a model. Common approaches to protecting information don\u2019t work; the model still incorporates secret information in ways that can be extracted. Differential privacy\u2014the practice of carefully inserting extraneous data into a data set in ways that don\u2019t change its statistical properties\u2014has some promise, but with significant cost: the authors point out that training is much slower. Furthermore, the number of developers who understand and can implement differential privacy is small.\n\nWhile this may sound like an academic concern, it\u2019s not; writing a script to probe machine learning applications isn\u2019t difficult. Furthermore, Michael Veale and others write that inversion attacks raise legal problems. Under the GDPR, if protected data is memorized by models, are those models subject to the same regulations as personal data? In that case, developers would have to remove personal data from models\u2014not just the training data sets\u2014on request; it would be very difficult to sell products that incorporated models, and even techniques like automated model generation could become problematic. Again, the authors point to differential privacy, but with the caution that few companies have the expertise to deploy models with differential privacy correctly.\n\nOther vulnerabilities, other attacks\n\nThis brief taxonomy of vulnerabilities doesn\u2019t come close to listing all the problems that machine learning will face in the field. Many of these vulnerabilities are easily exploited. You can probe Amazon to find out what products are recommended along with your products, possibly finding out who your real competitors are, and discovering who to attack. You might even be able to reverse-engineer how Amazon makes recommendations and use that knowledge to influence the recommendations they make.\n\nMore complex attacks have been seen in the field. One involves placing fake reviews on an Amazon seller\u2019s site, so that when the seller removes the reviews, Amazon bans the seller for review manipulation. Is this an attack against machine learning? The attacker tricks the human victim into violating Amazon\u2019s rules. Ultimately, though, it\u2019s the machine learning system that\u2019s tricked into taking an incorrect action (banning the victim) that it could have prevented.\n\n\u201cGoogle bowling\u201d means creating large numbers of links to a competitor\u2019s website in hopes that Google\u2019s ranking algorithm will penalize the competitor for purchasing bulk links. It\u2019s similar to the fake review attack, except that it doesn\u2019t require a human intermediary; it\u2019s a direct attack against the algorithm that analyzes inbound links.\n\nAdvertising was one of the earliest adopters of machine learning, and one of the earliest victims. Click fraud is out of control, and the machine learning community is reluctant to talk about (or is unaware of) the issue\u2014even though, as online advertising becomes ever more dependent on machine learning, fraudsters will learn how to attack models directly in their attempts to appear legitimate. If click data is unreliable, then models built from that data are unreliable, along with any results or recommendations generated by those models. And click fraud is similar to many attacks against recommendation systems and trend analysis. Once a \u201cfake news\u201d item has been planted, it\u2019s simple to make it trend with some automated clicks. At that point, the recommendation takes over, generating recommendations which in turn generate further clicks. Anything automated is prone to attack, and automation allows those attacks to take place at scale.\n\nThe advent of autonomous vehicles, ranging from cars to drones, presents yet another set of threats. If the machine learning systems on an autonomous vehicle are vulnerable to attack, a car or truck could conceivably be used as a murder weapon. So could a drone\u2014either a weaponized military drone or a consumer drone. The military already knows that drones are vulnerable; in 2011, Iran captured a U.S. drone, possibly by spoofing GPS signals. We expect to see attacks on \u201csmart\u201d consumer health devices and professional medical devices, many of which we know are already vulnerable.\n\nTaking action\n\nMerely scolding and thinking about possible attacks won\u2019t help. What can be done to defend machine learning models? First, we can start with traditional software. The biggest problem with insecure software isn\u2019t that we don\u2019t understand security; it\u2019s that software vendors, and software users, never take the basic steps they would need to defend themselves. It\u2019s easy to feel defenseless before hyper-intelligent hackers, but the reality is that sites like Equifax become victims because they didn\u2019t take basic precautions, such as installing software updates. So, what do machine learning developers need to do?\n\nSecurity audits are a good starting point. What are the assets that you need to protect? Where are they, and how vulnerable are they? Who has access to those resources, and who actually needs that access? How can you minimize access to critical data? For example, a shipping system needs customer addresses, but it doesn\u2019t need credit card information; a payment system needs credit card information, but not complete purchase histories. Can this data be stored and managed in separate, isolated databases? Beyond that, are basic safeguards in place, such as two-factor authentication? It\u2019s easy to fault Equifax for not updating their software, but almost any software system depends on hundreds, if not thousands, of external libraries. What strategy do you have in place to ensure they\u2019re updated, and that updates don't break working systems?\n\nLike conventional software, machine learning systems should use monitoring systems that generate alerts to notify staff when something abnormal or suspicious occurs. Some of these monitoring systems are already using machine learning for anomaly detection\u2014which means the monitoring software itself can be attacked.\n\nPenetration testing is a common practice in the online world: your security staff (or, better, consultants) attack your site to discover its vulnerabilities. Attack simulation is an extension of penetration testing that shows you \u201chow attackers actually achieve goals against your organization.\u201d What are they looking for? How do they get to it? Can you gain control over a system by poisoning its inputs?\n\nTools for testing computer vision systems by generating \"adversarial images\" are already appearing, such as cleverhans and IBM\u2019s ART. We are starting to see papers describing adversarial attacks against speech recognition systems. Adversarial input is a special case of a more general problem. Most machine learning developers assume their training data is similar to the data their systems will face in the real world. That\u2019s an idealized best case. It\u2019s easy to build a face identification system if all your faces are well-lit, well-focused, and have light-skinned subjects. A working system needs to handle all kinds of images, including images that are blurry, badly focused, poorly lighted\u2014and have dark-skinned subjects.\n\nSafety verification is a new area for AI research, still in its infancy. Safety verification asks questions like whether models can deliver consistent results, or whether small changes in the input lead to large changes in the output. If machine learning is at all like conventional software, we expect an escalating struggle between attackers and defenders; better defenses will lead to more sophisticated attacks, which will lead to a new generation of defenses. It will never be possible to say that a model has been \u201cverifiably safe.\u201d But it is important to know that a model has been tested, and that it is reasonably well-behaved against all known attacks.\n\nModel explainability has become an important area of research in machine learning. Understanding why a model makes specific decisions is important for several reasons, not the least of which is that it makes people more comfortable with using machine learning. That \u201ccomfort\u201d can be deceptive, of course. But being able to ask models why they made particular decisions will conceivably make it easier to see when they\u2019ve been compromised. During development, explainability will make it possible to test how easy it is for an adversary to manipulate a model, in applications from image classification to credit scoring. In addition to knowing what a model does, explainability will tell us why, and help us build models that are more robust, less subject to manipulation; understanding why a model makes decisions should help us understand its limitations and weaknesses. At the same time, it\u2019s conceivable that explainability will make it easier to discover weaknesses and attack vectors. If you want to poison the data flowing into a model, it can only help to know how the model responds to data.\n\nIn \u201cDeep Automation in Machine Learning,\u201d we talked about the importance of data lineage and provenance, and tools for tracking them. Lineage and provenance are important whether or not you\u2019re developing the model yourself. While there are many cloud platforms to automate model building and even deployment, ultimately your organization is responsible for the model\u2019s behavior. The downside of that responsibility includes everything from degraded profits to legal liability. If you don\u2019t know where your data is coming from and how it has been modified, you have no basis for knowing whether your data has been corrupted, either through accident or malice.\n\n\u201cDatasheets for Datasets\u201d proposes a standard set of questions about a data set\u2019s sources, how the data was collected, its biases, and other basic information. Given a specification that records a data set\u2019s properties, it should be easy to test and detect sudden and unexpected changes. If an attacker corrupts your data, you should be able to detect that and correct it up front; if not up front, then later in an audit.\n\nDatasheets are a good start, but they are only a beginning. Whatever tools we have for tracking data lineage and provenance need to be automated. There will be too many models and data sets to rely on manual tracking and audits.\n\nBalancing openness against tipping off adversaries\n\nIn certain domains, users and regulators will increasingly prefer machine learning services and products that can provide simple explanations for how automated decisions and recommendations are being made. But we\u2019ve already seen that too much information can lead to certain parties gaming models (as in SEO). How much to disclose depends on the specific application, domain, and jurisdiction.\n\nThis balancing act is starting to come up in machine learning and related areas that involve the work of researchers (who tend to work in the open) who are up against adversaries who prize unpublished vulnerabilities. The question of whether or not to \u201ctemporarily hold back\u201d research results is a discussion that the digital media forensics community has been having. In a 2018 essay, Hany Farid noted: \u201cWithout necessarily advocating this as a solution for everyone, when students are not involved on a specific project, I have held back publication of new techniques for a year or so. This approach allows me to always have a few analyses that our adversaries are not aware of.\u201d\n\nPrivacy and security are converging\n\nDevelopers will also need to understand and use techniques for privacy-preserving machine learning, such as differential privacy, homomorphic encryption, secure multi-party computation, and federated learning. Differential privacy is one of the few techniques that protects user data from \u201cinverting\u201d a model and extracting private data from it. Homomorphic encryption allows systems to do computations directly on encrypted data, without the need for decryption. And federated learning allows individual nodes to compute parts of a model, and then send their portion back to be combined to build a complete model; individual users\u2019 data doesn\u2019t have to be transferred. Federated learning is already being used by Google to improve suggested completions for Android users. However, some of these techniques are slow (in some cases, extremely slow), and require specialized expertise that most companies don\u2019t have. And you often will need a combination of these techniques to achieve privacy. It\u2019s conceivable that future tools for automated model building will incorporate these techniques, minimizing the need for local expertise.\n\nLive data\n\nMachine learning applications increasingly interact with live data, complicating the task of building safe, reliable, and secure systems. An application as simple as a preference engine has to update itself constantly as its users make new choices. Some companies are introducing personalization and recommendation models that incorporate real-time user behavior. Disinformation campaigns occur in real time, so detecting disinformation requires knowledge bases that can be updated dynamically, along with detection and mitigation models that can also be updated in real time. Bad actors who create and propagate disinformation are constantly getting more sophisticated, making it harder to detect, particularly with text-based content. And recent developments in automatic text generation means that the creation of \u201cfake news\u201d can be automated. Machine learning can detect potential misinformation, but at present, humans are needed to verify and reject misinformation. Machine learning can aid and support human action, but humans must remain in the loop.\n\nApplications of reinforcement learning frequently interact with live data, and researchers are well aware of the need to build reinforcement learning applications that are safe and robust. For applications like autonomous driving, failures are catastrophic; but at the same time, the scarcity of failure makes it harder to train systems effectively.\n\nOrganization and culture\n\nIn traditional software development, we are finally learning that security experts have to be part of development teams from the beginning. Security needs to be part of the organization\u2019s culture. The same is true for machine learning: from the beginning, it\u2019s important to incorporate security experts and domain experts who understand how a system is likely to be abused. As Facebook\u2019s former chief security officer Alex Stamos has said, \u201cthe actual responsibility [for security] has to be there when you\u2019re making the big design decisions.\u201d Every stage of a machine learning project must think about security: the initial design, building the data pipelines, collecting the data, creating the models, and deploying the system. Unfortunately, as Stamos notes, few teams are actually formed this way.\n\nConclusions\n\nWhatever they might believe, most organizations are in the very early stages of adopting machine learning. The companies with capabilities equivalent to Google, Facebook, Amazon, or Microsoft are few and far between; at this point, most are still doing some early experiments and proofs of concepts. Thought and effort haven\u2019t gone into security. And maybe that\u2019s fair; does a demo need to be secure?\n\nPerhaps not, but it\u2019s worth thinking carefully about history. Security is a problem in part because the inventors of modern computer networking didn\u2019t think it was necessary. They were building the ARPAnet: an academic research net that would never go beyond a few hundred sites. Nobody anticipated the public internet. And yet, even on the proto-internet, we had the Morris worm in the 80s, and email spam in the '70s. One of the things we do with any technology is abuse it. By ignoring the reality of abuse, we entered a never-ending race; it\u2019s impossible to win, impossible to quit, and easy to lose.\n\nBut even if we can give the internet\u2019s early naivete a pass, there\u2019s no question that we live in a world where security is a paramount concern. There is no question that applications of machine learning will touch (indeed, invade) people\u2019s lives, frequently without their knowledge or consent. It is time to put a high priority on security for machine learning.\n\nWe believe that attacks against machine learning systems will become more frequent and sophisticated. That\u2019s the nature of the security game: an attack is countered by a defense, which is countered in turn by a more sophisticated attack, in a game of endlessly increasing complexity. We\u2019ve listed a few kinds of attacks, but keep in mind we\u2019re in the early days. Our examples aren\u2019t exhaustive, and there are certainly many vulnerabilities that nobody has yet thought of. These vulnerabilities will inevitably be discovered; cybercrime is a substantial international business, and the bad actors even include government organizations.\n\nMeanwhile, the stakes are getting higher. We\u2019ve only begun to pay the penalty for highly vulnerable networked devices\u2014the Internet of Things (IoT)\u2014and while the security community is aware of the problems, there are few signs that manufacturers are addressing the issues. IoT devices are only becoming more powerful, and 5G networking promises to extend high-bandwidth, low-latency connectivity to the edges of the network. We are already using machine learning in our phones; will machine learning extend to near-microscopic chips embedded in our walls? There are already voice activity detectors that can run on a microwatt; as someone on Twitter suggests, a future generation could possibly run on energy generated from sound waves. And there are already microphones where we least suspect them.\u00a0Deploying insecure \"smart devices\" on this scale isn't a disaster waiting to happen; it's a disaster that's already happening.\n\nWe have derived a lot of value from machine learning, and we will continue to derive value from pushing it to the limits; but if security issues aren\u2019t addressed, we will have to pay the consequences. The software industry has demonstrated, all too clearly, what happens when you don\u2019t pay attention to security. As machine learning penetrates our lives, those stakes will inevitably become higher.\n\nRelated resources:\n\n\n\t\u201cDeep automation in machine learning\u201d\n\t\u201cManaging risk in machine learning\u201d\n\tSiwei Lyu on \u201cThe technical, societal, and cultural challenges that come with the rise of fake media\u201d\n\n\tAndrew Burt on \u201cHow machine learning impacts information security\u201d\n\n\t\u201cLessons learned turning machine learning models into real products and services\u201d\n\t\u201cWhat machine learning means for software development\u201d\n\t\u201cBuilding tools for the AI applications of tomorrow\u201d\n\t\u201cWhat are machine learning engineers?\u201d\n\nContinue reading You created a machine learning application. Now make sure it\u2019s secure..",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/up7UrpKv1Qg/you-created-a-machine-learning-application-now-make-sure-its-secure"
 },
 {
  "title": "Four short links: 28 February 2019",
  "content": "Breakthrough Technologies, AI Habitat, Simplified Datomic, and Metrics\n\t\nBreakthrough Technologies 2019 (MIT TR) -- robot dexterity, new-wave nuclear power, predicting preemies, gut probe in a pill, custom cancer vaccines, the cow-free burger, CO2 catcher, ECG on your wrist, sanitation without sewers, AI assistants.\n\t\nAI Habitat (Facebook) -- enables training of embodied AI agents (virtual robots) in a highly photorealistic & efficient 3D simulator, before transferring the learned skills to reality. \n\n\t\nAsami -- In-memory graph store that implements the Naga storage protocol. This has a query API that looks very similar to a simplified Datomic.\n\n\t\nMetrics -- Metrics are lossily compressed logs. Traces are logs with parent child relationships between entries. The only reason we have three terms is because getting value from them has required different compromises to make them cost effective. --Clint Sharp. (via Simon Willison)\n\nContinue reading Four short links: 28 February 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Ln_YaCrHBqc/four-short-links-28-february-2019"
 },
 {
  "title": "Four short links: 27 February 2019",
  "content": "Universal Binaries, Front-End Training, ML Myths, and Recommended Books\n\t\nWASMer -- Universal Binaries Powered by WebAssembly. (open source)\n\t\nFrontend Workshop from HTML/CSS/JS to TypeScript/React/Redux -- Microsoft's training materials. Open sourced.\n\t\nMyths in Machine Learning -- TensorFlow is a Tensor manipulation library; Image datasets are representative of real images found in the wild; Machine Learning researchers do not use the test set for validation; Every datapoint is used in training a neural network; We need (batch) normalization to train very deep residual networks; Attention > Convolution; Saliency maps are robust ways to interpret neural networks.\n\n\t\nBooks I Recommend (Jessie Frazelle) -- tight set of recommendations that overlap enough with my own reading that I'm already ordering the books that are new to me.\n\nContinue reading Four short links: 27 February 2019.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/pz5-5XAqHCg/four-short-links-27-february-2019"
 },
 {
  "title": "3 reasons to add deep learning to your time series toolkit",
  "content": "The most promising area in the application of deep learning methods to time series forecasting is in the use of CNNs, LSTMs, and hybrid models.The ability to accurately forecast a sequence into the future is critical in many industries: finance, supply chain, and manufacturing are just a few examples. Classical time series techniques have served this task for decades, but now deep learning methods\u2014similar to those used in computer vision and automatic translation\u2014have the potential to revolutionize time series forecasting as well.\n\nDue to their applicability to many real-life problems\u2014such as fraud detection, spam email filtering, finance, and medical diagnosis\u2014and their ability to produce actionable results, deep learning neural networks have gained a lot of attention in recent years. Generally, deep learning methods have been developed and applied to univariate time series forecasting scenarios, where the time series consists of single observations recorded sequentially over equal time increments. For this reason, they have often performed worse than na\u00efve and classical forecasting methods, such as exponential smoothing (ETS) and autoregressive integrated moving average (ARIMA). This has led to a general misconception that deep learning models are inefficient in time series forecasting scenarios, and many data scientists wonder whether it\u2019s really necessary to add another class of methods\u2014such as convolutional neural networks or recurrent neural networks\u2014to their time series toolkit.\n\nIn this post, I'll discuss some of the practical reasons why data scientists may still want to think about deep learning when they build time series forecasting solutions.\n\nDeep learning neural networks: Some foundational concepts\n\nThe goal of machine learning is to find features to train a model that transforms input data (such as pictures, time series, or audio) to a given output (such as captions, price values, transcriptions). Deep learning is a subset of machine learning algorithms that learn to extract these features by representing input data as vectors and transforming them with a series of clever linear algebra operations into a given output.\n\nData scientists then evaluate whether the output is what they expected using an equation called loss function. The goal of the process is to use the result of the loss function from each training input to guide the model to extract features that will result in a lower loss value on the next pass. This process has been used to cluster and classify large volumes of information\u2014for example, millions of satellite images; thousands of video and audio recordings from YouTube; and historical, textual, and sentiment data from Twitter.\n\nDeep learning neural networks have three main intrinsic capabilities:\n\n\n\tThey can learn from arbitrary mappings from inputs to outputs\n\tThey support multiple inputs and outputs\n\tThey can automatically extract patterns in input data that spans over long sequences\n\n\nThanks to these three characteristics, deep learning neural networks can o\ufb00er a lot of help when data scientists deal with more complex but still very common problems, such as time series forecasting.\n\nHere are three reasons data scientists should consider adding deep learning to their time series toolkits.\n\n\nReason #1: Deep learning neural networks are capable of automatically learning and extracting features from raw and imperfect data\n\nTime series is a type of data that measures how things change over time. In time series, time isn\u2019t just a metric, but a primary axis. This additional dimension represents both an opportunity and a constraint for time series data because it provides a source of additional information but makes time series problems more challenging, as specialized handling of the data is required. Moreover, this temporal structure can carry additional information, like trends and seasonality, that data scientists need to deal with in order to make their time series easier to model with any type of classical forecasting methods.\n\nNeural networks can be useful for time series forecasting problems by eliminating the immediate need for massive feature engineering processes, data scaling procedures, and the need for making the data stationary by differencing.\n\nIn real-world time series scenarios\u2014for example, weather forecasting, air quality and traffic flow forecasting, and forecasting scenarios based on streaming IoT devices like geo-sensors\u2014irregular temporal structures, missing values, heavy noise, and complex interrelationships between multiple variables present limitations for classical forecasting methods. These techniques typically rely on clean, complete data sets in order to perform well: missing values, outliers, and other imperfect features are generally unsupported.\n\nSpeaking of more artificial and perfect data sets, classical forecasting methods are based on the assumption that a linear relationship and a \ufb01xed temporal dependence exist among variables of a data set, and this assumption by default excludes the possibility of exploring more complex (and probably more interesting) relationships among variables. Data scientists must make subjective judgements when preparing data for classical analysis\u2014like the lag period used to remove trends\u2014which is time consuming and introduces human biases to the process. On the contrary, neural networks are robust to noise in input data and in the mapping function, and can even support learning and prediction in the presence of missing values.\n\nConvolutional neural networks (CNNs) are a category of neural networks that have proven very effective in areas such as image recognition and classification. CNNs have been successful in identifying faces, objects, and traffic signs in addition to powering vision in robots and self-driving cars. CNNs derive their name from the \u201cconvolution\u201d operator. The primary purpose of convolution in the case of CNNs is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features using small squares of input data. In other words, the model learns how to automatically extract the features from the raw data that are directly useful for the problem being addressed. This is called \"representation learning\" and the CNN achieves this in such a way that the features are extracted regardless of how they occur in the data, so-called \"transform\" or \"distortion\" invariance.\n\nThe ability of CNNs to learn and automatically extract features from raw input data can be applied to time series forecasting problems. A sequence of observations can be treated like a one-dimensional image that a CNN model can read and refine into the most relevant elements. This capability of CNNs has been demonstrated to great e\ufb00ect on time series classi\ufb01cation tasks, such as indoor movement prediction using wireless sensor strength data to predict the location and motion of subjects within a building.\n\n\n\nReason #2: Deep learning supports multiple inputs and outputs\n\nReal-world time series forecasting is challenging for several reasons, such as having multiple input variables, the requirement to predict multiple time steps, and the need to perform the same type of prediction for multiple physical sites. Deep learning algorithms can be applied to time series forecasting problems and offer benefits such as the ability to handle multiple input variables with noisy complex dependencies. Specifically, neural networks can be con\ufb01gured to support an arbitrary but \ufb01xed number of inputs and outputs in the mapping function. This means that neural networks can directly support multivariate inputs, providing direct support for multivariate forecasting. A univariate time series, as the name suggests, is a series with a single time-dependent variable. For example, if we want to predict the next energy consumption in a specific location: in a univariate time series scenario, our data set will be based on two variables: time values and historical energy consumption observations.\n\nA multivariate time series has more than one time-dependent variable. Each variable depends not only on its past values, but also has some dependency on other variables. This dependency is used for forecasting future values. Let\u2019s consider the above example again. Now suppose our data set includes weather data, such as temperature values, dew point, wind speed, cloud cover percentage, etc., along with the energy consumption value for the past four years. In this case, there are multiple variables to be considered to optimally predict an energy consumption value. A series like this would fall under the category of a multivariate time series.\n\nWith neural networks, an arbitrary number of output values can be speci\ufb01ed, offering direct support for more complex time series scenarios that require multivariate forecasting and even multi-step forecast methods. There are two main approaches to using deep learning methods to make multi-step forecasts: 1) direct, where a separate model is developed to forecast each forecast lead time; and 2) recursive, where a single model is developed to make one-step forecasts, and the model is used recursively where prior forecasts are used as input to forecast the subsequent lead time.\n\nThe recursive approach can make sense when forecasting a short contiguous block of lead times, whereas the direct approach may make more sense when forecasting discontiguous lead times. The direct approach may be more appropriate when we need to forecast a mixture of multiple contiguous and discontiguous lead times over a period of a few days; such is the case, for example, with air pollution forecasting problems or for anticipatory shipping forecasting, used to predict what customers want and then ship the products automatically.\n\nKey to the use of deep learning algorithms for time series forecasting is the choice of multiple input data. We can think about three main sources of data that can be used as input and mapped to each forecast lead time for a target variable; they are: 1) univariate data, such as lag observations from the target variable that is being forecasted; 2) multivariate data, such as lag observations from other variables (for example, weather and targets in case of air pollution forecasting problems); 3) metadata, such as data about the date or time being forecast. Data can be drawn from across all chunks, providing a rich data set for learning a mapping from inputs to the target forecast lead time.\n\n\n\nReason #3: Deep learning networks are good at extracting patterns in input data that span over relatively long sequences\n\nDeep learning is an active research area, and CNNs are not the only class of neural network architectures being used for time series and sequential data. Recurrent neural networks (RNNs) were created in the 1980s but have been recently gaining popularity and increased computational power from graphic processing units. They are especially useful with sequential data because each neuron or unit can use its internal memory to maintain information about the previous input. An RNN has loops that allow information to be carried across neurons while reading in input.\n\nHowever, a simple recurrent network suffers from a fundamental problem of not being able to capture long-term dependencies in a sequence. This is a major reason why RNNs faded from practice for a while until some great results were achieved using a long short-term memory (LSTM) unit inside the neural network. Adding the LSTM to the network is like adding a memory unit that can remember context from the very beginning of the input.\n\nLSTM neural networks are a particular type of RNN that have internal contextual state cells that act as long-term or short-term memory cells. The output of the LSTM network is modulated by the state of these cells. This is a very important property when we need the prediction of the neural network to depend on the historical context of inputs, rather than only on the very last input. They are a type of neural network that adds native support for input data comprised of sequences of observations. The addition of sequence is a new dimension to the function being approximated. Instead of mapping inputs to outputs alone, the network can learn a mapping function for the inputs over time to an output. The example of video processing can be very effective when we need to understand how LSTM networks work: in a movie, what happens in the current frame is heavily dependent on what was in the previous frame. Over a period of time, an LSTM network tries to learn what to keep and how much to keep from the past, and how much information to keep from the present state, which makes it powerful compared to other types of neural networks.\n\nThis capability can be used in any time series forecasting context, where it can be extremely helpful to automatically learn the temporal dependence from the data. In the simplest case, the network is shown one observation at a time from a sequence and can learn which prior observations are important and how they are relevant to forecasting. The model both learns a mapping from inputs to outputs and learns what context from the input sequence is useful for mapping and can dynamically change this context as needed. Not surprisingly, this approach has been often used in the finance industry to build models that forecast exchange rates based on the idea that past behavior and price patterns may affect currency movements and can be used to predict future price behavior and patterns.\n\nOn the other hand, there are downsides that data scientists need to be careful about with neural network architectures. Large volumes of data are required, and models require hyper-parameter tuning and multiple optimization cycles.\n\n \n\nConclusion\n\nDeep learning neural networks are powerful engines capable of learning from arbitrary mappings from inputs to outputs, supporting multiple inputs and outputs, and automatically extracting patterns in input data that span long sequences of time. All these characteristics together make neural networks helpful tools when dealing with more complex time series forecasting problems that involve large amounts of data, multiple variables with complicated relationships, and even multi-step time series tasks. A lot of research has been invested into using neural networks for time series forecasting with modest results. Perhaps the most promising area in the application of deep learning methods to time series forecasting is in the use of CNNs, LSTMs, and hybrid models.\n\n\n\nUseful resources\n\nRecent improvements in tools and technologies has meant that techniques like deep learning are now being used to solve common problems, including forecasting, text mining, language understanding, and personalization. Below are some useful resources and presentations involving deep learning:\n\n\n\tForecasting Financial Time Series with Deep Learning on Azure\n\tDeep Learning and AI Frameworks\n\tSilver, Gold & Electrum: 3 Data Techniques for Multi-Task Deep Learning\n\tNeural Networks for Forecasting Financial and Economic Time Series\n\tDeep Learning Virtual Machine\n\tPyTorch on Azure: Deep learning in the Oil and Gas Industry\n\tAI and Machine Learning in the Enterprise\n\tDeep Learning\n\tTemporal Data and Time Series Analytics\n\n\n\n\n\nContinue reading 3 reasons to add deep learning to your time series toolkit.",
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2A-HpaOHYRc/3-reasons-to-add-deep-learning-to-your-time-series-toolkit"
 }
]